---
title: Workflows
author: Robrecht Cannoodt, Data Intuitive
---

Single-cell analysis has revolutionized our understanding of cellular heterogeneity and complex biological processes. However, this cutting-edge field often demands the use of multiple programming languages and frameworks, each with its strengths and specialized tools [@Heumos2023]. This polyglot approach, while powerful, introduces significant technical challenges in terms of interoperability, usability, and reproducibility.

In the previous chapters, we've explored strategies for supporting data operability across programming language. Now, we turn our attention to how to effectively integrate these tools and languages into a cohesive and scalable analysis workflow.

## Productionization

Productionization is the process of transforming research-oriented analysis pipelines into robust, scalable, and maintainable workflows that can be reliably executed in a production environment ([@fig-productionization]). This transition is essential for ensuring the reproducibility of results, facilitating collaboration among researchers, and enabling the efficient processing of large and complex single-cell datasets.

![An example of the productionization process for single-cell analysis workflows. **A)** The research environment is characterized by scattered data, manual steps, and ad-hoc analysis pipelines. **B)** The production environment is streamlined, automated, and standardized, with reproducibility engines in place.](figures/productionization.svg){#fig-productionization}

### But how to ensure that your workflow is production-ready? {.unnumbered}

In this chapter, we will explore:

* **Key qualities** of workflows built to stand the test of time
* Which **technologies and workflow frameworks** contribute to these qualities
* **Best practices** to keep in mind during development


## Review of Workflow Frameworks

[A lot of different workflow frameworks exist](https://github.com/pditommaso/awesome-pipeline), and there are a lot of factors to consider when choosing the right one for your project.
@Wratten2021 conducted a review of popular workflow managers for bioinformatics, evaluating them based on several key aspects, including ease of use, expressiveness, portability, scalability, and learning resources ([@tbl-strengths]).


| Tool | Class | Ease of use | Expressiveness | Portability | Scalability | Learning resources | Pipeline initiatives |
| ---- | ----- | ----------- | -------------- | ----------- | ----------- | ------------------ | -------------------- |
| Galaxy | Graphical | ●●● | ●○○ | ●●● | ●●● | ●●● | ●●○ |
| KNIME  | Graphical | ●●● | ●○○ | ○○○ | ●●◐ | ●●● | ●●○ |
| Nextflow | DSL | ●●○ | ●●● | ●●● | ●●● | ●●● | ●●● |
| Snakemake | DSL | ●●○ | ●●● | ●●◐ | ●●● | ●●○ | ●●● |
| GenPipes | DSL | ●●○ | ●●● | ●●○ | ●●○ | ●●○ | ●●○ |
| bPipe | DSL | ●●○ | ●●● | ●●○ | ●●◐ | ●●○ | ●○○ |
| Pachyderm | DSL | ●●○ | ●●● | ●○○ | ●●○ | ●●● | ○○○ |
| SciPipe | Library | ●●○ | ●●● | ○○○ | ○○○ | ●●○ | ○○○ |
| Luigi | Library | ●●○ | ●●● | ●○○ | ●●◐ | ●●○ | ○○○ |
| Cromwell + WDL | Execution + workflow specification | ●○○ | ●●○ | ●●● | ●●◐ | ●●○ | ●●○ |
| cwltool + CWL | Execution + workflow specification | ●○○ | ●●○ | ●●◐ | ○○○ | ●●● | ●●○ |
| Toil + CWL/WDL/Python | Execution + workflow specification | ●○○ | ●●● | ●◐○ | ●●● | ●●○ | ●●○ |

: Overview of workflow managers for bioinformatics [@Wratten2021]. {#tbl-strengths}

Even more interesting is the accompanying GitHub repository ([GoekeLab/bioinformatics-workflows](https://github.com/GoekeLab/bioinformatics-workflows)), which contains a **Proof of Concept (PoC) RNA-seq workflow** implemented in the different workflow frameworks. These implementations were contributed and reviewed by the **developers of the respective frameworks** themselves!

![Wow! ;)](https://media2.giphy.com/media/v1.Y2lkPTc5MGI3NjExcDZ0bmhuNmx1c3Y4dmUzNDUwZTdrM3dwdHk3MXVjcXVtZnlrYzQ2cSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/J87jeioCWcipG/giphy.webp){}

Looking at these implementations, at first glance, one would think that the differences between the frameworks are minimal, and that the choice of framework is mostly a matter of personal preference.

### Comparing PoC Workflows to Community-Made Modules

However, comparing the POC workflows (_left_) to community-made modules (_right_), it becomes clear that **creating production-ready components** requires a lot more than **specifying a command's input and output files**.


:::{.column-page}

#### Nextflow

:::{.grid}

:::{.g-col-5}

Wratten et al. 2021 PoC ([Source](https://github.com/GoekeLab/bioinformatics-workflows/tree/master/nextflow)):

:::{.panel-tabset}

## `main.nf`

```{embed lang="groovy"}
examples/nextflow/wratten2021_poc/main.nf
```

:::

:::

:::{.g-col-7}

nf-core ([Source](https://github.com/nf-core/modules/tree/master/modules/nf-core/fastqc)):

:::{.panel-tabset}

## `environment.yml`

```{embed lang="yaml"}
examples/nextflow/nf-core/environment.yml
```

## `main.nf`

```{embed lang="groovy"}
examples/nextflow/nf-core/main.nf
```

## `meta.yaml`

```{embed lang="yaml"}
examples/nextflow/nf-core/meta.yml
```

## `tests/main.nf.test`

```{embed lang="groovy"}
examples/nextflow/nf-core/tests/main.nf.test
```

:::

:::

:::

:::



:::{.column-page}

#### Snakemake

:::{.grid}

:::{.g-col-5}

Wratten et al. 2021 PoC ([Source](https://github.com/GoekeLab/bioinformatics-workflows/tree/master/snakemake)):

:::{.panel-tabset}

## `fastqc.smk`

```{embed lang="python"}
examples/snakemake/wratten2021_poc/fastqc.smk
```

:::

:::

:::{.g-col-7}

snakemake-wrappers ([Source](https://github.com/snakemake/snakemake-wrappers/tree/master/bio/fastqc)):

:::{.panel-tabset}

## `environment.yaml`

```{embed lang="yaml"}
examples/snakemake/snakemake-wrappers/environment.yaml
```

## `meta.yaml`

```{embed lang="yaml"}
examples/snakemake/snakemake-wrappers/meta.yaml
```

## `wrapper.py`

```{embed lang="python"}
examples/snakemake/snakemake-wrappers/wrapper.py
```

## `test/Snakefile`

```{embed lang="python"}
examples/snakemake/snakemake-wrappers/test/Snakefile
```

:::

:::

:::

:::

:::{.column-page}

#### Toil + WDL

:::{.grid}

:::{.g-col-5}

Wratten et al. 2021 PoC ([Source](https://github.com/GoekeLab/bioinformatics-workflows/tree/master/wdl)):

:::{.panel-tabset}

## `fastqc.wdl`

```{embed lang="bash"}
examples/wdl/wratten2021_poc/fastqc.wdl
```

:::

:::

:::{.g-col-7}

BioWDL ([Source](https://github.com/biowdl/tasks/blob/develop/fastqc.wdl)):

:::{.panel-tabset}

## `fastqc.wdl` (Excerpt)

```{embed lang="bash"}
examples/wdl/biowdl/fastqc.wdl
```

## `fastqc.wdl` (Full)

```{embed lang="bash"}
examples/wdl/biowdl/fastqc_full.wdl
```

:::

:::

:::

:::


### Limitations of the study

However, the [Supplementary Table](https://static-content.springer.com/esm/art%3A10.1038%2Fs41592-021-01254-9/MediaObjects/41592_2021_1254_MOESM2_ESM.xlsx) shows that the comparison in [@tbl-strengths] was rather limited, since the score of each category was only based on a single criterion. Of the following categories, only "Scalability" was determined by more than one criterion:

* **Ease of Use**: Graphical interface with execution environment (score of 3), programming interface with in-built execution environment (score of 2), separated development and execution environment (score of 1).
* **Expressiveness**: Based on an existing programming language (3) or a new language or restricted vocabulary (2), primary interaction with graphical user interface (1).
* **Portability**:  Integration with three or more container and package manager platforms (3), two platforms are supported (2), one platform is supported (1).
* **Scalability**: Considers cloud support, scheduler and orchestration tool integration, and executor support. Please refer to Supplementary Table 1 - Sheet 2 (Scalability).
* **Learning resources**: Official tutorials, forums and events (3), tutorials and forums (2), tutorials or forums (1).
* **Pipelines Initiatives**: Community and curated (3), community or curated (2), not community or curated (1).

By comparing the example code of the respective workflow frameworks, it also becomes clear that we need not only look at example code of POC workflows, but actual production-ready workflows and pipelines. Such code often require a lot more functionality, including:

* Error handling
* Logging
* Data provenance
* Parameterization
* Testing
* Documentation
* Containerization
* Resource management


## Qualities of a Production-Ready Workflow

Building production-ready workflows for single-cell analysis involves integrating a variety of tools, technologies, and best practices. In order to meet the demands of large-scale data processing, reproducibility, and collaboration, a production-ready workflow should exhibit the following essential qualities ([@fig-qualities]):

![Essential qualities of a production-ready workflow.](figures/qualities.svg){#fig-qualities}

* **Polyglot**: Seamlessly integrate tools and libraries from different programming languages, allowing you to leverage the strengths of each language for specific tasks. This facilitates the use of specialized tools and optimizes the analysis pipeline for performance and efficiency.
* **Modular**: A well-structured workflow should be composed of modular and reusable components, promoting code maintainability and facilitating collaboration. Each module should have a clear purpose and well-defined inputs and outputs, enabling easy integration and replacement of individual steps within the pipeline.
* **Scalable**: Single-cell datasets can be massive, and a production-ready workflow should be able to handle large volumes of data efficiently. This involves utilizing scalable compute environments, optimizing data storage and retrieval, and implementing parallelization strategies to accelerate analysis.
* **Reproducible**: Ensuring reproducibility is crucial for scientific rigor and validation. A production-ready workflow should capture all the necessary information, including code, data, parameters, and software environments, to enable others to replicate the analysis and obtain consistent results.
* **Portable**: The workflow should be designed to run seamlessly across different computing platforms and environments, promoting accessibility and collaboration. Containerization technologies like Docker can help achieve portability by encapsulating the workflow and its dependencies into a self-contained unit.
* **Community**: Leveraging community resources, tools, and best practices can accelerate the development of production-ready workflows. This is because developing high-quality components can at times be time-consuming, and sharing resources can help reduce duplication of effort and promote collaboration.
* **Maintainable**: A production-ready workflow should be well-documented, organized, and easy to understand, facilitating updates, modifications, and troubleshooting. Clear documentation of code, data, and parameters ensures that the workflow remains accessible and usable over time.

## Technologies for Production-Ready Workflows

The essential qualities of a production-ready workflow are achieved through a combination of enabling technologies ([@fig-technologies]). These technologies provide the foundation for building scalable, reproducible, and maintainable workflows for single-cell analysis.

![The essential qualities of a production-ready workflow are achieved through a combination of enabling technologies.](figures/technologies.svg){#fig-technologies}


## Quality Assessment of Workflow Frameworks

```{r tibble, echo=FALSE, eval=FALSE}
library(tibble)
library(dplyr)
library(tidyr)
library(purrr)

googlesheets4::gs4_auth(email = "robrecht@data-intuitive.com")
wf_qc_sheet <- "https://docs.google.com/spreadsheets/d/1OMsnm8bIWc_Hp5neBOj7hWURT7ZL-L6qSbr0oAzmvSI"
wf_qc <- googlesheets4::read_sheet(wf_qc_sheet, sheet = "qc") |>
  mutate_each(function(x) {
    map_chr(x, function(y) {
      if (is.null(y)) {
        NA_character_
      } else {
        y
      }
    })
  })

wf_qc2 <- wf_qc

# fill in NAs due to merged cells
for (i in seq(2, nrow(wf_qc2), by = 1)) {
  if (is.na(wf_qc2$name[[i]])) {
    wf_qc2$name[[i]] <- wf_qc2$name[[i - 1]]
  }
  if (is.na(wf_qc$aspect[[i]])) {
    wf_qc2$aspect[[i]] <- wf_qc2$aspect[[i - 1]]
  }
}

# remove incomplete frameworks and unused questions
wf_qc2 <- wf_qc2 |>
  select(name, aspect, item, Nextflow, Snakemake, Galaxy, `Viash + Nextflow`, `Argo Workflows`) %>%
  filter(!is.na(Nextflow)) %>%
  filter(item != "The framework provides clear error messages and debugging tools.")

# sanitize string
string_to_colname <- function(x) {
  x <- tolower(x)
  x <- gsub(" ", "_", x)
  x <- gsub("[^[:alnum:]_]", "", x)
  x <- gsub("__", "_", x)
  x
}

# process metadata
meta <- wf_qc2 |>
  filter(name == "Metadata") |>
  select(-name, -aspect) |>
  as.matrix() |>
  t() |>
  as.data.frame() %>%
  rownames_to_column("project")
colnames(meta) <- string_to_colname(unname(unlist(meta[1,])))
meta <- meta[-1,]
rownames(meta) <- NULL
meta <- meta |>
  rename(project = item) |>
  mutate(id = string_to_colname(project))

# process criteria
criteria <- wf_qc2 |>
  filter(name != "Metadata") |>
  select(name, aspect, item)

# process scores
scores <- wf_qc2 |>
  filter(name != "Metadata") |>
  gather(key = "framework", value = "data", -item, -name, -aspect) %>%
  mutate(
    score = gsub("^([0-9\\.]*).*", "\\1", data) |> as.numeric(),
    url = gsub(".*(http[^ ]*)$", "\\1", data) %>% ifelse(. == data, NA_character_, .),
    explanation = gsub("^[0-9\\.]* ", "", data) %>% gsub("http[^ ]*$", "", .)
  )

agg <- scores |>
  group_by(name, framework) |>
  summarize(mean_score = mean(score), .groups = "drop")

# write to file
write.csv(meta, "notebooks/workflows/data/wf_metadata.csv", row.names = FALSE)
write.csv(criteria, "notebooks/workflows/data/criteria.csv", row.names = FALSE)
write.csv(agg, "notebooks/workflows/data/wf_aggregated_scores.csv", row.names = FALSE)
```

Given the abovementioned limitations, we decided to conduct our own quality assessment of workflow frameworks. This assessment is still largely in the works, but we're happy to share the preliminary results with you.

The data is based on a review of the documentation and community resources for each framework. We evaluated the frameworks based on the list of essential qualities mentioned in the previous section ([@fig-qualities]).

### Included frameworks

The following workflow frameworks were included in the assessment:

```{r wf_metadata, echo=FALSE, results="asis"}
meta <- read.csv("data/wf_metadata.csv")

for (i in seq_len(nrow(meta))) {
  project <- meta$project[[i]]
  project_description <- meta$project_description[[i]]
  project_website <- meta$project_website[[i]]
  project_repository <- meta$project_repository[[i]]
  documentation <- meta$documentation[[i]]
  doi <- meta$doi[[i]]
  community <- meta$community[[i]]
  community_repository <- meta$community_repository[[i]]

  strs <- c(
    paste0("[", project, "](", project_website, "): ",
    gsub("\\. *$", "", project_description), ".")
  )

  if (!is.na(project_repository)) {
    strs <- c(strs, paste0("[{{< fa brands github >}}](", project_repository, ")"))
  }

  if (!is.na(documentation)) {
    strs <- c(strs, paste0("[{{< fa circle-info >}}](", documentation, ")"))
  }

  if (!is.na(doi)) {
    strs <- c(strs, paste0("[{{< fa book-open >}}](", doi, ")"))
  }

  if (!is.na(community)) {
    strs <- c(strs, paste0("[{{< fa users >}}](", community, ")"))
  }

  cat(paste0(
    "- ", paste(strs, collapse = " "), "\n"
  ))
}
```

### Quality Assessment Criteria

The quality assessment was based on the following criteria:

:::{.panel-tabset}

```{r criteria, echo=FALSE, results="asis"}
criteria <- read.csv("data/criteria.csv")

unique_names <- unique(criteria$name)

for (name in sort(unique(criteria$name))) {
  crit <- criteria[criteria$name == name, , drop = FALSE]

  cat(paste0("#### ", name, " {.unnumbered}\n\n"))

  for (aspect in sort(unique(crit$aspect))) {
    asp <- crit[crit$aspect == aspect, , drop = FALSE]
    cat(paste0("**", aspect, "**:\n\n"))

    for (item in asp$item) {
      cat(paste0(" * ", item, "\n"))
    }

    cat("\n\n")
  }
}
```

:::

These criteria and subsequent scores will be further refined and validated as part of our ongoing research.

### Quality Scores

The aggregated quality scores for each framework are shown below. The scores are based on the evaluation of the essential qualities of a production-ready workflow.

```{r plot_scores, echo=FALSE}
#| fig-cap: Quality scores for different workflow frameworks. 
#| fig-format: svg
library(ggplot2)

agg <- read.csv("data/wf_aggregated_scores.csv")
agg$framework <- factor(agg$framework, levels = sort(unique(agg$framework), decreasing = TRUE))

ggplot(agg) +
  geom_point(aes(mean_score, framework, color = framework)) +
  geom_segment(aes(x = 0, xend = mean_score, y = framework, yend = framework, color = framework), linewidth = 1) +
  facet_wrap(~name, ncol = 2) +
  theme_minimal() +
  theme(
    panel.grid.minor = element_blank(),
    panel.grid.major.y = element_blank(),
    legend.position = "none"
  ) +
  labs(
    x = NULL,
    y = NULL
  ) +
  scale_color_brewer(palette = "Set1")
```

Raw scores and detailed explanations behind the reasoning of the resulting scores can be found in the [Workflow Quality Assessment Spreadsheet](https://docs.google.com/spreadsheets/d/1OMsnm8bIWc_Hp5neBOj7hWURT7ZL-L6qSbr0oAzmvSI).

### Quality assessment contributors

* Jakub Majerčík
* Michaela Müller 
* Robrecht Cannoodt
* Toni Verbeiren

## Conclusion

