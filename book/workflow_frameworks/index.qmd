---
title: Workflows
engine: knitr
---

In the previous chapters, we've explored strategies for supporting data operability across programming language. Now, we turn our attention to how to effectively integrate these tools and languages into a cohesive and scalable analysis workflow.

## Productionization

Productionization is the process of transforming research-oriented analysis pipelines into robust, scalable, and maintainable workflows that can be reliably executed in a production environment ([@fig-productionization]). This transition is essential for ensuring the reproducibility of results, facilitating collaboration among researchers, and enabling the efficient processing of large and complex single-cell datasets.

![An example of the productionization process for single-cell analysis workflows. **A)** The research environment is characterized by scattered data, manual steps, and ad-hoc analysis pipelines. **B)** The production environment is streamlined, automated, and standardized, with reproducibility engines in place.](images/productionization.svg){#fig-productionization}

### But how to ensure that your workflow is production-ready? {.unnumbered}

In this chapter, we will explore:

* **Key qualities** of workflows built to stand the test of time
* Which **technologies and workflow frameworks** contribute to these qualities
* **Best practices** to keep in mind during development


## Review of Workflow Frameworks

[A lot of different workflow frameworks exist](https://github.com/pditommaso/awesome-pipeline), and there are a lot of factors to consider when choosing the right one for your project.
@Wratten2021 conducted a review of popular workflow managers for bioinformatics, evaluating them based on several key aspects, including ease of use, expressiveness, portability, scalability, and learning resources ([@tbl-strengths]).


| Tool | Class | Ease of use | Expressiveness | Portability | Scalability | Learning resources | Pipeline initiatives |
| ---- | ----- | ----------- | -------------- | ----------- | ----------- | ------------------ | -------------------- |
| Galaxy | Graphical | ●●● | ●○○ | ●●● | ●●● | ●●● | ●●○ |
| KNIME  | Graphical | ●●● | ●○○ | ○○○ | ●●◐ | ●●● | ●●○ |
| Nextflow | DSL | ●●○ | ●●● | ●●● | ●●● | ●●● | ●●● |
| Snakemake | DSL | ●●○ | ●●● | ●●◐ | ●●● | ●●○ | ●●● |
| GenPipes | DSL | ●●○ | ●●● | ●●○ | ●●○ | ●●○ | ●●○ |
| bPipe | DSL | ●●○ | ●●● | ●●○ | ●●◐ | ●●○ | ●○○ |
| Pachyderm | DSL | ●●○ | ●●● | ●○○ | ●●○ | ●●● | ○○○ |
| SciPipe | Library | ●●○ | ●●● | ○○○ | ○○○ | ●●○ | ○○○ |
| Luigi | Library | ●●○ | ●●● | ●○○ | ●●◐ | ●●○ | ○○○ |
| Cromwell + WDL | Execution + workflow specification | ●○○ | ●●○ | ●●● | ●●◐ | ●●○ | ●●○ |
| cwltool + CWL | Execution + workflow specification | ●○○ | ●●○ | ●●◐ | ○○○ | ●●● | ●●○ |
| Toil + CWL/WDL/Python | Execution + workflow specification | ●○○ | ●●● | ●◐○ | ●●● | ●●○ | ●●○ |

: Overview of workflow managers for bioinformatics [@Wratten2021]. {#tbl-strengths}

Even more interesting is the accompanying GitHub repository ([GoekeLab/bioinformatics-workflows](https://github.com/GoekeLab/bioinformatics-workflows)), which contains a **Proof of Concept (PoC) RNA-seq workflow** implemented in the different workflow frameworks. These implementations were contributed and reviewed by the **developers of the respective frameworks** themselves!

![Wow! ;)](https://media2.giphy.com/media/v1.Y2lkPTc5MGI3NjExcDZ0bmhuNmx1c3Y4dmUzNDUwZTdrM3dwdHk3MXVjcXVtZnlrYzQ2cSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/J87jeioCWcipG/giphy.webp){}

Looking at these implementations, at first glance, one would think that the differences between the frameworks are minimal, and that the choice of framework is mostly a matter of personal preference.

### Comparing PoC Workflows to Community-Made Modules

However, comparing the POC workflows (_left_) to community-made modules (_right_), it becomes clear that **creating production-ready components** requires a lot more than **specifying a command's input and output files**.


:::{.column-page}

#### Nextflow

:::{.grid}

:::{.g-col-5}

Wratten et al. 2021 PoC ([Source](https://github.com/GoekeLab/bioinformatics-workflows/tree/master/nextflow)):

:::{.panel-tabset}

## `main.nf`

```{embed lang="groovy"}
examples/nextflow/wratten2021_poc/main.nf
```

:::

:::

:::{.g-col-7}

nf-core ([Source](https://github.com/nf-core/modules/tree/master/modules/nf-core/fastqc)):

:::{.panel-tabset}

## `environment.yml`

```{embed lang="yaml"}
examples/nextflow/nf-core/environment.yml
```

## `main.nf`

```{embed lang="groovy"}
examples/nextflow/nf-core/main.nf
```

## `meta.yaml`

```{embed lang="yaml"}
examples/nextflow/nf-core/meta.yml
```

## `tests/main.nf.test`

```{embed lang="groovy"}
examples/nextflow/nf-core/tests/main.nf.test
```

:::

:::

:::

:::



:::{.column-page}

#### Snakemake

:::{.grid}

:::{.g-col-5}

Wratten et al. 2021 PoC ([Source](https://github.com/GoekeLab/bioinformatics-workflows/tree/master/snakemake)):

:::{.panel-tabset}

## `fastqc.smk`

```{embed lang="python"}
examples/snakemake/wratten2021_poc/fastqc.smk
```

:::

:::

:::{.g-col-7}

snakemake-wrappers ([Source](https://github.com/snakemake/snakemake-wrappers/tree/master/bio/fastqc)):

:::{.panel-tabset}

## `environment.yaml`

```{embed lang="yaml"}
examples/snakemake/snakemake-wrappers/environment.yaml
```

## `meta.yaml`

```{embed lang="yaml"}
examples/snakemake/snakemake-wrappers/meta.yaml
```

## `wrapper.py`

```{embed lang="python"}
examples/snakemake/snakemake-wrappers/wrapper.py
```

## `test/Snakefile`

```{embed lang="python"}
examples/snakemake/snakemake-wrappers/test/Snakefile
```

:::

:::

:::

:::

:::{.column-page}

#### Toil + WDL

:::{.grid}

:::{.g-col-5}

Wratten et al. 2021 PoC ([Source](https://github.com/GoekeLab/bioinformatics-workflows/tree/master/wdl)):

:::{.panel-tabset}

## `fastqc.wdl`

```{embed lang="bash"}
examples/wdl/wratten2021_poc/fastqc.wdl
```

:::

:::

:::{.g-col-7}

BioWDL ([Source](https://github.com/biowdl/tasks/blob/develop/fastqc.wdl)):

:::{.panel-tabset}

## `fastqc.wdl` (Excerpt)

```{embed lang="bash"}
examples/wdl/biowdl/fastqc.wdl
```

## `fastqc.wdl` (Full)

```{embed lang="bash"}
examples/wdl/biowdl/fastqc_full.wdl
```

:::

:::

:::

:::


### Limitations of the study

However, the [Supplementary Table](https://static-content.springer.com/esm/art%3A10.1038%2Fs41592-021-01254-9/MediaObjects/41592_2021_1254_MOESM2_ESM.xlsx) shows that the comparison in [@tbl-strengths] was rather limited, since the score of each category was only based on a single criterion. Of the following categories, only "Scalability" was determined by more than one criterion:

* **Ease of Use**: Graphical interface with execution environment (score of 3), programming interface with in-built execution environment (score of 2), separated development and execution environment (score of 1).
* **Expressiveness**: Based on an existing programming language (3) or a new language or restricted vocabulary (2), primary interaction with graphical user interface (1).
* **Portability**:  Integration with three or more container and package manager platforms (3), two platforms are supported (2), one platform is supported (1).
* **Scalability**: Considers cloud support, scheduler and orchestration tool integration, and executor support. Please refer to Supplementary Table 1 - Sheet 2 (Scalability).
* **Learning resources**: Official tutorials, forums and events (3), tutorials and forums (2), tutorials or forums (1).
* **Pipelines Initiatives**: Community and curated (3), community or curated (2), not community or curated (1).

By comparing the example code of the respective workflow frameworks, it also becomes clear that we need not only look at example code of POC workflows, but actual production-ready workflows and pipelines. Such code often require a lot more functionality, including:

* Error handling
* Logging
* Data provenance
* Parameterization
* Testing
* Documentation
* Containerization
* Resource management


## Qualities of a Production-Ready Workflow

Building production-ready workflows for single-cell analysis involves integrating a variety of tools, technologies, and best practices. In order to meet the demands of large-scale data processing, reproducibility, and collaboration, a production-ready workflow should exhibit the following essential qualities ([@fig-qualities]):

![Essential qualities of a production-ready workflow.](images/qualities.svg){#fig-qualities}

* **Polyglot**: Seamlessly integrate tools and libraries from different programming languages, allowing you to leverage the strengths of each language for specific tasks. This facilitates the use of specialized tools and optimizes the analysis pipeline for performance and efficiency.
* **Modular**: A well-structured workflow should be composed of modular and reusable components, promoting code maintainability and facilitating collaboration. Each module should have a clear purpose and well-defined inputs and outputs, enabling easy integration and replacement of individual steps within the pipeline.
* **Scalable**: Single-cell datasets can be massive, and a production-ready workflow should be able to handle large volumes of data efficiently. This involves utilizing scalable compute environments, optimizing data storage and retrieval, and implementing parallelization strategies to accelerate analysis.
* **Reproducible**: Ensuring reproducibility is crucial for scientific rigor and validation. A production-ready workflow should capture all the necessary information, including code, data, parameters, and software environments, to enable others to replicate the analysis and obtain consistent results.
* **Portable**: The workflow should be designed to run seamlessly across different computing platforms and environments, promoting accessibility and collaboration. Containerization technologies like Docker can help achieve portability by encapsulating the workflow and its dependencies into a self-contained unit.
* **Community**: Leveraging community resources, tools, and best practices can accelerate the development of production-ready workflows. This is because developing high-quality components can at times be time-consuming, and sharing resources can help reduce duplication of effort and promote collaboration.
* **Maintainable**: A production-ready workflow should be well-documented, organized, and easy to understand, facilitating updates, modifications, and troubleshooting. Clear documentation of code, data, and parameters ensures that the workflow remains accessible and usable over time.

## Technologies for Production-Ready Workflows

The essential qualities of a production-ready workflow are achieved through a combination of enabling technologies ([@fig-technologies]). These technologies provide the foundation for building scalable, reproducible, and maintainable workflows for single-cell analysis.

![The essential qualities of a production-ready workflow are achieved through a combination of enabling technologies.](images/technologies.svg){#fig-technologies}


## Quality Assessment of Workflow Frameworks

```{r tibble, echo=FALSE, eval=FALSE}
library(tibble)
library(dplyr)
library(tidyr)
library(purrr)

googlesheets4::gs4_auth(email = "robrecht@data-intuitive.com")
wf_qc_sheet <- "https://docs.google.com/spreadsheets/d/1OMsnm8bIWc_Hp5neBOj7hWURT7ZL-L6qSbr0oAzmvSI"
wf_qc <- googlesheets4::read_sheet(wf_qc_sheet, sheet = "qc") |>
  mutate_each(function(x) {
    map_chr(x, function(y) {
      if (is.null(y)) {
        NA_character_
      } else {
        y
      }
    })
  })

wf_qc2 <- wf_qc

# fill in NAs due to merged cells
for (i in seq(2, nrow(wf_qc2), by = 1)) {
  if (is.na(wf_qc2$name[[i]])) {
    wf_qc2$name[[i]] <- wf_qc2$name[[i - 1]]
  }
  if (is.na(wf_qc$aspect[[i]])) {
    wf_qc2$aspect[[i]] <- wf_qc2$aspect[[i - 1]]
  }
}

# remove incomplete frameworks and unused questions
wf_qc2 <- wf_qc2 |>
  select(name, aspect, item, Nextflow, Snakemake, Galaxy, `Viash + Nextflow`, `Argo Workflows`) %>%
  filter(!is.na(Nextflow)) %>%
  filter(item != "The framework provides clear error messages and debugging tools.")

# sanitize string
string_to_colname <- function(x) {
  x <- tolower(x)
  x <- gsub(" ", "_", x)
  x <- gsub("[^[:alnum:]_]", "", x)
  x <- gsub("__", "_", x)
  x
}

# process metadata
meta <- wf_qc2 |>
  filter(name == "Metadata") |>
  select(-name, -aspect) |>
  as.matrix() |>
  t() |>
  as.data.frame() %>%
  rownames_to_column("project")
colnames(meta) <- string_to_colname(unname(unlist(meta[1,])))
meta <- meta[-1,]
rownames(meta) <- NULL
meta <- meta |>
  rename(project = item) |>
  mutate(id = string_to_colname(project))

# process criteria
criteria <- wf_qc2 |>
  filter(name != "Metadata") |>
  select(name, aspect, item)

# process scores
scores <- wf_qc2 |>
  filter(name != "Metadata") |>
  gather(key = "framework", value = "data", -item, -name, -aspect) %>%
  mutate(
    score = gsub("^([0-9\\.]*).*", "\\1", data) |> as.numeric(),
    url = gsub(".*(http[^ ]*)$", "\\1", data) %>% ifelse(. == data, NA_character_, .),
    explanation = gsub("^[0-9\\.]* ", "", data) %>% gsub("http[^ ]*$", "", .)
  )

agg <- scores |>
  group_by(name, framework) |>
  summarize(mean_score = mean(score), .groups = "drop")

# write to file
write.csv(meta, "notebooks/workflows/data/wf_metadata.csv", row.names = FALSE)
write.csv(criteria, "notebooks/workflows/data/criteria.csv", row.names = FALSE)
write.csv(agg, "notebooks/workflows/data/wf_aggregated_scores.csv", row.names = FALSE)
```

Given the abovementioned limitations, we decided to conduct our own quality assessment of workflow frameworks. This assessment is still largely in the works, but we're happy to share the preliminary results with you.

The data is based on a review of the documentation and community resources for each framework. We evaluated the frameworks based on the list of essential qualities mentioned in the previous section ([@fig-qualities]).

### Included frameworks

The following workflow frameworks were included in the assessment:

```{r wf_metadata, echo=FALSE, results="asis"}
meta <- read.csv("data/wf_metadata.csv")

for (i in seq_len(nrow(meta))) {
  project <- meta$project[[i]]
  project_description <- meta$project_description[[i]]
  project_website <- meta$project_website[[i]]
  project_repository <- meta$project_repository[[i]]
  documentation <- meta$documentation[[i]]
  doi <- meta$doi[[i]]
  community <- meta$community[[i]]
  community_repository <- meta$community_repository[[i]]

  strs <- c(
    paste0("[", project, "](", project_website, "): ",
    gsub("\\. *$", "", project_description), ".")
  )

  if (!is.na(project_repository)) {
    strs <- c(strs, paste0("[{{< fa brands github >}}](", project_repository, ")"))
  }

  if (!is.na(documentation)) {
    strs <- c(strs, paste0("[{{< fa circle-info >}}](", documentation, ")"))
  }

  if (!is.na(doi)) {
    strs <- c(strs, paste0("[{{< fa book-open >}}](", doi, ")"))
  }

  if (!is.na(community)) {
    strs <- c(strs, paste0("[{{< fa users >}}](", community, ")"))
  }

  cat(paste0(
    "- ", paste(strs, collapse = " "), "\n"
  ))
}
```

### Quality Assessment Criteria

The quality assessment was based on the following criteria:

:::{.panel-tabset}

```{r criteria, echo=FALSE, results="asis"}
criteria <- read.csv("data/criteria.csv")

unique_names <- unique(criteria$name)

for (name in sort(unique(criteria$name))) {
  crit <- criteria[criteria$name == name, , drop = FALSE]

  cat(paste0("#### ", name, " {.unnumbered}\n\n"))

  for (aspect in sort(unique(crit$aspect))) {
    asp <- crit[crit$aspect == aspect, , drop = FALSE]
    cat(paste0("**", aspect, "**\n\n"))

    for (item in asp$item) {
      cat(paste0(" * ", item, "\n"))
    }

    cat("\n\n")
  }
}
```

:::

These criteria and subsequent scores will be further refined and validated as part of our ongoing research.

### Quality Scores

The aggregated quality scores for each framework are shown below. The scores are based on the evaluation of the essential qualities of a production-ready workflow.

```{r plot_scores, echo=FALSE}
#| fig-cap: Quality scores for different workflow frameworks. 
#| fig-format: svg
library(ggplot2)

agg <- read.csv("data/wf_aggregated_scores.csv")
agg$framework <- factor(agg$framework, levels = sort(unique(agg$framework), decreasing = TRUE))

ggplot(agg) +
  geom_point(aes(mean_score, framework, color = framework)) +
  geom_segment(aes(x = 0, xend = mean_score, y = framework, yend = framework, color = framework), linewidth = 1) +
  facet_wrap(~name, ncol = 2) +
  theme_minimal() +
  theme(
    panel.grid.minor = element_blank(),
    panel.grid.major.y = element_blank(),
    legend.position = "none"
  ) +
  labs(
    x = NULL,
    y = NULL
  ) +
  scale_color_brewer(palette = "Set1")
```

Raw scores and detailed explanations behind the reasoning of the resulting scores can be found in the [Workflow Quality Assessment Spreadsheet](https://docs.google.com/spreadsheets/d/1OMsnm8bIWc_Hp5neBOj7hWURT7ZL-L6qSbr0oAzmvSI).

## Viash + Nextflow: A use-case

In the following sections, we will explore the use of [Viash](https://viash.io). Viash is a code generation tool that allows you to augment your scripts and Nextflow scripts with code generation.

Use case components:


#### Bash

:::{.grid}

:::{.g-col-6}

`src/load_data/config.vsh.yaml`:

```{embed lang="yaml"}
examples/viash_nextflow/src/load_data/config.vsh.yaml
```

:::

:::{.g-col-6}

`src/load_data/script.sh`:

```{embed lang="bash"}
examples/viash_nextflow/src/load_data/script.sh
```

:::

:::

#### Python

:::{.grid}

:::{.g-col-6}

`src/subset_obs/config.vsh.yaml`:

```{embed lang="yaml"}
examples/viash_nextflow/src/subset_obs/config.vsh.yaml
```

:::

:::{.g-col-6}

`src/subset_obs/script.py`:

```{embed lang="python"}
examples/viash_nextflow/src/subset_obs/script.py
```

:::

:::

#### R

:::{.grid}

:::{.g-col-6}

`src/compute_pseudobulk/config.vsh.yaml`:

```{embed lang="yaml"}
examples/viash_nextflow/src/compute_pseudobulk/config.vsh.yaml
```

:::

:::{.g-col-6}

`src/compute_pseudobulk/script.R`:
```{embed lang="r"}
examples/viash_nextflow/src/compute_pseudobulk/script.R
```

:::

:::

#### Nextflow

:::{.grid}

:::{.g-col-6}

`src/workflow/config.vsh.yaml`:

```{embed lang="yaml"}
examples/viash_nextflow/src/workflow/config.vsh.yaml
```

:::

:::{.g-col-6}

`src/workflow/main.nf`:

```{embed lang="groovy"}
examples/viash_nextflow/src/workflow/main.nf
```

:::

:::

### Running the workflow

To run the workflow, you must first build the project:

```bash
viash ns build --parallel --setup cachedbuild
```

    Exporting load_data =executable=> target/executable/load_data
    [notice] Building container 'polygloty_usecase/load_data:0.1.0' with Dockerfile
    Exporting load_data =nextflow=> target/nextflow/load_data
    Exporting compute_pseudobulk =executable=> target/executable/compute_pseudobulk
    [notice] Building container 'polygloty_usecase/compute_pseudobulk:0.1.0' with Dockerfile
    Exporting compute_pseudobulk =nextflow=> target/nextflow/compute_pseudobulk
    Exporting subset_obs =executable=> target/executable/subset_obs
    [notice] Building container 'polygloty_usecase/subset_obs:0.1.0' with Dockerfile
    Exporting subset_obs =nextflow=> target/nextflow/subset_obs
    Exporting subset_var =executable=> target/executable/subset_var
    [notice] Building container 'polygloty_usecase/subset_var:0.1.0' with Dockerfile
    Exporting subset_var =nextflow=> target/nextflow/subset_var
    Exporting differential_expression =executable=> target/executable/differential_expression
    [notice] Building container 'polygloty_usecase/differential_expression:0.1.0' with Dockerfile
    Exporting differential_expression =nextflow=> target/nextflow/differential_expression
    Exporting workflow =nextflow=> target/nextflow/workflow
    All 11 configs built successfully

Then, you can run the workflow:

```bash
nextflow run \
  target/nextflow/workflow/main.nf \
  -with-docker \
  --id dataset \
  --input s3://openproblems-bio/public/neurips-2023-competition/sc_counts_reannotated_with_counts.h5ad \
  --contrast 'sm_name;Belinostat;Dimethyl Sulfoxide' \
  --design_formula '~ sm_name + plate_name' \
  --publish_dir output
```

    N E X T F L O W  ~  version 23.10.0
    Launching `target/nextflow/workflow/main.nf` [condescending_engelbart] DSL2 - revision: f54b192abd
    executor >  local (6)
    [e2/da368b] process > workflow:wf:subset_sm_name:processWf:subset_sm_name_process (dataset)                   [100%] 1 of 1 ✔
    [d5/fea947] process > workflow:wf:subset_cell_type:processWf:subset_cell_type_process (dataset)               [100%] 1 of 1 ✔
    [23/a2b0a7] process > workflow:wf:subset_var:processWf:subset_var_process (dataset)                           [100%] 1 of 1 ✔
    [55/a59f07] process > workflow:wf:compute_pseudobulk:processWf:compute_pseudobulk_process (dataset)           [100%] 1 of 1 ✔
    [10/5c0650] process > workflow:wf:differential_expression:processWf:differential_expression_process (dataset) [100%] 1 of 1 ✔
    [91/5f7431] process > workflow:publishStatesSimpleWf:publishStatesProc (dataset)                              [100%] 1 of 1 ✔
    Completed at: 07-Sep-2024 21:33:16
    Duration    : 8m 59s
    CPU hours   : (a few seconds)
    Succeeded   : 6

The workflow will process the dataset, subset the data, compute pseudobulk samples, and perform differential expression analysis. The results will be saved in the `output` directory.

```bash
tree output
```

    output/
    ├── dataset.workflow.output.h5ad
    └── dataset.workflow.state.yaml

    1 directory, 2 files

The resulting pseudobulk samples and differential expression analysis results are stored in the `dataset.workflow.output.h5ad` file.

```python
import anndata as ad

adata = ad.read("output/dataset.workflow.output.h5ad")

adata
```

    AnnData object with n_obs × n_vars = 96 × 2000
        obs: 'sm_name', 'cell_type', 'plate_name', 'well'
        var: 'highly_variable', 'means', 'dispersions', 'dispersions_norm'
        varm: 'de_sm_name_belinostat_dimethyl_sulfoxide'

```python
adata.varm["de_sm_name_belinostat_dimethyl_sulfoxide"]
```

              baseMean  log2FoldChange     lfcSE      stat        pvalue          padj
    A2M      12.320857       -0.696714  0.113490 -6.138993  8.304620e-10  2.699001e-09
    A2M-AS1  31.563323       -0.652155  0.074559 -8.746780  2.195265e-18  1.065788e-17
    A2MP1     1.712214       -0.844991  0.262029 -3.224806  1.260582e-03  2.551988e-03
    AARD      0.125909        0.086614  1.234496  0.070161  9.440653e-01           NaN
    ABCA1     2.003082       -0.154770  0.240886 -0.642504  5.205457e-01  6.193785e-01
    ...            ...             ...       ...       ...           ...           ...
    ZNF860    0.024027        0.214981  2.915260  0.073743  9.412147e-01           NaN
    ZNF876P   0.568178       -0.703739  0.422580 -1.665340  9.584497e-02  1.486826e-01
    ZNF891   28.045500        0.515844  0.066416  7.766889  8.043700e-15  3.338454e-14
    ZNF92    46.860620        0.076892  0.048781  1.576261  1.149657e-01  1.743887e-01
    ZNF99     0.000000             NaN       NaN       NaN           NaN           NaN

    [2000 rows x 6 columns]

## Best Practices

To ensure that your workflow is production-ready, consider the following best practices:

* **Version control**: Use a version control system like Git to track changes to your workflow code and configuration files. This allows you to collaborate with others, revert to previous versions, and maintain a history of your work.

* **Automated testing**: Implement automated tests to validate the correctness of your workflow components and ensure that changes do not introduce regressions. This includes unit tests, integration tests, and end-to-end tests.

* **Continuous integration**: Set up a continuous integration (CI) pipeline to automatically build, test, and deploy your workflow whenever changes are made to the codebase. This helps catch errors early and ensures that your workflow remains functional.

* **Documentation**: Document your workflow code, configuration, and usage to make it easier for others to understand and use your workflow. Include information on how to run the workflow, what inputs are required, and what outputs are produced.

* **Containerization**: Use containerization technologies like Docker to package your workflow and its dependencies into a self-contained unit. This ensures that your workflow runs consistently across different environments and platforms.

* **Resource management**: Optimize the use of computational resources by parallelizing tasks, optimizing memory usage, and monitoring resource consumption. This helps improve the performance and scalability of your workflow.

* **Error handling**: Implement robust error handling mechanisms to gracefully handle failures and exceptions during workflow execution. This includes logging errors, retrying failed tasks, and notifying users of issues.

* **Data provenance**: Capture metadata about the inputs, outputs, and parameters of your workflow to enable reproducibility and traceability. This includes recording information about the data sources, processing steps, and results produced by the workflow.

* **Versioned releases**: Create versioned releases of your workflow and accompanying **container images** to ensure that users can reproduce the exact results of a specific analysis. This involves tagging releases, documenting changes, and archiving previous versions.

## Conclusion

Quality assessment contributors:

* Jakub Majerčík
* Michaela Müller 
* Robrecht Cannoodt
* Toni Verbeiren
