---
title: Disk-based pipelines
engine: knitr
---

Disk-based pipelines are a way to run a sequence of scripts in different languages by communicating only via files on disk. This is useful when you have to use different languages for different parts of the analysis and don't want the hassle of managing an [in-memory interoperability](../in_memory/) setup. Either you keep the notebook format of the exploratory analysis or you convert the code snippets to scripts.

## Notebook pipelines

You can use [Quarto](https://quarto.org/) to run code snippets in different languages in the same `.qmd` notebook. **Our [Use-case chapter](./usecase/) is one example of this.**. The polyglot powers of Quarto for [executing code blocks](https://quarto.org/docs/computations/execution-options.html) come from tools such as [knitr](https://yihui.org/knitr/) or [Jupyter kernels](https://docs.jupyter.org/en/latest/projects/kernels.html). You don't need [in-memory interoperability](../in_memory), but it

There are frameworks that keep the notebooks and create a pipeline with it. The upside is that you can avoid converting the code snippets in the notebooks to scripts. The downside is that you have to use a specific framework and the notebooks can become very large and unwieldy. For example, [Papermill](https://github.com/nteract/papermill) can execute parameterized Jupyter notebooks in sequence and pass variables between them. [nbdev](https://nbdev.fast.ai/) and [Ploomber](https://github.com/ploomber/ploomber) are similar such frameworks.

### Execute notebooks via the CLI

You can execute notebooks via the command line as a job, so you don't have to wait for them to finish. This is useful when running experiments, as the output is saved in the notebook together with plots, available to inspect them at a later time. 

Jupyter notebook via [nbconvert](https://nbconvert.readthedocs.io/en/latest/#):
```bash
jupyter nbconvert --to notebook --execute my_notebook.ipynb --allow-errors --output-dir outputs/
```

Similar functionality exists for [RMarkdown](https://rmarkdown.rstudio.com/) notebooks via Rscript:
```bash
Rscript -e "rmarkdown::render('my_notebook.Rmd',params=list(args = myarg))"
```

## Script pipelines

The scripts in a script pipeline are a collection of the code snippets from the notebooks and can be written in different languages and executed in sequence.

These scripts have the same functionality as the code in the [use-case example notebook](../usecase/):

:::{.panel-tabset}

# `1_load_data.sh`

```{embed lang="bash"}
scripts/1_load_data.sh
```

# `2_compute_pseudobulk.py`

```{embed lang="python"}
scripts/2_compute_pseudobulk.py
```

# `3_analysis_de.R`

```{embed lang="r"}
scripts/3_analysis_de.R
```

:::

You can use a shell script or your language of preference to run the pipeline in a sequential manner. This usually requires all the dependencies to be installed in one large environment.

From Bash:
```bash
#!/bin/bash

bash scripts/1_load_data.sh
python scripts/2_compute_pseudobulk.py
Rscript scripts/3_analysis_de.R
```

From R:
```r
system("bash scripts/1_load_data.sh")
system("python scripts/2_compute_pseudobulk.py")
# Alternatively you can run R code here instead of calling an R script
system("Rscript scripts/3_analysis_de.R")
```

From Python:
```python
import subprocess

subprocess.run("bash scripts/1_load_data.sh", shell=True)
# Alternatively you can run Python code here instead of calling a Python script
subprocess.run("python scripts/2_compute_pseudobulk.py", shell=True)
subprocess.run("Rscript scripts/3_analysis_de.R", shell=True)
```

## Pipelines with different environments

Sometimes you might want to run scripts in different environments, as it's too much hassle to install all dependencies in one environment. Other reasons can be that you want to reuse existing container images or you want keep parts of the pipeline separate and maintainable.

You can **interleave your Bash script with environment activation functions** e.g. `conda activate {script_env}` commands. This requires e.g. a `conda .yaml` file for each script environment in order to be reproducible. This can grow unwieldy as Conda environment sometimes need to be adapted for the platform and availability of accelerators like GPU's. Another important consideration is that packages that impact the on-disk data format should be the same version across environments.

A simple and elegant, but very new solution is to use the **[Pixi package managment tool](https://pixi.sh/latest/)** to manage the environments and tasks for you. The environments can be composed from multiple features containing dependencies, so you can have a `scverse` environment with only Python, a `rverse` environment with only R and even an `all` environment with both by adding the respective features (if such an environment is resolvable at least).

Run scripts in different environments with `pixi`:
```bash
pixi run -e bash scripts/1_load_data.sh
pixi run -e scverse scripts/2_compute_pseudobulk.py
pixi run -e rverse scripts/3_analysis_de.R
```

With the Pixi task runner, you can define these tasks in their respective environments, make them dependant on each other and run them in a single command.

```bash
pixi run pipeline
```

You can create a Docker image with all the `pixi` environments and run the pipeline in one containerized environment. The image is ~5GB and the pipeline can require a lot of working memory ~20GB, so make sure to increase the RAM allocated to Docker in your settings. Note that the `usecase/data/` and `scripts/` folders are mounted to the Docker container, so you can interactively edit the scripts and access the data.

```bash
docker pull berombau/polygloty-docker:latest
docker run -it -v $(pwd)/usecase/data:/app/usecase/data -v $(pwd)/scripts:/app/scripts berombau/polygloty-docker:latest pixi run pipeline
```

Another approach is to use **multi-package containers**. Tools like [Multi-Package BioContainers](https://midnighter.github.io/mulled/) and [Seqara Wave Containers](https://seqera.io/containers/) can make this quick and easy.

When your project grows more bespoke you can use a **[workflow frameworks](../workflow_frameworks)** like Nextflow or Snakemake to manage the environments and dependencies for you.
