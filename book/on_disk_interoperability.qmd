---
title: On-disk interoperability
engine: knitr
---

On-disk interoperability is a strategy for achieving interoperability between tools written in different programming languages by storing intermediate results in standardized, language-agnostic file formats. This approach allows for sequential execution of scripts written in different languages, enabling researchers to leverage the best tools for each analysis step.

The upside of this approach is that it is relatively simple as the scripts are mostly unchanged, only prepended with a reading operation and appended with writing operation. Each script can be written and tested independently in their suitable respective frameworks. This modular polyglotism of on-disk interoperability is one of the key strengths of workflow languages like Nextflow and Snakemake.

The downside is that it can lead to increased storage requirements and I/O overhead which grows with the amount of scripts. As disk serialization and deserialization can be much slower than memory operations, this SerDe problem can become a problem for very large datasets.

Debugging is only possible for individual scripts and the workflow in not as interactive and explorative as the in-memory approach.

## Different file formats

It's important to differentiate between language specific file formats and language agnostic file formats. For example, most languages can serialize objects to disk. R has the `.RDS` file format and Python has the `.pickle/.pkl` file format, but these are not interoperable.

While the file formats themselves can be backwards compatible and you can read in old objects, it's not necessarily the case newer versions of some package can read in files from older versions e.g. SeuratObject versions. Another possible problem is that when working with older versions of Python or R you can have errors reading in serialized objects created by the latest language version.

For a file format to solve these issues and be language agnostic, it should have a mature standard describing how the data is stored on disk. This standard should be implemented in multiple languages, documented and tested for compatibility. Some file formats have a reference implementation in C, which can be used to create bindings for other languages. Most also have a status page that list the implementations and which version or how much of the standard they support.

Note that there can be a difference in-memory layouts and on-disk layouts of the same object. [Apache Arrow](https://arrow.apache.org/docs/index.html) is a memory layout standard important for in-memory interoperability. It can be save to disk as a Feather file, but also as a Parquet file.

### Non-exhaustive list of file formats

[@tbl-file-formats] shows some of the features of file formats of interest for Python and R.

- **Fast compression** is important for large datasets and most of these formats support it, as it can reduce the storage requirements. Standard compressions techniques like zip can always be applied to text-based formats like CSV and JSON, but standardized support within the file format leads to faster and higher compression performance.
- **Sparse matrix** support is important for single-cell data, as it can also greatly reduce the storage requirements. If a part of the pipeline does not have this support and needs to make the matrix dense, the memory requirements will increase rapidly. 
- Support for **raster data** like microscopy images is important for bioimaging and spatial omics datasets. For large datasets and whole-slide images, support for subsetting and **lazy loading of chunks** or tiles is also needed. This also allows for out-of-core processing of larger-than-memory datasets.
- Some file formats have built-in support for **remote storage** like a web server or S3. This can be important for large datasets that do not fit on a local disk or for distributed computing. This can also be used to share data between collaborators without having to download the whole dataset.

The lower section of the table shows some specialized file standards that are built on top of the more general file formats e.g. the standardized way AnnData serializes a sparse matrix within Zarr to support large single cell data.

| File Format | Python | R | Sparse matrix | Raster data | Lazy chunk loading | Remote storage |
|-------------|--------|---|---------------|-------------|--------------------|----------------|
| RDS         | ○○     | ●●|    ○○       | ●○          | ○○                      | ○○ |
| Pickle      | ●●     | ○○|    ○○       | ●○          | ○○                      | ○○ |
| CSV         | ●●     | ●●|    ○○       | ○○          | ○○                      | ○○ |
| JSON        | ●●     | ●●|    ○○       | ○○          | ○○                      | ○○ |
| TIFF        | ●●     | ●●|    ○○       | ●●          | ●●                      | ●○ |
| [Numpy .npy](https://numpy.org/doc/stable/reference/generated/numpy.lib.format.html#module-numpy.lib.format) | ●●                | ○○ |  ○○    | ●●          | ○○                      | ○○ |
| [Parquet](https://parquet.apache.org/) | ●● | ●● | ○○ | ○○ | ●● | ●● |
| [Feather](https://arrow.apache.org/docs/python/feather.html) | ●● | ●● | ●● | ○○ | ●● | ●● |
| [Lance](https://github.com/lancedb/lance) | ●● | ○○ | ●● | ○○ | ●● | ●●
| [HDF5](https://www.hdfgroup.org/) | ●● | ●● | ○○ | ●● | ●● | ●○ |
| [Zarr](https://zarr.readthedocs.io/en/stable/) | ●● | ●● | ○○ | ●● | ●● | ●● |
| [TileDB](https://tiledb.io/) | ●● | ●● | ●● | ●● | ●● | ●● |
||
| [Seurat RDS](https://satijalab.org/seurat/) | ○○ | ●● | ○○ | ●○ | ○○ | ○○
| [Indexed OME-TIFF](http://viv.gehlenborglab.org/#data-preparation) | ●● | ●● | ○○ | ●● | ●● | ●● |
| [h5Seurat](https://mojaveazure.github.io/seurat-disk/index.html) | ●● | ●● | ○○ | ●○ | ●● | ●○ |
| [Loom HDF5](https://loompy.org/) | ●● | ●● | ●● | ●● | ●○ | ●○ |
| [AnnData h5ad](https://anndata.readthedocs.io/en/latest/anndata.zarr.html) | ●● | ●● | ●● | ●○ | ●● | ●○ |
| [AnnData Zarr](https://anndata.readthedocs.io/en/latest/anndata.zarr.html) | ●● | ●● | ●● | ●○ | ●● | ●● |
| [TileDB-SOMA](https://tiledb.com/open-source/life-sciences) | ●● | ●● | ●● | ●○ | ●● | ●● |
| [TileDB-BioImaging](https://tiledb.com/open-source/life-sciences) | ●● | ●● | ●○ | ●● | ●● | ●● |
| [SpatialData Zarr](https://spatialdata.scverse.org/en/stable/) | ●● | ●● | ●● | ●● | ●● | ●● |

: Overview of file formats for single cell. {#tbl-file-formats}

## Different on-disk pipelines

You can use a shell script to run the pipeline in a sequential manner. This requires all the dependencies to be installed in one large environment.

Usually you start in a notebook with an exploratory analysis, then move to a script for reproducibility and finally to a pipeline for scalability.

The scripts in such a script pipeline are a collection of the code snippets from the notebooks and can be written in different languages and executed in sequence.

Alternatively, there are frameworks that keep the notebooks and create a pipeline with it. The upside is that you can avoid converting the code snippets in the notebooks to scripts. The downside is that you have to use a specific framework and the notebooks can become very large and unwieldy.

## Notebook pipelines

You can use [Quarto](https://quarto.org/) to run code snippets in different languages in the same `.qmd` notebook. Our [Use-case chapter](./usecase/) is one example of this. The polyglot powers of Quarto for [executing code blocks](https://quarto.org/docs/computations/execution-options.html) come from tools such as [knitr](https://yihui.org/knitr/) or [Jupyter kernels](https://docs.jupyter.org/en/latest/projects/kernels.html).

It is possible to create whole pipelines out of multiple notebooks. For example, [Papermill](https://github.com/nteract/papermill) can execute parameterized Jupyter notebooks in sequence and pass variables between them. [nbdev](https://nbdev.fast.ai/) and [Ploomber](https://github.com/ploomber/ploomber) are similar such frameworks.

### Execute notebooks via the CLI

Jupyter notebook via [nbconvert](https://nbconvert.readthedocs.io/en/latest/#):
```bash
jupyter nbconvert --to notebook --execute my_notebook.ipynb --allow-errors --output-dir outputs/
```

[RMarkdown](https://rmarkdown.rstudio.com/) notebook via Rscript:
```bash
Rscript -e "rmarkdown::render('my_notebook.Rmd',params=list(args = myarg))"
```

## Script pipelines

### Calling scripts in the same environment

TODO: test these snippets

From Bash:
```bash
#!/bin/bash

bash scripts/1_load_data.sh
python scripts/2_compute_pseudobulk.py
Rscript scripts/3_plot_results.R
```

From R:
```r
system("bash scripts/1_load_data.sh")
system("python scripts/2_compute_pseudobulk.py")
system("Rscript scripts/3_plot_results.R")
```

From Python:
```python
import subprocess

subprocess.run("bash scripts/1_load_data.sh", shell=True)
subprocess.run("python scripts/2_compute_pseudobulk.py", shell=True)
subprocess.run("Rscript scripts/3_plot_results.R", shell=True)
```

### Calling scripts in different environments

Sometimes you might want to run scripts in different environments, as they it's too much hassle to install all dependencies in one environment, you want to reuse existing ones or you want keep them separate and maintainable.

You can interleave your Bash script with environment activation functions e.g. `conda activate {script_env}` commands. This requires a `conda .yaml` file for each script environment in order to be reproducible. An important consideration is that packages that impact the on-disk data format should be the same version across environments.

Alternatively, you can use a workflow manager like Nextflow or Snakemake to manage the environments and dependencies for you. A interesting, but very new approach is to use the [Pixi package managment tool](https://pixi.sh/latest/) to manage the environments and tasks for you. The environments can be composed from multiple features containing dependencies, so you can have a `scverse` environment with only Python, a `rverse` environment with only R and even an `all` environment with both by adding the respective features (if such an environment is resolvable at least).

Run scripts in different environments with `pixi`:
```bash
pixi run -e bash scripts/1_load_data.sh
pixi run -e scverse scripts/2_compute_pseudobulk.py
pixi run -e rverse scripts/3_plot_results.R
```

With the Pixi task runner, you can define these tasks in their respective environments, make them dependant on each other and run them in a single command.

```bash
pixi run pipeline
```

You can create a Docker image with all the `pixi` environments and run the pipeline in one containerized environment. The image is ~5GB and the pipeline can require a lot of working memory ~20GB, so make sure to increase the RAM allocated to Docker in your settings. Note that the `usecase/data/` and `scripts/` folders are mounted to the Docker container, so you can interactively edit the scripts and access the data.

```bash
docker pull berombau/polygloty-docker:latest
docker run -it -v $(pwd)/usecase/data:/app/usecase/data -v $(pwd)/scripts:/app/scripts berombau/polygloty-docker:latest pixi run pipeline
```
