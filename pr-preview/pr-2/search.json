[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Polyglot programming for single-cell analysis",
    "section": "",
    "text": "Preface\nThis book is a collection of notebooks and explanations for the workshop on Polyglot programming for single-cell analysis given at the scverse Conference 2024. For more information, please visit the workshop page.\nIn order to use the best performing methods for each step of the single-cell analysis process, bioinformaticians need to use multiple ecosystems and programming languages. This is unfortunately not that straightforward. This workshop gives an overview of the different levels of interoperability, and how it is possible to integrate them in a single workflow.\nTo get started, read the Introduction chapter.\nTo learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "book/introduction.html",
    "href": "book/introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Code porting\nPorting tools from one language to another can offer complete control and eliminate interoperability concerns. However, one should not underestimate the effort required to reimplement complex algorithms, and the risk of introducing errors.\nFurthermore, work is not done after the initial port – in order for the researcher’s work to be useful to others, the ported code must be maintained and kept up-to-date with the original implementation. For this reason, we don’t consider reimplementation a viable option for most use-cases and will not discuss it further in this book.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "book/introduction.html#in-memory-interoperability",
    "href": "book/introduction.html#in-memory-interoperability",
    "title": "1  Introduction",
    "section": "1.2 In-memory interoperability",
    "text": "1.2 In-memory interoperability\nTools like rpy2 and reticulate allow for direct communication between languages within a single analysis session. This approach provides flexibility and avoids intermediate file I/O, but can introduce complexity in managing dependencies and environments.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "book/introduction.html#disk-based-interoperability",
    "href": "book/introduction.html#disk-based-interoperability",
    "title": "1  Introduction",
    "section": "1.3 Disk-based interoperability",
    "text": "1.3 Disk-based interoperability\nStoring intermediate results to disk in standardized, language-agnostic file formats (e.g., HDF5, Parquet) allows for sequential execution of scripts written in different languages. This approach is relatively simple but can lead to increased storage requirements and I/O overhead.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "book/introduction.html#workflow-frameworks",
    "href": "book/introduction.html#workflow-frameworks",
    "title": "1  Introduction",
    "section": "1.4 Workflow frameworks",
    "text": "1.4 Workflow frameworks\nWorkflow management systems (e.g., Nextflow, Snakemake) provide a structured approach to orchestrate complex, multi-language pipelines, enhancing reproducibility and automation. However, they may require a learning curve and additional configuration.\n\n\n\n\nHeumos, Lukas, Anna C. Schaar, Christopher Lance, Anastasia Litinetskaya, Felix Drost, Luke Zappia, Malte D. Lücken, et al. 2023. “Best Practices for Single-Cell Analysis Across Modalities.” Nature Reviews Genetics 24 (8): 550–72. https://doi.org/10.1038/s41576-023-00586-w.\n\n\nZappia, Luke, and Fabian J. Theis. 2021. “Over 1000 Tools Reveal Trends in the Single-Cell RNA-Seq Analysis Landscape.” Genome Biology 22 (1). https://doi.org/10.1186/s13059-021-02519-4.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "book/usecase/index.html",
    "href": "book/usecase/index.html",
    "title": "2  Use-case",
    "section": "",
    "text": "2.1 1. Retrieving the data\nThe dataset has since been uploaded to SRA (“SRA SRP527159” 2024), will be uploaded to GEO, and is currently available from S3 (“OP3 H5AD on S3” 2024).\nIf you haven’t already, you can download the dataset from S3 using the following command:\nif [[ ! -f data/sc_counts_reannotated_with_counts.h5ad ]]; then\n  aws s3 cp \\\n    --no-sign-request \\\n    s3://openproblems-bio/public/neurips-2023-competition/sc_counts_reannotated_with_counts.h5ad \\\n    data/sc_counts_reannotated_with_counts.h5ad\nfi",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Use-case</span>"
    ]
  },
  {
    "objectID": "book/usecase/index.html#loading-the-data",
    "href": "book/usecase/index.html#loading-the-data",
    "title": "2  Use-case",
    "section": "2.2 2. Loading the data",
    "text": "2.2 2. Loading the data\nThe dataset is stored in an AnnData object, which can be loaded in Python as follows:\n\nimport anndata as ad\n\nadata = ad.read_h5ad(\"data/sc_counts_reannotated_with_counts.h5ad\")\n\nadata\n\nAnnData object with n_obs × n_vars = 298087 × 21265\n    obs: 'dose_uM', 'timepoint_hr', 'well', 'row', 'col', 'plate_name', 'cell_id', 'cell_type', 'split', 'donor_id', 'sm_name', 'control', 'SMILES', 'sm_lincs_id', 'library_id', 'leiden_res1', 'group', 'cell_type_orig', 'plate_well_celltype_reannotated', 'cell_count_by_well_celltype', 'cell_count_by_plate_well'\n    var: 'highly_variable', 'means', 'dispersions', 'dispersions_norm'\n    uns: 'cell_type_colors', 'celltypist_celltype_colors', 'donor_id_colors', 'hvg', 'leiden_res1_colors', 'log1p', 'neighbors', 'over_clustering', 'rank_genes_groups'\n    obsm: 'HTO_clr', 'X_pca', 'X_umap', 'protein_counts'\n    obsp: 'connectivities', 'distances'\n\n\nThe same code can be run in R using the anndata package (not run):\nlibrary(anndata)\n\nadata &lt;- read_h5ad(\"data/sc_counts_reannotated_with_counts.h5ad\")\n\nadata",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Use-case</span>"
    ]
  },
  {
    "objectID": "book/usecase/index.html#subset-data",
    "href": "book/usecase/index.html#subset-data",
    "title": "2  Use-case",
    "section": "2.3 3. Subset data",
    "text": "2.3 3. Subset data\nSince the dataset is large, we will subset the data to a single small molecule, control, and cell type.\n\nsm_name = \"Belinostat\"\ncontrol_name = \"Dimethyl Sulfoxide\"\ncell_type = \"T cells\"\n\nadata = adata[\n  adata.obs[\"sm_name\"].isin([sm_name, control_name]) &\n  adata.obs[\"cell_type\"].isin([cell_type]),\n].copy()\n\nadata.write_h5ad(\"data/sc_counts_subset.h5ad\")\n\nWe will also subset the genes to the top 2000 most variable genes.\n\nadata = adata[:, adata.var[\"highly_variable\"]].copy()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Use-case</span>"
    ]
  },
  {
    "objectID": "book/usecase/index.html#compute-pseudobulk",
    "href": "book/usecase/index.html#compute-pseudobulk",
    "title": "2  Use-case",
    "section": "2.4 4. Compute pseudobulk",
    "text": "2.4 4. Compute pseudobulk\n\nimport pandas as pd\n\nCombine data in a single data frame and compute pseudobulk\n\ncombined = pd.DataFrame(\n  adata.X.toarray(),\n  index=adata.obs[\"plate_well_celltype_reannotated\"],\n)\ncombined.columns = adata.var_names\npb_X = combined.groupby(level=0).sum()\n\n&lt;string&gt;:1: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\nConstruct obs for pseudobulk. Use ‘plate_well_celltype_reannotated’ as index and make sure to retain the columns ‘sm_name’, ‘cell_type’, and ‘plate_name’:\n\npb_obs = adata.obs[[\"sm_name\", \"cell_type\", \"plate_name\", \"well\"]].copy()\npb_obs.index = adata.obs[\"plate_well_celltype_reannotated\"]\npb_obs = pb_obs.drop_duplicates()\n\nCreate AnnData object:\n\npb_adata = ad.AnnData(\n  X=pb_X.loc[pb_obs.index].values,\n  obs=pb_obs,\n  var=adata.var,\n)\n\nStore to disk:\n\npb_adata.write_h5ad(\"data/pseudobulk.h5ad\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Use-case</span>"
    ]
  },
  {
    "objectID": "book/usecase/index.html#compute-de",
    "href": "book/usecase/index.html#compute-de",
    "title": "2  Use-case",
    "section": "2.5 5. Compute DE",
    "text": "2.5 5. Compute DE\n\nlibrary(anndata)\nlibrary(dplyr, warn.conflicts = FALSE)\n\npb_adata &lt;- read_h5ad(\"data/pseudobulk.h5ad\")\n\nSelect small molecule and control:\n\nsm_name &lt;- \"Belinostat\"\ncontrol_name &lt;- \"Dimethyl Sulfoxide\"\n\nCreate DESeq dataset:\n\n# transform counts matrix\ncount_data &lt;- t(pb_adata$X)\nstorage.mode(count_data) &lt;- \"integer\"\n\n# create dataset\ndds &lt;- DESeq2::DESeqDataSetFromMatrix(\n  countData = count_data,\n  colData = pb_adata$obs,\n  design = ~ sm_name + plate_name,\n)\n\nWarning: replacing previous import 'S4Arrays::makeNindexFromArrayViewport' by\n'DelayedArray::makeNindexFromArrayViewport' when loading 'SummarizedExperiment'\n\n\n  Note: levels of factors in the design contain characters other than\n  letters, numbers, '_' and '.'. It is recommended (but not required) to use\n  only letters, numbers, and delimiters '_' or '.', as these are safe characters\n  for column names in R. [This is a message, not a warning or an error]\n\n\nRun DESeq2:\n\ndds &lt;- DESeq2::DESeq(dds)\n\nestimating size factors\n\n\n  Note: levels of factors in the design contain characters other than\n  letters, numbers, '_' and '.'. It is recommended (but not required) to use\n  only letters, numbers, and delimiters '_' or '.', as these are safe characters\n  for column names in R. [This is a message, not a warning or an error]\n\n\nestimating dispersions\n\n\ngene-wise dispersion estimates\n\n\nmean-dispersion relationship\n\n\n  Note: levels of factors in the design contain characters other than\n  letters, numbers, '_' and '.'. It is recommended (but not required) to use\n  only letters, numbers, and delimiters '_' or '.', as these are safe characters\n  for column names in R. [This is a message, not a warning or an error]\n\n\nfinal dispersion estimates\n\n\nfitting model and testing\n\n\n  Note: levels of factors in the design contain characters other than\n  letters, numbers, '_' and '.'. It is recommended (but not required) to use\n  only letters, numbers, and delimiters '_' or '.', as these are safe characters\n  for column names in R. [This is a message, not a warning or an error]\n\n\nGet results:\n\nres &lt;- DESeq2::results(dds, contrast=c(\"sm_name\", sm_name, control_name)) |&gt;\n  as.data.frame()\n\nPreview results:\n\nres |&gt;\n  arrange(padj) |&gt;\n  head(10)\n\n          baseMean log2FoldChange      lfcSE      stat        pvalue\nBEX5      59.24944       2.187350 0.05660399  38.64304  0.000000e+00\nHIST1H1D 301.38741       1.356543 0.03092962  43.85901  0.000000e+00\nSTMN1    234.72112       2.224633 0.04104002  54.20642  0.000000e+00\nPCSK1N    64.91604       1.899149 0.05480612  34.65214 4.147855e-263\nGZMM     141.39238      -1.309959 0.03806665 -34.41224 1.654371e-259\nMARCKSL1  95.82726       1.423057 0.04311798  33.00380 7.163953e-239\nH1FX     376.28247       1.054890 0.03221858  32.74168 3.988563e-235\nHIST1H1B  30.81805       4.317984 0.14074738  30.67896 1.086254e-206\nFXYD7     61.11526       2.331406 0.07725771  30.17700 4.746707e-200\nING2      79.68893       1.218777 0.04336609  28.10437 8.663682e-174\n                  padj\nBEX5      0.000000e+00\nHIST1H1D  0.000000e+00\nSTMN1     0.000000e+00\nPCSK1N   1.631144e-260\nGZMM     5.204651e-257\nMARCKSL1 1.878150e-236\nH1FX     8.962871e-233\nHIST1H1B 2.135848e-204\nFXYD7    8.296189e-198\nING2     1.362797e-171\n\n\nWrite to disk:\n\nwrite.csv(res, \"data/de_contrasts.csv\")\n\n\n\n\n\n“OP3 H5AD on S3.” 2024. https://openproblems-bio.s3.amazonaws.com/public/neurips-2023-competition/sc_counts_reannotated_with_counts.h5ad.\n\n\n“Open Problems Kaggle Competition - Single Cell Perturbations.” 2023. https://www.kaggle.com/competitions/open-problems-single-cell-perturbations/overview.\n\n\n“OpenProblems Perturbation Prediction Benchmark.” 2024. https://openproblems.bio/results/perturbation_prediction/.\n\n\n“SRA SRP527159.” 2024. https://trace.ncbi.nlm.nih.gov/Traces/?view=study&acc=SRP527159.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Use-case</span>"
    ]
  },
  {
    "objectID": "book/in_memory/pitfalls.html",
    "href": "book/in_memory/pitfalls.html",
    "title": "3  Pitfalls when using both Python and R",
    "section": "",
    "text": "3.1 Column major vs row major\nMatrices are stored contiguously in-memory, and are adressed by a single memory addresses, instead of multiple indices along the axis. A translation needs to happen between this single adress and the indices along the axes, and how that translation happens depens on how the matrix is represented in-memory.\nThe figure below illustrates the most common ways a matrix is stored in memory, column major and row major order.\nIn R, every dense matrix is represented in column major order. In Python, the standard is row major, but you can specify column major order as well. There is usually no issue when converting R matrices to Python matrices: reticulate will take care to present these as column major Python matrices. The reverse is not true: all dense (even row major) Python matrices are presented to R as column major matrices.\nIf you notice something amiss with your matrices, check whether you need to transpose them or change the row/column major attribute.",
    "crumbs": [
      "In-memory interoperability",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Pitfalls</span>"
    ]
  },
  {
    "objectID": "book/in_memory/pitfalls.html#column-major-vs-row-major",
    "href": "book/in_memory/pitfalls.html#column-major-vs-row-major",
    "title": "3  Pitfalls when using both Python and R",
    "section": "",
    "text": "Figure 3.1: Different in-memory representations",
    "crumbs": [
      "In-memory interoperability",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Pitfalls</span>"
    ]
  },
  {
    "objectID": "book/in_memory/pitfalls.html#indexing-0-based-or-1-based",
    "href": "book/in_memory/pitfalls.html#indexing-0-based-or-1-based",
    "title": "3  Pitfalls when using both Python and R",
    "section": "3.2 Indexing: 0-based or 1-based",
    "text": "3.2 Indexing: 0-based or 1-based\nTake care to remember that arrays and matrices in Python are indexed starting from 0 (as in, index 0 refers to the first element), while R uses 1-based indexing, where index 1 refers to the first element.\n\n\n\n\n\n\nFigure 3.2: 0-based vs 1-based indexing",
    "crumbs": [
      "In-memory interoperability",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Pitfalls</span>"
    ]
  },
  {
    "objectID": "book/in_memory/pitfalls.html#dots-in-variable-names",
    "href": "book/in_memory/pitfalls.html#dots-in-variable-names",
    "title": "3  Pitfalls when using both Python and R",
    "section": "3.3 Dots in variable names",
    "text": "3.3 Dots in variable names\nIn R it is very common to use dots in symbols and variable names. This is invalid in Python: dots are used for function calls.\nWhen using rpy2, these dots are usually translated to underscores _. If this translation can result in errors, this does not happen automatically. In this case, you can specify mappings for these symbols.\n\nfrom rpy2.robjects.packages import importr\n\n/home/runner/work/polygloty/polygloty/renv/python/virtualenvs/renv-python-3.12/lib/python3.12/site-packages/rpy2/rinterface_lib/embedded.py:276: UserWarning: R was initialized outside of rpy2 (R_NilValue != NULL). Trying to use it nevertheless.\n  warnings.warn(msg)\nR was initialized outside of rpy2 (R_NilValue != NULL). Trying to use it nevertheless.\n\n\nd = {'package.dependencies': 'package_dot_dependencies',\n     'package_dependencies': 'package_uscore_dependencies'}\ntools = importr('tools', robject_translations = d)",
    "crumbs": [
      "In-memory interoperability",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Pitfalls</span>"
    ]
  },
  {
    "objectID": "book/in_memory/pitfalls.html#integers-and-floating-point-numbers",
    "href": "book/in_memory/pitfalls.html#integers-and-floating-point-numbers",
    "title": "3  Pitfalls when using both Python and R",
    "section": "3.4 Integers and floating point numbers",
    "text": "3.4 Integers and floating point numbers\nUnless you explicitely specify, any number is represented as a floating point number in R. By adding a L at the end of the number, you specify that it is an integer.\nPython is usually more strict about using integers or floating point numbers than R.\n\nfloat_ex &lt;- 12\nint_ex &lt;- 12L\n\nis.integer(float_ex)\n\n[1] FALSE\n\nis.integer(int_ex)\n\n[1] TRUE\n\n\nThis can often lead to errors when using reticulate! If you’re calling a Python function and provide it with just a number in R, it probably won’t be recognised as an integer, leading to errors:\n\nlibrary(reticulate)\nbi &lt;- reticulate::import_builtins()\n\nbi$list(bi$range(0, 5))\n\n'float' object cannot be interpreted as an integer\n\n\nAs you can see, you get errors: TypeError: 'float' object cannot be interpreted as an integer.\nThis is easily fixed by specifiying integers:\n\nbi$list(bi$range(0L, 5L))\n\n[1] 0 1 2 3 4",
    "crumbs": [
      "In-memory interoperability",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Pitfalls</span>"
    ]
  },
  {
    "objectID": "book/in_memory/rpy2.html",
    "href": "book/in_memory/rpy2.html",
    "title": "4  Rpy2",
    "section": "",
    "text": "4.1 Rpy2: basic functionality\nRpy2 is a foreign function interface to R. It can be used in the following way:\nimport rpy2\nimport rpy2.robjects as robjects\n\n/home/runner/work/polygloty/polygloty/renv/python/virtualenvs/renv-python-3.12/lib/python3.12/site-packages/rpy2/rinterface_lib/embedded.py:276: UserWarning: R was initialized outside of rpy2 (R_NilValue != NULL). Trying to use it nevertheless.\n  warnings.warn(msg)\nR was initialized outside of rpy2 (R_NilValue != NULL). Trying to use it nevertheless.\n\nvector = robjects.IntVector([1,2,3])\nrsum = robjects.r['sum']\n\nrsum(vector)\n\n\n        IntVector with 1 elements.\n        \n\n\n\n6\nLuckily, we’re not restricted to just calling R functions and creating R objects. The real power of this in-memory interoperability lies in the conversion of Python objects to R objects to call R functions on, and then to the conversion of the results back to Python objects.\nRpy2 requires specific conversion rules for different Python objects. It is straightforward to create R vectors from corresponding Python lists:\nstr_vector = robjects.StrVector(['abc', 'def', 'ghi'])\nflt_vector = robjects.FloatVector([0.3, 0.8, 0.7])\nint_vector = robjects.IntVector([1, 2, 3])\nmtx = robjects.r.matrix(robjects.IntVector(range(10)), nrow=5)\nHowever, for single cell biology, the objects that are most interesting to convert are (count) matrices, arrays and dataframes. In order to do this, you need to import the corresponding rpy2 modules and specify the conversion context.\nimport numpy as np\n\nfrom rpy2.robjects import numpy2ri\nfrom rpy2.robjects import default_converter\n\nrd_m = np.random.random((10, 7))\n\nwith (default_converter + numpy2ri.converter).context():\n    mtx2 = robjects.r.matrix(rd_m, nrow = 10)\nimport pandas as pd\n\nfrom rpy2.robjects import pandas2ri\n\npd_df = pd.DataFrame({'int_values': [1,2,3],\n                      'str_values': ['abc', 'def', 'ghi']})\n\nwith (default_converter + pandas2ri.converter).context():\n    pd_df_r = robjects.DataFrame(pd_df)\nOne big limitation of rpy2 is the inability to convert sparse matrices: there is no built-in conversion module for scipy. The anndata2ri package provides, apart from functionality to convert SingleCellExperiment objects to an anndata objects, functions to convert sparse matrices.\nimport scipy as sp\n\nfrom anndata2ri import scipy2ri\n\nsparse_matrix = sp.sparse.csc_matrix(rd_m)\n\nwith (default_converter + scipy2ri.converter).context():\n    sp_r = scipy2ri.py2rpy(sparse_matrix)\nWe will showcase how to use anndata2ri to convert an anndata object to a SingleCellExperiment object and vice versa as well:\nimport anndata as ad\nimport scanpy.datasets as scd\n\nimport anndata2ri\n\nadata_paul = scd.paul15()\n\n\n  0%|          | 0.00/9.82M [00:00&lt;?, ?B/s]\n  0%|          | 8.00k/9.82M [00:00&lt;02:10, 78.7kB/s]\n  0%|          | 32.0k/9.82M [00:00&lt;01:01, 167kB/s] \n  1%|          | 96.0k/9.82M [00:00&lt;00:27, 367kB/s]\n  2%|1         | 200k/9.82M [00:00&lt;00:16, 609kB/s] \n  4%|3         | 384k/9.82M [00:00&lt;00:09, 1.01MB/s]\n  8%|7         | 776k/9.82M [00:00&lt;00:04, 1.92MB/s]\n 15%|#5        | 1.51M/9.82M [00:00&lt;00:02, 3.66MB/s]\n 30%|##9       | 2.93M/9.82M [00:00&lt;00:01, 6.33MB/s]\n 55%|#####5    | 5.43M/9.82M [00:00&lt;00:00, 11.9MB/s]\n 69%|######9   | 6.80M/9.82M [00:01&lt;00:00, 12.6MB/s]\n 85%|########5 | 8.37M/9.82M [00:01&lt;00:00, 13.7MB/s]\n100%|##########| 9.82M/9.82M [00:01&lt;00:00, 8.13MB/s]\n\n\nwith anndata2ri.converter.context():\n    sce = anndata2ri.py2rpy(adata_paul)\n    ad2 = anndata2ri.rpy2py(sce)",
    "crumbs": [
      "In-memory interoperability",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Rpy2</span>"
    ]
  },
  {
    "objectID": "book/in_memory/rpy2.html#interactive-sessions",
    "href": "book/in_memory/rpy2.html#interactive-sessions",
    "title": "4  Rpy2",
    "section": "4.2 Interactive sessions",
    "text": "4.2 Interactive sessions\nOne of the most useful ways to take advantage of in-memory interoperability is to use it in interactive sessions, where you’re exploring the data and want to try out some functions non-native to your language of choice.\nJupyter notebooks (and some other notebooks) make this possible from the Python side: using IPython line and cell magic and rpy2, you can easily run an R jupyter cell in your notebooks.\n\n%load_ext rpy2.ipython  # line magic that loads the rpy2 ipython extension.\n                        # this extension allows the use of the following cell magic\n\n%%R -i input -o output  # this line allows to specify inputs \n                        # (which will be converted to R objects) and outputs \n                        # (which will be converted back to Python objects) \n                        # this line is put at the start of a cell\n                        # the rest of the cell will be run as R code",
    "crumbs": [
      "In-memory interoperability",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Rpy2</span>"
    ]
  },
  {
    "objectID": "book/in_memory/rpy2.html#usecase-ran-in-python",
    "href": "book/in_memory/rpy2.html#usecase-ran-in-python",
    "title": "4  Rpy2",
    "section": "4.3 Usecase: ran in Python",
    "text": "4.3 Usecase: ran in Python\nWe will perform the Compute DE step not in R, but in Python The pseudobulked data is read in:\n\nimport anndata as ad\n\npd_adata = ad.read_h5ad(\"../usecase/data/pseudobulk.h5ad\")\n\nSelect small molecule and control:\n\nsm_name = \"Belinostat\"\ncontrol_name = \"Dimethyl Sulfoxide\"\n\nCreating a DESeq dataset: This requires a bit more effort: we need to import the DESeq2 package, and combine the default, numpy2ri and pandas2ri converter to convert the count matrix and the obs dataframe.\n\nimport numpy as np\n\nimport rpy2\nimport rpy2.robjects as robjects\n\nfrom rpy2.robjects import numpy2ri\nfrom rpy2.robjects import pandas2ri\n\nfrom rpy2.robjects import default_converter\nfrom rpy2.robjects.packages import importr\n\nDESeq2 = importr(\"DESeq2\")\n\nnp_cv_rules = default_converter + numpy2ri.converter + pandas2ri.converter\n\nwith np_cv_rules.context() as cv:\n    counts_dense = np.transpose(pd_adata.X.astype(np.int32))\n\n    robjects.globalenv[\"count_data\"] = counts_dense\n    robjects.globalenv[\"obs_data\"] = pd_adata.obs\n\nWe can also specify R formulas!\n\nfrom rpy2.robjects import Formula\n\ndesign_formula = Formula('~ sm_name + plate_name')\n\ndds = DESeq2.DESeqDataSetFromMatrix(countData = robjects.globalenv[\"count_data\"],\n        colData = robjects.globalenv[\"obs_data\"],\n        design = design_formula)\n\nRun DESeq2:\n\ndds = DESeq2.DESeq(dds)\n\nGet results:\n\ncontrastv = robjects.StrVector([\"sm_name\", sm_name, control_name])\nres = DESeq2.results(dds, contrast=contrastv)\n\nbase = importr('base')\nres = base.as_data_frame(res)\n\nPreview results:\n\ndplyr = importr('dplyr')\nutils = importr('utils')\n\nres = utils.head(dplyr.arrange(res, 'padj'), 10)\n\nWrite to disk: this again requires the pandas2ri converter to convert the results to a pandas dataframe.\n\nwith (robjects.default_converter + pandas2ri.converter).context():\n    res_pd = robjects.conversion.get_conversion().rpy2py(res)\n\n    res_pd.to_csv(\"../usecase/data/de_contrasts.csv\")",
    "crumbs": [
      "In-memory interoperability",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Rpy2</span>"
    ]
  },
  {
    "objectID": "book/in_memory/reticulate.html",
    "href": "book/in_memory/reticulate.html",
    "title": "5  Reticulate",
    "section": "",
    "text": "Reticulate is a foreign function interface in R to Python.\n\n6 Reticulate: basic functionality\nData types are automatically converted from Python to R and vice versa. A useful table of automatic conversions can be found here.\nYou can easily import python modules, and call the functions in the following way:\n\nlibrary(reticulate)\n\nbi &lt;- reticulate::import_builtins()\nrd &lt;- reticulate::import(\"random\")\n\nexample &lt;- c(1,2,3)\nbi$max(example)\n\n[1] 3\n\nrd$choice(example)\n\n[1] 3\n\nbi$list(bi$reversed(example))\n\n[1] 3 2 1\n\n\nNumpy is also easily used:\n\nnp &lt;- reticulate::import(\"numpy\")\n\na &lt;- np$asarray(tuple(list(1,2), list(3, 4)))\nb &lt;- np$asarray(list(5,6))\nb &lt;- np$reshape(b, newshape = tuple(1L,2L))\n\nnp$concatenate(tuple(a, b), axis=0L)\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n[3,]    5    6\n\n\nIf you want more finegrained control over conversion, you can specify in the import statement that you do not want results of functions of that package to be converted to R data types.\n\nnp &lt;- reticulate::import(\"numpy\", convert = FALSE)\n\na &lt;- np$asarray(tuple(list(1,2), list(3, 4)))\nb &lt;- np$asarray(list(5,6))\nb &lt;- np$reshape(b, newshape = tuple(1L,2L))\n\nnp$concatenate(tuple(a, b), axis=0L)\n\narray([[1., 2.],\n       [3., 4.],\n       [5., 6.]])\n\n\nYou can explicitly convert data types:\n\nresult &lt;- np$concatenate(tuple(a, b), axis=0L)\n\npy_to_r(result)\n\n     [,1] [,2]\n[1,]    1    2\n[2,]    3    4\n[3,]    5    6\n\nresult_r &lt;- py_to_r(result)\nr_to_py(result_r)\n\narray([[1., 2.],\n       [3., 4.],\n       [5., 6.]])\n\n\n\n\n7 Interactivity\nYou can easily include Python chunks in Rmarkdown notebooks using the Python engine in knitr.\n\n\n8 Usecase\nWe will not showcase the usefulness of reticulate by using the DE analysis: it would involve loading in pandas to create a Python dataframe, adding rownames and columnnames and then grouping them, but that is easier to do natively in R.\nA more interesting thing you can do using reticulate is interacting with anndata-based Python packages, such as scanpy!\n\nlibrary(anndata)\nlibrary(reticulate)\nsc &lt;- import(\"scanpy\")\n\nadata_path &lt;- \"../usecase/data/sc_counts_subset.h5ad\"\nadata &lt;- anndata::read_h5ad(adata_path)\n\nWe can preprocess the data:\n\nsc$pp$filter_cells(adata, min_genes = 200)\nsc$pp$filter_genes(adata, min_cells = 3)\n\n\nsc$pp$pca(adata)\nsc$pp$neighbors(adata)\nsc$tl$umap(adata)\n\nadata\n\nAnnData object with n_obs × n_vars = 32727 × 20542\n    obs: 'dose_uM', 'timepoint_hr', 'well', 'row', 'col', 'plate_name', 'cell_id', 'cell_type', 'split', 'donor_id', 'sm_name', 'control', 'SMILES', 'sm_lincs_id', 'library_id', 'leiden_res1', 'group', 'cell_type_orig', 'plate_well_celltype_reannotated', 'cell_count_by_well_celltype', 'cell_count_by_plate_well', 'n_genes'\n    var: 'highly_variable', 'means', 'dispersions', 'dispersions_norm', 'n_cells'\n    uns: 'cell_type_colors', 'celltypist_celltype_colors', 'donor_id_colors', 'hvg', 'leiden_res1_colors', 'log1p', 'neighbors', 'over_clustering', 'rank_genes_groups', 'pca', 'umap'\n    obsm: 'HTO_clr', 'X_pca', 'X_umap', 'protein_counts'\n    varm: 'PCs'\n    obsp: 'connectivities', 'distances'\n\n\nWe can’t easily show the result of the plot in this Quarto notebook, but we can save it and show it:\n\npath &lt;- \"umap.png\"\nsc$pl$umap(adata, color=\"leiden_res1\", save=path)\n\n\n\n\n\n\n\nFigure 8.1: UMAP plot of the adata object",
    "crumbs": [
      "In-memory interoperability",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Reticulate</span>"
    ]
  },
  {
    "objectID": "book/disk_based/file_formats.html",
    "href": "book/disk_based/file_formats.html",
    "title": "6  Interoperable file formats",
    "section": "",
    "text": "6.1 General file formats\nTable 6.1 shows some of the features of file formats of interest for Python and R and weather they they lack support (○), support it partially (◐) or have full support (●).",
    "crumbs": [
      "Disk-based interoperability",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Interoperable file formats</span>"
    ]
  },
  {
    "objectID": "book/disk_based/file_formats.html#general-file-formats",
    "href": "book/disk_based/file_formats.html#general-file-formats",
    "title": "6  Interoperable file formats",
    "section": "",
    "text": "Fast compression is important for large datasets and most of these formats support it, as it can reduce the storage requirements. Standard compressions techniques like zip can always be applied to text-based formats like CSV and JSON, but standardized support within the file format leads to faster and higher compression performance.\nSparse matrix support is important for single-cell data, as it can also greatly reduce the storage requirements. If a part of the pipeline does not have this support and needs to make the matrix dense, the memory requirements will increase rapidly.\nSupport for large images like microscopy images is important for bioimaging and spatial omics datasets. Some formats support small images or transcript coordinates, but do not support large whole-slide images or multiscale and labeled multidimensional images. This is indicated with ◐.\nFor large datasets and whole-slide images, support for subsetting and lazy loading of chunks or tiles is also needed. This also allows for out-of-core and parallel processing of larger-than-memory datasets.\nSome file formats have built-in support for remote storage like a web server or S3. This can be important for large datasets that do not fit on a local disk or for distributed computing. This can also be used to share data between collaborators without having to download the whole dataset. Some formats can be accessed remotely, but only for public data (SpatialData) or with additional setup (HDF5). This is indicated with ◐. Mounting a remote folder via SFTP or rclone and downloading the data on demand only works for exploded file formats like Zarr or by supporting byte range requests.\n\n\n\n\nTable 6.1: Overview of general file formats of use for single cell.\n\n\n\n\n\nFile Format\nPython\nR\nSparse matrix\nLarge images\nLazy chunk loading\nRemote storage\n\n\n\n\nRDS\n○\n●\n○\n◐\n○\n○\n\n\nPickle\n●\n○\n○\n◐\n○\n○\n\n\nCSV\n●\n●\n○\n○\n○\n○\n\n\nJSON\n●\n●\n○\n○\n○\n○\n\n\nTIFF\n●\n●\n○\n◐\n●\n◐\n\n\n.npy\n●\n○\n○\n●\n○\n○\n\n\nParquet\n●\n●\n○\n○\n●\n●\n\n\nFeather\n●\n●\n●\n○\n●\n●\n\n\nLance\n●\n○\n●\n○\n●\n●\n\n\nHDF5\n●\n●\n○\n●\n●\n◐\n\n\nZarr\n●\n●\n○\n●\n●\n●\n\n\nTileDB\n●\n●\n●\n●\n●\n●",
    "crumbs": [
      "Disk-based interoperability",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Interoperable file formats</span>"
    ]
  },
  {
    "objectID": "book/disk_based/file_formats.html#specialized-file-formats",
    "href": "book/disk_based/file_formats.html#specialized-file-formats",
    "title": "6  Interoperable file formats",
    "section": "6.2 Specialized file formats",
    "text": "6.2 Specialized file formats\nTable 6.2 shows some specialized file standards that are built on top of the more general file formats. One example is the standardized way AnnData serializes a sparse matrix within Zarr to support large single cell data. Note that while a format can support a feature in theory, it does not mean it’s available in practice. For example, the TileDB implementations do not yet fully support large spatial omics datasets in a standardized way like SpatialData does. SpatialData does not yet fully support reading and writing using a remote private S3 dataset.\n\n\n\nTable 6.2: Overview of specialized file formats of use for single cell.\n\n\n\n\n\nFile Format\nPython\nR\nSparse matrix\nLarge images\nLazy chunk loading\nRemote storage\n\n\n\n\nSeurat RDS\n○\n●\n○\n◐\n○\n○\n\n\nIndexed OME-TIFF\n●\n●\n○\n●\n●\n●\n\n\nh5Seurat\n●\n●\n○\n◐\n●\n◐\n\n\nLoom HDF5\n●\n●\n●\n○\n●\n◐\n\n\nAnnData h5ad\n●\n●\n●\n◐\n●\n◐\n\n\nAnnData Zarr\n●\n●\n●\n◐\n●\n●\n\n\nTileDB-SOMA\n●\n●\n●\n◐\n●\n●\n\n\nTileDB-BioImaging\n●\n●\n◐\n●\n●\n●\n\n\nSpatialData Zarr\n●\n●\n●\n●\n●\n◐",
    "crumbs": [
      "Disk-based interoperability",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Interoperable file formats</span>"
    ]
  },
  {
    "objectID": "book/disk_based/disk_based_pipelines.html",
    "href": "book/disk_based/disk_based_pipelines.html",
    "title": "7  Disk-based pipelines",
    "section": "",
    "text": "7.1 Notebook pipelines\nYou can use Quarto to run code snippets in different languages in the same .qmd notebook. Our Use-case chapter is one example of this.. The polyglot powers of Quarto for executing code blocks come from tools such as knitr or Jupyter kernels. You don’t need in-memory interoperability, but it\nThere are frameworks that keep the notebooks and create a pipeline with it. The upside is that you can avoid converting the code snippets in the notebooks to scripts. The downside is that you have to use a specific framework and the notebooks can become very large and unwieldy. For example, Papermill can execute parameterized Jupyter notebooks in sequence and pass variables between them. nbdev and Ploomber are similar such frameworks.",
    "crumbs": [
      "Disk-based interoperability",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Disk-based pipelines</span>"
    ]
  },
  {
    "objectID": "book/disk_based/disk_based_pipelines.html#notebook-pipelines",
    "href": "book/disk_based/disk_based_pipelines.html#notebook-pipelines",
    "title": "7  Disk-based pipelines",
    "section": "",
    "text": "7.1.1 Execute notebooks via the CLI\nYou can execute notebooks via the command line as a job, so you don’t have to wait for them to finish. This is useful when running experiments, as the output is saved in the notebook together with plots, available to inspect them at a later time.\nJupyter notebook via nbconvert:\njupyter nbconvert --to notebook --execute my_notebook.ipynb --allow-errors --output-dir outputs/\nSimilar functionality exists for RMarkdown notebooks via Rscript:\nRscript -e \"rmarkdown::render('my_notebook.Rmd',params=list(args = myarg))\"",
    "crumbs": [
      "Disk-based interoperability",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Disk-based pipelines</span>"
    ]
  },
  {
    "objectID": "book/disk_based/disk_based_pipelines.html#script-pipelines",
    "href": "book/disk_based/disk_based_pipelines.html#script-pipelines",
    "title": "7  Disk-based pipelines",
    "section": "7.2 Script pipelines",
    "text": "7.2 Script pipelines\nThe scripts in a script pipeline are a collection of the code snippets from the notebooks and can be written in different languages and executed in sequence.\nThese scripts have the same functionality as the code in the use-case example notebook:\n\n1_load_data.sh2_compute_pseudobulk.py3_analysis_de.R\n\n\nif [[ ! -f usecase/data/sc_counts_reannotated_with_counts.h5ad ]]; then\n  aws s3 cp \\\n    --no-sign-request \\\n    s3://openproblems-bio/public/neurips-2023-competition/sc_counts_reannotated_with_counts.h5ad \\\n    usecase/data/sc_counts_reannotated_with_counts.h5ad\nfi\n\n\n# The dataset is stored in an AnnData object, which can be loaded in Python as follows:\nimport anndata as ad\n\nprint(\"Load data\")\nadata = ad.read_h5ad(\"usecase/data/sc_counts_reannotated_with_counts.h5ad\")\n\nsm_name = \"Belinostat\"\ncontrol_name = \"Dimethyl Sulfoxide\"\ncell_type = \"T cells\"\n\nadata = adata[\n  adata.obs[\"sm_name\"].isin([sm_name, control_name]) &\n  adata.obs[\"cell_type\"].isin([cell_type]),\n].copy()\n\n# We will also subset the genes to the top 2000 most variable genes.\n\nadata = adata[:, adata.var[\"highly_variable\"]].copy()\n\nprint(\"Compute pseudobulk\")\n# Combine data in a single data frame and compute pseudobulk\nimport pandas as pd\n\ncombined = pd.DataFrame(\n  adata.X.toarray(),\n  index=adata.obs[\"plate_well_celltype_reannotated\"],\n)\ncombined.columns = adata.var_names\npb_X = combined.groupby(level=0).sum()\n\nprint(\"Construct obs for pseudobulk\")\n\n# Use 'plate_well_celltype_reannotated' as index and make sure to retain the columns 'sm_name', 'cell_type', and 'plate_name':\n\npb_obs = adata.obs[[\"sm_name\", \"cell_type\", \"plate_name\", \"well\"]].copy()\npb_obs.index = adata.obs[\"plate_well_celltype_reannotated\"]\npb_obs = pb_obs.drop_duplicates()\n\nprint(\"Create AnnData object\")\npb_adata = ad.AnnData(\n  X=pb_X.loc[pb_obs.index].values,\n  obs=pb_obs,\n  var=adata.var,\n)\n\nprint(\"Store to disk\")\npb_adata.write_h5ad(\"usecase/data/pseudobulk.h5ad\")\n\n\ncat(\"Loading libraries...\\n\")\nlibrary(anndata)\nlibrary(dplyr, warn.conflicts = FALSE)\n\ncat(\"Reading data...\\n\")\npb_adata &lt;- read_h5ad(\"usecase/data/pseudobulk.h5ad\")\n\n# Select small molecule and control:\nsm_name &lt;- \"Belinostat\"\ncontrol_name &lt;- \"Dimethyl Sulfoxide\"\n\ncat(\"Create DESeq dataset\\n\")\n# transform counts matrix\ncount_data &lt;- t(pb_adata$X)\nstorage.mode(count_data) &lt;- \"integer\"\n\n# create dataset\ndds &lt;- DESeq2::DESeqDataSetFromMatrix(\n  countData = count_data,\n  colData = pb_adata$obs,\n  design = ~ sm_name + plate_name,\n)\n\ncat(\"Run DESeq2\\n\")\ndds &lt;- DESeq2::DESeq(dds)\n\n# Get results:\nres &lt;- DESeq2::results(dds, contrast=c(\"sm_name\", sm_name, control_name)) |&gt;\n  as.data.frame()\n\n# Preview results:\nres |&gt;\n  arrange(padj) |&gt;\n  head(10)\n\n# Write to disk:\nwrite.csv(res, \"usecase/data/de_contrasts.csv\")\n\n\n\nYou can use a shell script or your language of preference to run the pipeline in a sequential manner. This usually requires all the dependencies to be installed in one large environment.\nFrom Bash:\n#!/bin/bash\n\nbash scripts/1_load_data.sh\npython scripts/2_compute_pseudobulk.py\nRscript scripts/3_analysis_de.R\nFrom R:\nsystem(\"bash scripts/1_load_data.sh\")\nsystem(\"python scripts/2_compute_pseudobulk.py\")\n# Alternatively you can run R code here instead of calling an R script\nsystem(\"Rscript scripts/3_analysis_de.R\")\nFrom Python:\nimport subprocess\n\nsubprocess.run(\"bash scripts/1_load_data.sh\", shell=True)\n# Alternatively you can run Python code here instead of calling a Python script\nsubprocess.run(\"python scripts/2_compute_pseudobulk.py\", shell=True)\nsubprocess.run(\"Rscript scripts/3_analysis_de.R\", shell=True)",
    "crumbs": [
      "Disk-based interoperability",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Disk-based pipelines</span>"
    ]
  },
  {
    "objectID": "book/disk_based/disk_based_pipelines.html#pipelines-with-different-environments",
    "href": "book/disk_based/disk_based_pipelines.html#pipelines-with-different-environments",
    "title": "7  Disk-based pipelines",
    "section": "7.3 Pipelines with different environments",
    "text": "7.3 Pipelines with different environments\nSometimes you might want to run scripts in different environments, as it’s too much hassle to install all dependencies in one environment. Other reasons can be that you want to reuse existing container images or you want keep parts of the pipeline separate and maintainable.\nYou can interleave your Bash script with environment activation functions e.g. conda activate {script_env} commands. This requires e.g. a conda .yaml file for each script environment in order to be reproducible. This can grow unwieldy as Conda environment sometimes need to be adapted for the platform and availability of accelerators like GPU’s. Another important consideration is that packages that impact the on-disk data format should be the same version across environments.\nA simple and elegant, but very new solution is to use the Pixi package managment tool to manage the environments and tasks for you. The environments can be composed from multiple features containing dependencies, so you can have a scverse environment with only Python, a rverse environment with only R and even an all environment with both by adding the respective features (if such an environment is resolvable at least).\nRun scripts in different environments with pixi:\npixi run -e bash scripts/1_load_data.sh\npixi run -e scverse scripts/2_compute_pseudobulk.py\npixi run -e rverse scripts/3_analysis_de.R\nWith the Pixi task runner, you can define these tasks in their respective environments and make them called in sequence by an overarching task. The specific lines of the pixi.toml file that allow for this are:\n...\n[feature.bash.tasks]\nload_data = \"bash book/disk_based/scripts/1_load_data.sh\"\n...\n[feature.scverse.tasks]\ncompute_pseudobulk = \"python book/disk_based/scripts/2_compute_pseudobulk.py\"\n...\n[feature.rverse.tasks]\nanalysis_de = \"Rscript --no-init-file book/disk_based/scripts/3_analysis_de.R\"\n...\n[tasks]\npipeline = { depends-on = [\"load_data\", \"compute_pseudobulk\", \"analysis_de\"] }\nNow you can run this multi-environment pipeline with a single command and it will always run the tasks in their respective and up-to-date environment:\npixi run pipeline\n\n\n\n\n\n\nOutput\n\n\n\n\n\nPixi task (load_data in bash): bash book/disk_based/scripts/1_load_data.sh\n\nPixi task (compute_pseudobulk in scverse): python book/disk_based/scripts/2_compute_pseudobulk.py\nLoad data\nCompute pseudobulk\n/app/book/disk_based/scripts/2_compute_pseudobulk.py:29: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n  pb_X = combined.groupby(level=0).sum()\nConstruct obs for pseudobulk\nCreate AnnData object\nStore to disk\n\nPixi task (analysis_de in rverse): Rscript --no-init-file book/disk_based/scripts/3_analysis_de.R\nLoading libraries...\nReading data...\nCreate DESeq dataset\n  Note: levels of factors in the design contain characters other than\n  letters, numbers, '_' and '.'. It is recommended (but not required) to use\n  only letters, numbers, and delimiters '_' or '.', as these are safe characters\n  for column names in R. [This is a message, not a warning or an error]\nRun DESeq2\nestimating size factors\n  Note: levels of factors in the design contain characters other than\n  letters, numbers, '_' and '.'. It is recommended (but not required) to use\n  only letters, numbers, and delimiters '_' or '.', as these are safe characters\n  for column names in R. [This is a message, not a warning or an error]\nestimating dispersions\ngene-wise dispersion estimates\nmean-dispersion relationship\n  Note: levels of factors in the design contain characters other than\n  letters, numbers, '_' and '.'. It is recommended (but not required) to use\n  only letters, numbers, and delimiters '_' or '.', as these are safe characters\n  for column names in R. [This is a message, not a warning or an error]\nfinal dispersion estimates\nfitting model and testing\n  Note: levels of factors in the design contain characters other than\n  letters, numbers, '_' and '.'. It is recommended (but not required) to use\n  only letters, numbers, and delimiters '_' or '.', as these are safe characters\n  for column names in R. [This is a message, not a warning or an error]\n          baseMean log2FoldChange      lfcSE      stat        pvalue\nBEX5      59.24944       2.187350 0.05660399  38.64304  0.000000e+00\nHIST1H1D 301.38741       1.356543 0.03092962  43.85901  0.000000e+00\nSTMN1    234.72112       2.224633 0.04104002  54.20642  0.000000e+00\nPCSK1N    64.91604       1.899149 0.05480612  34.65214 4.147855e-263\nGZMM     141.39238      -1.309959 0.03806665 -34.41224 1.654371e-259\nMARCKSL1  95.82726       1.423057 0.04311798  33.00380 7.163953e-239\nH1FX     376.28247       1.054890 0.03221858  32.74168 3.988563e-235\nHIST1H1B  30.81805       4.317984 0.14074738  30.67896 1.086254e-206\nFXYD7     61.11526       2.331406 0.07725771  30.17700 4.746707e-200\nING2      79.68893       1.218777 0.04336609  28.10437 8.663682e-174\n                  padj\nBEX5      0.000000e+00\nHIST1H1D  0.000000e+00\nSTMN1     0.000000e+00\nPCSK1N   1.631144e-260\nGZMM     5.204651e-257\nMARCKSL1 1.878150e-236\nH1FX     8.962871e-233\nHIST1H1B 2.135848e-204\nFXYD7    8.296189e-198\nING2     1.362797e-171\n\n\n\nYou can also still run the tasks individually when debugging a step and change behavior using environment variables.",
    "crumbs": [
      "Disk-based interoperability",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Disk-based pipelines</span>"
    ]
  },
  {
    "objectID": "book/disk_based/disk_based_pipelines.html#containerized-pipelines",
    "href": "book/disk_based/disk_based_pipelines.html#containerized-pipelines",
    "title": "7  Disk-based pipelines",
    "section": "7.4 Containerized pipelines",
    "text": "7.4 Containerized pipelines\nContainers are a great way to manage the environments for your pipeline and make them reproducible on different platforms, given that you make accessible and store the container images for a long time.\nYou can create a Docker image with all the pixi environments and run the pipeline in multiple environments with a single container. The image is ~5GB and the pipeline can require a lot of working memory ~20GB, so make sure to increase the RAM allocated to Docker in your settings. Note that the usecase/ and book/ folders are mounted to the Docker container, so you can interactively edit the scripts and access the data.\ndocker pull berombau/polygloty-docker:latest\ndocker run -it -v $(pwd)/usecase:/app/usecase -v $(pwd)/book:/app/book berombau/polygloty-docker:latest pixi run pipeline\nAnother approach is to use multi-package containers. Tools like Multi-Package BioContainers and Seqera Containers can make this quick and easy, by allowing for custom combinations of packages.\nYou can go a long way with a folder of notebooks or scripts and the right tools. But as your project grows more bespoke, it can be worth the effort to use a workflow framework like Nextflow or Snakemake to manage the pipeline for you.",
    "crumbs": [
      "Disk-based interoperability",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Disk-based pipelines</span>"
    ]
  },
  {
    "objectID": "book/workflow_frameworks/review.html",
    "href": "book/workflow_frameworks/review.html",
    "title": "8  Review of Workflow Frameworks",
    "section": "",
    "text": "8.1 Comparing PoC Workflows to Community-Made Modules\nHowever, comparing the POC workflows (left) to community-made modules (right), it becomes clear that creating production-ready components requires a lot more than specifying a command’s input and output files.",
    "crumbs": [
      "Workflow frameworks",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>A review</span>"
    ]
  },
  {
    "objectID": "book/workflow_frameworks/review.html#comparing-poc-workflows-to-community-made-modules",
    "href": "book/workflow_frameworks/review.html#comparing-poc-workflows-to-community-made-modules",
    "title": "8  Review of Workflow Frameworks",
    "section": "",
    "text": "8.1.1 Nextflow\n\n\nWratten et al. 2021 PoC (Source):\n\nmain.nf\n\n\nprocess FASTQC {\n  publishDir params.outdir\n\n  input:\n    path index\n    path left\n    path right\n  output:\n    path 'qc'\n\n  \"\"\"\n    mkdir qc && fastqc --quiet '${params.left}' '${params.right}' --outdir qc\n  \"\"\"\n}\n\n\n\n\n\nnf-core (Source):\n\nenvironment.ymlmain.nfmeta.yamltests/main.nf.test\n\n\nchannels:\n  - conda-forge\n  - bioconda\ndependencies:\n  - bioconda::fastqc=0.12.1\n\n\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda \"${moduleDir}/environment.yml\"\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.12.1--hdfd78af_0' :\n        'biocontainers/fastqc:0.12.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    // Make list of old name and new name pairs to use for renaming in the bash while loop\n    def old_new_pairs = reads instanceof Path || reads.size() == 1 ? [[ reads, \"${prefix}.${reads.extension}\" ]] : reads.withIndex().collect { entry, index -&gt; [ entry, \"${prefix}_${index + 1}.${entry.extension}\" ] }\n    def rename_to = old_new_pairs*.join(' ').join(' ')\n    def renamed_files = old_new_pairs.collect{ old_name, new_name -&gt; new_name }.join(' ')\n\n    // The total amount of allocated RAM by FastQC is equal to the number of threads defined (--threads) time the amount of RAM defined (--memory)\n    // https://github.com/s-andrews/FastQC/blob/1faeea0412093224d7f6a07f777fad60a5650795/fastqc#L211-L222\n    // Dividing the task.memory by task.cpu allows to stick to requested amount of RAM in the label\n    def memory_in_mb = MemoryUnit.of(\"${task.memory}\").toUnit('MB') / task.cpus\n    // FastQC memory value allowed range (100 - 10000)\n    def fastqc_memory = memory_in_mb &gt; 10000 ? 10000 : (memory_in_mb &lt; 100 ? 100 : memory_in_mb)\n\n    \"\"\"\n    printf \"%s %s\\\\n\" $rename_to | while read old_name new_name; do\n        [ -f \"\\${new_name}\" ] || ln -s \\$old_name \\$new_name\n    done\n\n    fastqc \\\\\n        $args \\\\\n        --threads $task.cpus \\\\\n        --memory $fastqc_memory \\\\\n        $renamed_files\n\n    cat &lt;&lt;-END_VERSIONS &gt; versions.yml\n    \"${task.process}\":\n        fastqc: \\$( fastqc --version | sed '/FastQC v/!d; s/.*v//' )\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.html\n    touch ${prefix}.zip\n\n    cat &lt;&lt;-END_VERSIONS &gt; versions.yml\n    \"${task.process}\":\n        fastqc: \\$( fastqc --version | sed '/FastQC v/!d; s/.*v//' )\n    END_VERSIONS\n    \"\"\"\n}\n\n\nname: fastqc\ndescription: Run FastQC on sequenced reads\nkeywords:\n  - quality control\n  - qc\n  - adapters\n  - fastq\ntools:\n  - fastqc:\n      description: |\n        FastQC gives general quality metrics about your reads.\n        It provides information about the quality score distribution\n        across your reads, the per base sequence content (%A/C/G/T).\n        You get information about adapter contamination and other\n        overrepresented sequences.\n      homepage: https://www.bioinformatics.babraham.ac.uk/projects/fastqc/\n      documentation: https://www.bioinformatics.babraham.ac.uk/projects/fastqc/Help/\n      licence: [\"GPL-2.0-only\"]\ninput:\n  - meta:\n      type: map\n      description: |\n        Groovy Map containing sample information\n        e.g. [ id:'test', single_end:false ]\n  - reads:\n      type: file\n      description: |\n        List of input FastQ files of size 1 and 2 for single-end and paired-end data,\n        respectively.\noutput:\n  - meta:\n      type: map\n      description: |\n        Groovy Map containing sample information\n        e.g. [ id:'test', single_end:false ]\n  - html:\n      type: file\n      description: FastQC report\n      pattern: \"*_{fastqc.html}\"\n  - zip:\n      type: file\n      description: FastQC report archive\n      pattern: \"*_{fastqc.zip}\"\n  - versions:\n      type: file\n      description: File containing software versions\n      pattern: \"versions.yml\"\nauthors:\n  - \"@drpatelh\"\n  - \"@grst\"\n  - \"@ewels\"\n  - \"@FelixKrueger\"\nmaintainers:\n  - \"@drpatelh\"\n  - \"@grst\"\n  - \"@ewels\"\n  - \"@FelixKrueger\"\n\n\nnextflow_process {\n\n    name \"Test Process FASTQC\"\n    script \"../main.nf\"\n    process \"FASTQC\"\n\n    tag \"modules\"\n    tag \"modules_nfcore\"\n    tag \"fastqc\"\n\n    test(\"sarscov2 single-end [fastq]\") {\n\n        when {\n            process {\n                \"\"\"\n                input[0] = Channel.of([\n                    [ id: 'test', single_end:true ],\n                    [ file(params.modules_testdata_base_path + 'genomics/sarscov2/illumina/fastq/test_1.fastq.gz', checkIfExists: true) ]\n                ])\n                \"\"\"\n            }\n        }\n\n        then {\n            assertAll (\n                { assert process.success },\n                // NOTE The report contains the date inside it, which means that the md5sum is stable per day, but not longer than that. So you can't md5sum it.\n                // looks like this: &lt;div id=\"header_filename\"&gt;Mon 2 Oct 2023&lt;br/&gt;test.gz&lt;/div&gt;\n                // https://github.com/nf-core/modules/pull/3903#issuecomment-1743620039\n                { assert process.out.html[0][1] ==~ \".*/test_fastqc.html\" },\n                { assert process.out.zip[0][1] ==~ \".*/test_fastqc.zip\" },\n                { assert path(process.out.html[0][1]).text.contains(\"&lt;tr&gt;&lt;td&gt;File type&lt;/td&gt;&lt;td&gt;Conventional base calls&lt;/td&gt;&lt;/tr&gt;\") },\n                { assert snapshot(process.out.versions).match() }\n            )\n        }\n    }\n\n    /* The rest of the tests are omitted */\n}\n\n\n\n\n\n\n\n8.1.2 Snakemake\n\n\nWratten et al. 2021 PoC (Source):\n\nfastqc.smk\n\n\nrule fastqc:\n    input:\n        get_fastqs,\n    output:\n        directory(\"results/fastqc/{sample}\"),\n    log:\n        \"logs/fastqc/{sample}.log\",\n    conda:\n        \"envs/fastqc.yaml\"\n    params:\n        \"--quiet --outdir\",\n    shell:\n        \"mkdir {output}; fastqc {input} {params} {output} 2&gt; {log}\"\n\n\n\n\n\nsnakemake-wrappers (Source):\n\nenvironment.yamlmeta.yamlwrapper.pytest/Snakefile\n\n\nchannels:\n  - conda-forge\n  - bioconda\n  - nodefaults\ndependencies:\n  - fastqc =0.12.1\n  - snakemake-wrapper-utils =0.6.2\n\n\nname: fastqc\ndescription: |\n  Generate fastq qc statistics using fastqc.\nurl: https://github.com/s-andrews/FastQC\nauthors:\n  - Julian de Ruiter\ninput:\n  - fastq file\noutput:\n  - html file containing statistics\n  - zip file containing statistics\n\n\n\"\"\"Snakemake wrapper for fastqc.\"\"\"\n\n__author__ = \"Julian de Ruiter\"\n__copyright__ = \"Copyright 2017, Julian de Ruiter\"\n__email__ = \"julianderuiter@gmail.com\"\n__license__ = \"MIT\"\n\n\nfrom os import path\nimport re\nfrom tempfile import TemporaryDirectory\nfrom snakemake.shell import shell\nfrom snakemake_wrapper_utils.snakemake import get_mem\n\nextra = snakemake.params.get(\"extra\", \"\")\nlog = snakemake.log_fmt_shell(stdout=True, stderr=True)\n# Define memory per thread (https://github.com/s-andrews/FastQC/blob/master/fastqc#L201-L222)\nmem_mb = int(get_mem(snakemake, \"MiB\") / snakemake.threads)\n\n\ndef basename_without_ext(file_path):\n    \"\"\"Returns basename of file path, without the file extension.\"\"\"\n\n    base = path.basename(file_path)\n    # Remove file extension(s) (similar to the internal fastqc approach)\n    base = re.sub(\"\\\\.gz$\", \"\", base)\n    base = re.sub(\"\\\\.bz2$\", \"\", base)\n    base = re.sub(\"\\\\.txt$\", \"\", base)\n    base = re.sub(\"\\\\.fastq$\", \"\", base)\n    base = re.sub(\"\\\\.fq$\", \"\", base)\n    base = re.sub(\"\\\\.sam$\", \"\", base)\n    base = re.sub(\"\\\\.bam$\", \"\", base)\n\n    return base\n\n\n# If you have multiple input files fastqc doesn't know what to do. Taking silently only first gives unapreciated results\n\nif len(snakemake.input) &gt; 1:\n    raise IOError(\"Got multiple input files, I don't know how to process them!\")\n\n# Run fastqc, since there can be race conditions if multiple jobs\n# use the same fastqc dir, we create a temp dir.\nwith TemporaryDirectory() as tempdir:\n    shell(\n        \"fastqc\"\n        \" --threads {snakemake.threads}\"\n        \" --memory {mem_mb}\"\n        \" {extra}\"\n        \" --outdir {tempdir:q}\"\n        \" {snakemake.input[0]:q}\"\n        \" {log}\"\n    )\n\n    # Move outputs into proper position.\n    output_base = basename_without_ext(snakemake.input[0])\n    html_path = path.join(tempdir, output_base + \"_fastqc.html\")\n    zip_path = path.join(tempdir, output_base + \"_fastqc.zip\")\n\n    if snakemake.output.html != html_path:\n        shell(\"mv {html_path:q} {snakemake.output.html:q}\")\n\n    if snakemake.output.zip != zip_path:\n        shell(\"mv {zip_path:q} {snakemake.output.zip:q}\")\n\n\nrule fastqc:\n    input:\n        \"reads/{sample}.fastq\"\n    output:\n        html=\"qc/fastqc/{sample}.html\",\n        zip=\"qc/fastqc/{sample}_fastqc.zip\" # the suffix _fastqc.zip is necessary for multiqc to find the file. If not using multiqc, you are free to choose an arbitrary filename\n    params:\n        extra = \"--quiet\"\n    log:\n        \"logs/fastqc/{sample}.log\"\n    threads: 1\n    resources:\n        mem_mb = 1024\n    wrapper:\n        \"master/bio/fastqc\"\n\n\n\n\n\n\n\n8.1.3 Toil + WDL\n\n\nWratten et al. 2021 PoC (Source):\n\nfastqc.wdl\n\n\ntask FastQCone {\n  input {\n     File reads\n  }\n\n  command {\n     zcat \"${reads}\" | fastqc stdin:readsone\n  }\n\n  output {\n     File fastqc_res = \"readsone_fastqc.html\"\n  }\n  \n  runtime {\n     docker: 'pegi3s/fastqc'\n  }\n}\n\n\n\n\n\nBioWDL (Source):\n\nfastqc.wdl (Excerpt)fastqc.wdl (Full)\n\n\nversion 1.0\n\n# ... license ...\n\ntask Fastqc {\n    input {\n        File seqFile\n        String outdirPath\n        Boolean casava = false\n        ## ... other arguments ...\n\n        # Set javaXmx a little high.\n        String javaXmx=\"1750M\"\n        Int threads = 1\n        String memory = \"2GiB\"\n        Int timeMinutes = 1 + ceil(size(seqFile, \"G\")) * 4\n        String dockerImage = \"quay.io/biocontainers/fastqc:0.12.1--hdfd78af_0\"\n\n        Array[File]? noneArray\n        File? noneFile\n    }\n\n    # Chops of the .gz extension if present.\n    String name = basename(sub(seqFile, \"\\.gz$\",\"\"))\n    # This regex chops of the extension just as fastqc does it.\n    String reportDir = outdirPath + \"/\" + sub(name, \"\\.[^\\.]*$\", \"_fastqc\")\n\n    # We reimplement the perl wrapper here. This has the advantage that it\n    # gives us more control over the amount of memory used.\n    command &lt;&lt;&lt;\n        set -e\n        mkdir -p \"~{outdirPath}\"\n        FASTQC_DIR=\"/usr/local/opt/fastqc-0.12.1\"\n        export CLASSPATH=\"$FASTQC_DIR:$FASTQC_DIR/sam-1.103.jar:$FASTQC_DIR/jbzip2-0.9.jar:$FASTQC_DIR/cisd-jhdf5.jar\"\n        java -Djava.awt.headless=true -XX:ParallelGCThreads=1 \\\n        -Xms200M -Xmx~{javaXmx} \\\n        ~{\"-Dfastqc.output_dir=\" + outdirPath} \\\n        ~{true=\"-Dfastqc.casava=true\" false=\"\" casava} \\\n        # ... other arguments ...\n        ~{\"-Dfastqc.kmer_size=\" + kmers} \\\n        ~{\"-Djava.io.tmpdir=\" + dir} \\\n        uk.ac.babraham.FastQC.FastQCApplication \\\n        ~{seqFile}\n    &gt;&gt;&gt;\n\n    output {\n        File htmlReport = reportDir + \".html\"\n        File reportZip = reportDir + \".zip\"\n        File? summary = if extract then reportDir + \"/summary.txt\" else noneFile\n        File? rawReport = if extract then reportDir + \"/fastqc_data.txt\" else noneFile\n        Array[File]? images = if extract then glob(reportDir + \"/Images/*.png\") else noneArray\n    }\n\n    runtime {\n        cpu: threads\n        memory: memory\n        time_minutes: timeMinutes\n        docker: dockerImage\n    }\n\n    parameter_meta {\n        # inputs\n        seqFile: {description: \"A fastq file.\", category: \"required\"}\n        outdirPath: {description: \"The path to write the output to.\", catgory: \"required\"}\n        # ... other arguments ...\n        dockerImage: {description: \"The docker image used for this task. Changing this may result in errors which the developers may choose not to address.\", category: \"advanced\"}\n\n        # outputs\n        htmlReport: {description: \"HTML report file.\"}\n        reportZip: {description: \"Source data file.\"}\n        summary: {description: \"Summary file.\"}\n        rawReport: {description: \"Raw report file.\"}\n        images: {description: \"Images in report file.\"}\n    }\n\n    meta {\n        WDL_AID: {\n            exclude: [\"noneFile\", \"noneArray\"]\n        }\n    }\n}\n\n\nversion 1.0\n\n# Copyright (c) 2017 Leiden University Medical Center\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\ntask Fastqc {\n    input {\n        File seqFile\n        String outdirPath\n        Boolean casava = false\n        Boolean nano = false\n        Boolean noFilter = false\n        Boolean extract = false\n        Boolean nogroup = false\n\n        Int? minLength\n        String? format\n        File? contaminants\n        File? adapters\n        File? limits\n        Int? kmers\n        String? dir\n\n        # Set javaXmx a little high. Equal to fastqc default with 7 threads.\n        # This is because some fastq files need more memory. 2G per core\n        # is a nice cluster default, so we use all the rest of the memory for\n        # fastqc so we should have as little OOM crashes as possible even with\n        # weird edge case fastq's.\n        String javaXmx=\"1750M\"\n        Int threads = 1\n        String memory = \"2GiB\"\n        Int timeMinutes = 1 + ceil(size(seqFile, \"G\")) * 4\n        String dockerImage = \"quay.io/biocontainers/fastqc:0.12.1--hdfd78af_0\"\n\n        Array[File]? noneArray\n        File? noneFile\n    }\n\n    # Chops of the .gz extension if present.\n    # The Basename needs to be taken here. Otherwise paths might differ\n    # between similar jobs.\n    String name = basename(sub(seqFile, \"\\.gz$\",\"\"))\n    # This regex chops of the extension and replaces it with _fastqc for\n    # the reportdir.\n    # Just as fastqc does it.\n    String reportDir = outdirPath + \"/\" + sub(name, \"\\.[^\\.]*$\", \"_fastqc\")\n\n    # We reimplement the perl wrapper here. This has the advantage that it\n    # gives us more control over the amount of memory used.\n    command &lt;&lt;&lt;\n        set -e\n        mkdir -p \"~{outdirPath}\"\n        FASTQC_DIR=\"/usr/local/opt/fastqc-0.12.1\"\n        export CLASSPATH=\"$FASTQC_DIR:$FASTQC_DIR/sam-1.103.jar:$FASTQC_DIR/jbzip2-0.9.jar:$FASTQC_DIR/cisd-jhdf5.jar\"\n        java -Djava.awt.headless=true -XX:ParallelGCThreads=1 \\\n        -Xms200M -Xmx~{javaXmx} \\\n        ~{\"-Dfastqc.output_dir=\" + outdirPath} \\\n        ~{true=\"-Dfastqc.casava=true\" false=\"\" casava} \\\n        ~{true=\"-Dfastqc.nano=true\" false=\"\" nano} \\\n        ~{true=\"-Dfastqc.nofilter=true\" false=\"\" noFilter} \\\n        ~{true=\"-Dfastqc.unzip=true\" false=\"\" extract} \\\n        ~{true=\"-Dfastqc.nogroup=true\" false=\"\" nogroup} \\\n        ~{\"-Dfastqc.min_length=\" + minLength} \\\n        ~{\"-Dfastqc.sequence_format=\" + format} \\\n        ~{\"-Dfastqc.threads=\" + threads} \\\n        ~{\"-Dfastqc.contaminant_file=\" + contaminants} \\\n        ~{\"-Dfastqc.adapter_file=\" + adapters} \\\n        ~{\"-Dfastqc.limits_file=\" + limits} \\\n        ~{\"-Dfastqc.kmer_size=\" + kmers} \\\n        ~{\"-Djava.io.tmpdir=\" + dir} \\\n        uk.ac.babraham.FastQC.FastQCApplication \\\n        ~{seqFile}\n    &gt;&gt;&gt;\n\n    output {\n        File htmlReport = reportDir + \".html\"\n        File reportZip = reportDir + \".zip\"\n        File? summary = if extract then reportDir + \"/summary.txt\" else noneFile\n        File? rawReport = if extract then reportDir + \"/fastqc_data.txt\" else noneFile\n        Array[File]? images = if extract then glob(reportDir + \"/Images/*.png\") else noneArray\n    }\n\n    runtime {\n        cpu: threads\n        memory: memory\n        time_minutes: timeMinutes\n        docker: dockerImage\n    }\n\n    parameter_meta {\n        # inputs\n        seqFile: {description: \"A fastq file.\", category: \"required\"}\n        outdirPath: {description: \"The path to write the output to.\", catgory: \"required\"}\n        casava: {description: \"Equivalent to fastqc's --casava flag.\", category: \"advanced\"}\n        nano: {description: \"Equivalent to fastqc's --nano flag.\", category: \"advanced\"}\n        noFilter: {description: \"Equivalent to fastqc's --nofilter flag.\", category: \"advanced\"}\n        extract: {description: \"Equivalent to fastqc's --extract flag.\", category: \"advanced\"}\n        nogroup: {description: \"Equivalent to fastqc's --nogroup flag.\", category: \"advanced\"}\n        minLength: {description: \"Equivalent to fastqc's --min_length option.\", category: \"advanced\"}\n        format: {description: \"Equivalent to fastqc's --format option.\", category: \"advanced\"}\n        contaminants: {description: \"Equivalent to fastqc's --contaminants option.\", category: \"advanced\"}\n        adapters: {description: \"Equivalent to fastqc's --adapters option.\", category: \"advanced\"}\n        limits: {description: \"Equivalent to fastqc's --limits option.\", category: \"advanced\"}\n        kmers: {description: \"Equivalent to fastqc's --kmers option.\", category: \"advanced\"}\n        dir: {description: \"Equivalent to fastqc's --dir option.\", category: \"advanced\"}\n        javaXmx: {description: \"The maximum memory available to the program. Should be lower than `memory` to accommodate JVM overhead.\", category: \"advanced\"}\n        threads: {description: \"The number of cores to use.\", category: \"advanced\"}\n        memory: {description: \"The amount of memory this job will use.\", category: \"advanced\"}\n        timeMinutes: {description: \"The maximum amount of time the job will run in minutes.\", category: \"advanced\"}\n        dockerImage: {description: \"The docker image used for this task. Changing this may result in errors which the developers may choose not to address.\", category: \"advanced\"}\n\n        # outputs\n        htmlReport: {description: \"HTML report file.\"}\n        reportZip: {description: \"Source data file.\"}\n        summary: {description: \"Summary file.\"}\n        rawReport: {description: \"Raw report file.\"}\n        images: {description: \"Images in report file.\"}\n    }\n\n    meta {\n        WDL_AID: {\n            exclude: [\"noneFile\", \"noneArray\"]\n        }\n    }\n}",
    "crumbs": [
      "Workflow frameworks",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>A review</span>"
    ]
  },
  {
    "objectID": "book/workflow_frameworks/review.html#limitations-of-the-study",
    "href": "book/workflow_frameworks/review.html#limitations-of-the-study",
    "title": "8  Review of Workflow Frameworks",
    "section": "8.2 Limitations of the study",
    "text": "8.2 Limitations of the study\nHowever, the Supplementary Table shows that the comparison in Table 8.1 was rather limited, since the score of each category was only based on a single criterion. Of the following categories, only “Scalability” was determined by more than one criterion:\n\nEase of Use: Graphical interface with execution environment (score of 3), programming interface with in-built execution environment (score of 2), separated development and execution environment (score of 1).\nExpressiveness: Based on an existing programming language (3) or a new language or restricted vocabulary (2), primary interaction with graphical user interface (1).\nPortability: Integration with three or more container and package manager platforms (3), two platforms are supported (2), one platform is supported (1).\nScalability: Considers cloud support, scheduler and orchestration tool integration, and executor support. Please refer to Supplementary Table 1 - Sheet 2 (Scalability).\nLearning resources: Official tutorials, forums and events (3), tutorials and forums (2), tutorials or forums (1).\nPipelines Initiatives: Community and curated (3), community or curated (2), not community or curated (1).\n\nBy comparing the example code of the respective workflow frameworks, it also becomes clear that we need not only look at example code of POC workflows, but actual production-ready workflows and pipelines. Such code often require a lot more functionality, including:\n\nError handling\nLogging\nData provenance\nParameterization\nTesting\nDocumentation\nContainerization\nResource management\n\n\n\n\n\nWratten, Laura, Andreas Wilm, and Jonathan Göke. 2021. “Reproducible, Scalable, and Shareable Analysis Pipelines with Bioinformatics Workflow Managers.” Nature Methods 18 (10): 1161–68. https://doi.org/10.1038/s41592-021-01254-9.",
    "crumbs": [
      "Workflow frameworks",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>A review</span>"
    ]
  },
  {
    "objectID": "book/workflow_frameworks/qualities.html",
    "href": "book/workflow_frameworks/qualities.html",
    "title": "9  Essential Qualities and Technologies which Enable Them",
    "section": "",
    "text": "9.1 Essential Qualities\nIn order to meet the demands of large-scale data processing, reproducibility, and collaboration, a production-ready workflow should exhibit the following essential qualities (Figure 9.1):",
    "crumbs": [
      "Workflow frameworks",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Essential qualities and technologies</span>"
    ]
  },
  {
    "objectID": "book/workflow_frameworks/qualities.html#essential-qualities",
    "href": "book/workflow_frameworks/qualities.html#essential-qualities",
    "title": "9  Essential Qualities and Technologies which Enable Them",
    "section": "",
    "text": "Figure 9.1: Essential qualities of a production-ready workflow.\n\n\n\n\nPolyglot: Seamlessly integrate tools and libraries from different programming languages, allowing you to leverage the strengths of each language for specific tasks. This facilitates the use of specialized tools and optimizes the analysis pipeline for performance and efficiency.\nModular: A well-structured workflow should be composed of modular and reusable components, promoting code maintainability and facilitating collaboration. Each module should have a clear purpose and well-defined inputs and outputs, enabling easy integration and replacement of individual steps within the pipeline.\nScalable: Single-cell datasets can be massive, and a production-ready workflow should be able to handle large volumes of data efficiently. This involves utilizing scalable compute environments, optimizing data storage and retrieval, and implementing parallelization strategies to accelerate analysis.\nReproducible: Ensuring reproducibility is crucial for scientific rigor and validation. A production-ready workflow should capture all the necessary information, including code, data, parameters, and software environments, to enable others to replicate the analysis and obtain consistent results.\nPortable: The workflow should be designed to run seamlessly across different computing platforms and environments, promoting accessibility and collaboration. Containerization technologies like Docker can help achieve portability by encapsulating the workflow and its dependencies into a self-contained unit.\nCommunity: Leveraging community resources, tools, and best practices can accelerate the development of production-ready workflows. This is because developing high-quality components can at times be time-consuming, and sharing resources can help reduce duplication of effort and promote collaboration.\nMaintainable: A production-ready workflow should be well-documented, organized, and easy to understand, facilitating updates, modifications, and troubleshooting. Clear documentation of code, data, and parameters ensures that the workflow remains accessible and usable over time.",
    "crumbs": [
      "Workflow frameworks",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Essential qualities and technologies</span>"
    ]
  },
  {
    "objectID": "book/workflow_frameworks/qualities.html#enabling-technologies",
    "href": "book/workflow_frameworks/qualities.html#enabling-technologies",
    "title": "9  Essential Qualities and Technologies which Enable Them",
    "section": "9.2 Enabling Technologies",
    "text": "9.2 Enabling Technologies\nThe essential qualities of a production-ready workflow are achieved through a combination of enabling technologies (Figure 9.2). These technologies provide the foundation for building scalable, reproducible, and maintainable workflows for single-cell analysis.\n\n\n\n\n\n\nFigure 9.2: The essential qualities of a production-ready workflow are achieved through a combination of enabling technologies.\n\n\n\n\nInteroperable file formats: Alreay discussed in the previous chapter, interoperable file formats like HDF5, Parquet, and Zarr facilitate data exchange between different tools and programming languages. These formats are optimized for efficient storage, retrieval, and processing of large-scale single-cell datasets.\nContainerisation: Support for containerization technologies like Docker and Singularity enables the encapsulation of workflows and their dependencies into portable, self-contained units. Containers provide a consistent execution environment across different computing platforms, ensuring reproducibility and portability.\nCompute environments: Support for scalable compute environments like cloud computing platforms, high-performance computing clusters, and distributed computing frameworks enables the efficient processing of large-scale single-cell datasets. These environments provide the computational resources necessary to accelerate analysis and handle massive volumes of data.\nStorage solutions: Integration with multiple scalable storage solutions like cloud object storage, distributed file systems, and database systems enables the efficient storage and retrieval of single-cell datasets. These solutions provide the necessary infrastructure to manage and access data across different computing platforms.\nData provenance: Support for data provenance tracking tools ensures the traceability and reproducibility of data analysis workflows. Data provenance tools capture metadata, code, and parameters associated with each analysis step, enabling the validation and replication of results.\nFile schemas: Support for standardized file schemas like Cellranger’s output format, loom files, and Anndata objects promotes interoperability and data exchange between different tools and workflows. Standardized file schemas ensure consistency and compatibility across the single-cell analysis ecosystem.\nBest practices: Adopting best practices like unit testing, versioned releases, continuous integration, automated reference documentation, community open-source components, and open-source workflows ensures the quality, reliability, and maintainability of production-ready workflows. These practices promote collaboration, transparency, and reproducibility in the development and deployment of single-cell analysis pipelines.",
    "crumbs": [
      "Workflow frameworks",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Essential qualities and technologies</span>"
    ]
  },
  {
    "objectID": "book/workflow_frameworks/quality_assessment.html",
    "href": "book/workflow_frameworks/quality_assessment.html",
    "title": "10  Quality Assessment of Workflow Frameworks",
    "section": "",
    "text": "10.1 Included frameworks\nThe following workflow frameworks were included in the assessment:",
    "crumbs": [
      "Workflow frameworks",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quality assessment</span>"
    ]
  },
  {
    "objectID": "book/workflow_frameworks/quality_assessment.html#included-frameworks",
    "href": "book/workflow_frameworks/quality_assessment.html#included-frameworks",
    "title": "10  Quality Assessment of Workflow Frameworks",
    "section": "",
    "text": "Nextflow: A domain-specific language for creating scalable and reproducible data analysis pipelines.    \nSnakemake: A workflow management system that uses Python-based rules to define dependencies and execute tasks.    \nGalaxy: A web-based platform for creating, running, and sharing data analysis workflows without the need for programming expertise.    \nCromwell + WDL: An execution engine (Cromwell) that uses a workflow description language (WDL) for defining and running scientific workflows, particularly in genomics and bioinformatics.    \nToil + WDL: A workflow engine (Toil) that leverages WDL to define and execute scientific workflows with a focus on scalability and fault tolerance.    \nArgo Workflows: A Kubernetes-native workflow engine for orchestrating containerized tasks and automating complex workflows.  \nViash + Nextflow: A combination of Viash, a tool for defining bioinformatics workflow components, and Nextflow for scalable and reproducible execution.",
    "crumbs": [
      "Workflow frameworks",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quality assessment</span>"
    ]
  },
  {
    "objectID": "book/workflow_frameworks/quality_assessment.html#quality-assessment-criteria",
    "href": "book/workflow_frameworks/quality_assessment.html#quality-assessment-criteria",
    "title": "10  Quality Assessment of Workflow Frameworks",
    "section": "10.2 Quality Assessment Criteria",
    "text": "10.2 Quality Assessment Criteria\nThe quality assessment was based on the following criteria:\n\nCommunityMaintainabilityModularityPolyglotPortabilityReproducibilityScalability\n\n\nDoes a library of components exist?\n\nA library of OSS components is available\nCommunity repository has frequent commits\nCommunity repository has &gt; 10 frequent non-employee commitors\nAre there repositories of OSS workflows available?\n\n\n\nHow easy is it to update, modify, and troubleshoot workflows?\n\nWorkflows are well-structured and easy to understand.\nFramework supports documenting the interfaces of components and workflows\nVersion control and collaboration features facilitate team-based development.\nComponents can be unit tested\nDocumentation specifies how to unit test components\n\n\n\nHow effectively does the framework promote modular design and reusability?\n\nSoftware can be easily encapsulated as a modular component.\nComponents have well-defined inputs and outputs.\nComponents can be shared and reused across different workflows.\nThe framework supports dependency management\nThe framework supports versioning of components.\n\n\n\nIs it easy to switch between different programming / scripting language within one workflow?\n\nCalling a script from another language is possible\nComponents can be written in multiple languages and communicate via a file-based interface\nComponents can be written in multiple languages and communicate via an in-memory data channel\n\n\n\nSupport for various compute platforms\n\nSupport for AWS Batch\nSupport for Azure Batch\nSupport for Google Cloud\nSupport for Kubernetes\nSupport for Local execution\nSupport for PBS/Torque\nSupport for SLURM\nSupport for additional compute platforms\n\nSupport for various containerization technologies\n\nSupport for Apptainer\nSupport for Docker\nSupport for Podman\nSupport for additional containerization technologies\n\nSupport for various storage solutions\n\nAWS S3\nAzure Blob Storage / Files\nGoogle Storage\nHTTPS\nFTP\nSupport for additional storage solutions\n\n\n\nHow effectively does the framework ensure reproducibility of results?\n\nIndividual components can list their software dependencies\nPer-component containerisation is supported\nExtending images with additional dependencies is supported\nData provenance tracking is built-in or can be easily integrated.\nFramework promotes versioned releases of the workflow software and images to ensure reproducibility\n\n\n\nHow well does the framework handle large and complex workflows?\n\nThe framework supports asynchronous and distributed execution.\nResource management and optimization features are available.\nPerformance monitoring and profiling tools are provided.\n\n\n\n\nThese criteria and subsequent scores will be further refined and validated as part of our ongoing research.",
    "crumbs": [
      "Workflow frameworks",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quality assessment</span>"
    ]
  },
  {
    "objectID": "book/workflow_frameworks/quality_assessment.html#quality-scores",
    "href": "book/workflow_frameworks/quality_assessment.html#quality-scores",
    "title": "10  Quality Assessment of Workflow Frameworks",
    "section": "10.3 Quality Scores",
    "text": "10.3 Quality Scores\nThe aggregated quality scores for each framework are shown below. The scores are based on the evaluation of the essential qualities of a production-ready workflow.\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_segment()`).\n\n\n\n\n\nQuality scores for different workflow frameworks.\n\n\n\n\nRaw scores and detailed explanations behind the reasoning of the resulting scores can be found in the Workflow Quality Assessment Spreadsheet.",
    "crumbs": [
      "Workflow frameworks",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quality assessment</span>"
    ]
  },
  {
    "objectID": "book/workflow_frameworks/quality_assessment.html#quality-assessment-contributors",
    "href": "book/workflow_frameworks/quality_assessment.html#quality-assessment-contributors",
    "title": "10  Quality Assessment of Workflow Frameworks",
    "section": "10.4 Quality assessment contributors",
    "text": "10.4 Quality assessment contributors\n\nJakub Majerčík\nMichaela Müller\nRobrecht Cannoodt\nToni Verbeiren",
    "crumbs": [
      "Workflow frameworks",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Quality assessment</span>"
    ]
  },
  {
    "objectID": "book/workflow_frameworks/viash_nextflow.html",
    "href": "book/workflow_frameworks/viash_nextflow.html",
    "title": "11  Viash + Nextflow: A use-case",
    "section": "",
    "text": "11.1 Bash\nPath: src/load_data",
    "crumbs": [
      "Workflow frameworks",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Example workflow</span>"
    ]
  },
  {
    "objectID": "book/workflow_frameworks/viash_nextflow.html#bash",
    "href": "book/workflow_frameworks/viash_nextflow.html#bash",
    "title": "11  Viash + Nextflow: A use-case",
    "section": "",
    "text": "config.vsh.yamlscript.shtest.sh\n\n\nname: load_data\ndescription: Load data from an S3 bucket\n\narguments:\n  - type: string\n    name: --url\n    description: URL to the data\n    example: s3://my-bucket/my-data.csv\n    required: true\n  - type: file\n    name: --output\n    description: Path to the output file\n    example: /path/to/output.csv\n    required: true\n    direction: output\n\nresources:\n  - type: bash_script\n    path: script.sh\n\ntest_resources:\n  - type: bash_script\n    path: test.sh\n\nengines:\n  - type: docker\n    image: amazon/aws-cli\n\nrunners:\n  - type: executable\n  - type: nextflow\n\n\n#!/bin/bash\n\naws s3 cp \\\n  --no-sign-request \\\n  \"$par_url\" \\\n  \"$par_output\"\n\n\n#!/bin/bash\n\n# Run the executable\n\"$meta_executable\" \\\n  --url s3://openproblems-bio/public/neurips-2023-competition/moa_annotations.csv \\\n  --output moa_annotations.csv\n\n# Check if the output file exists\nif [[ ! -f moa_annotations.csv ]]; then\n  echo \"File not found!\"\n  exit 1\nfi\n\n# Check if the output file has the correct MD5 sum\nif [[ \"$(md5sum moa_annotations.csv | cut -d ' ' -f 1)\" != \"80ebe44ce6b8d73f31dbc653787089f9\" ]]; then\n  echo \"MD5 sum does not match!\"\n  exit 1\nfi\n\necho \"All tests passed!\"",
    "crumbs": [
      "Workflow frameworks",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Example workflow</span>"
    ]
  },
  {
    "objectID": "book/workflow_frameworks/viash_nextflow.html#python",
    "href": "book/workflow_frameworks/viash_nextflow.html#python",
    "title": "11  Viash + Nextflow: A use-case",
    "section": "11.2 Python",
    "text": "11.2 Python\nPath: src/subset_obs\n\nconfig.vsh.yamlscript.pytest.py\n\n\nname: subset_obs\ndescription: Subset the observations of an AnnData object\n\nargument_groups:\n  - name: Inputs\n    arguments:\n      - type: file\n        name: --input\n        description: Path to the input h5ad file\n        example: /path/to/input.h5ad\n        required: true\n  - name: Subsetting arguments\n    arguments:\n      - type: string\n        name: --obs_column\n        description: Name of the column to subset on\n        example: cell_type\n        required: true\n      - type: string\n        name: --obs_values\n        description: List of values to subset on. If column is a boolean, do not pass any values to this argument.\n        multiple: true\n        example: [\"B cell\", \"T cell\"]\n      - type: boolean_true\n        name: --invert\n        description: Invert the subset\n  - name: Outputs\n    arguments:\n      - type: file\n        name: --output\n        description: Path to the output h5ad file\n        example: /path/to/output.h5ad\n        required: true\n        direction: output\n\nresources:\n  - type: python_script\n    path: script.py\n\ntest_resources:\n  - type: python_script\n    path: test.py\n\nengines:\n  - type: docker\n    image: python:3.10\n    setup:\n      - type: python\n        pypi:\n          - anndata\n    test_setup:\n      - type: python\n        pypi:\n          - viashpy\n\nrunners:\n  - type: executable\n  - type: nextflow\n\n\nimport anndata as ad\n\n## VIASH START\npar = {\"input\": \"\", \"obs_column\": \"\", \"obs_values\": [], \"invert\": False, \"output\": \"\"}\n## VIASH END\n\nprint(\"Load data\", flush=True)\nadata = ad.read_h5ad(par[\"input\"])\n\nprint(f\"Format of input data: {adata}\", flush=True)\n\nprint(\"Subset data\", flush=True)\nfilt = adata.obs[par[\"obs_column\"]]\n\n# if filt is a list of booleans\nassert (filt.dtype == bool) == (not par[\"obs_values\"]), \\\n  f\"If column '{par['obs_column']}' is boolean, 'obs_values' must be empty, and vice versa.\"\n\nif filt.dtype != bool:\n  # if filt is a list of strings\n  filt = filt.isin(par[\"obs_values\"])\n\nif par[\"invert\"]:\n  filt = ~filt\n\nadata = adata[filt].copy()\n\nprint(f\"Format of output data: {adata}\", flush=True)\n\nprint(\"Store to disk\", flush=True)\nadata.write_h5ad(par[\"output\"], compression=\"gzip\")\n\n\nimport sys\nimport anndata as ad\nimport pytest\nimport numpy as np\n\ndef test_subset_var(run_component, tmp_path):\n  input_path = tmp_path / \"input.h5ad\"\n  output_path = tmp_path / \"output.h5ad\"\n\n  # create data\n  adata_in = ad.AnnData(\n    X=np.random.rand(4, 2),\n    obs={\"cell_type\": [\"A\", \"B\", \"C\", \"D\"]},\n    var={\"highly_variable\": [True, False]},\n  )\n\n  adata_in.write_h5ad(input_path)\n\n  # run component\n  run_component([\n    \"--input\", str(input_path),\n    \"--obs_column\", \"cell_type\",\n    \"--obs_values\", \"A;B\",\n    \"--output\", str(output_path),\n  ])\n\n  # load output\n  adata_out = ad.read_h5ad(output_path)\n\n  # check output\n  assert adata_out.X.shape == (2, 2)\n  assert adata_out.obs[\"cell_type\"].tolist() == [\"A\", \"B\"]\n  assert adata_out.var[\"highly_variable\"].tolist() == [True, False]\n\n\nif __name__ == \"__main__\":\n  sys.exit(pytest.main([__file__]))",
    "crumbs": [
      "Workflow frameworks",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Example workflow</span>"
    ]
  },
  {
    "objectID": "book/workflow_frameworks/viash_nextflow.html#r",
    "href": "book/workflow_frameworks/viash_nextflow.html#r",
    "title": "11  Viash + Nextflow: A use-case",
    "section": "11.3 R",
    "text": "11.3 R\nPath: src/differential_expression\n\nconfig.vsh.yamlscript.Rtest.R\n\n\nname: differential_expression\ndescription: Compute differential expression between two observation types\n\nargument_groups:\n  - name: Inputs\n    arguments:\n      - type: file\n        name: --input\n        description: Path to the input h5ad file\n        example: /path/to/input.h5ad\n        required: true\n  - name: Differential expression arguments\n    arguments:\n      - type: string\n        name: --contrast\n        description: |\n          Contrast to compute. Must be of length 3:\n\n          1. The name of the column to contrast on\n          2. The name of the first observation type\n          3. The name of the second observation type\n        example: [\"cell_type\", \"batch\", \"sample\"]\n        multiple: true\n        required: true\n      - type: string\n        name: --design_formula\n        description: Design formula for the differential expression model\n        example: ~ batch + cell_type\n  - name: Outputs\n    arguments:\n      - type: file\n        name: --output\n        description: Path to the output h5ad file\n        example: /path/to/output.h5ad\n        required: true\n        direction: output\n\nresources:\n  - type: r_script\n    path: script.R\n\ntest_resources:\n  - type: r_script\n    path: test.R\n\nengines:\n  - type: docker\n    image: rocker/r2u:22.04\n    setup:\n      - type: apt\n        packages:\n          - python3\n          - python3-pip\n          - python3-dev\n          - python-is-python3\n      - type: python\n        pypi:\n          - anndata\n      - type: r\n        cran:\n          - anndata\n          - processx\n        bioc:\n          - DESeq2\n\nrunners:\n  - type: executable\n  - type: nextflow\n\n\nlibrary(anndata)\nrequireNamespace(\"DESeq2\", quietly = TRUE)\n\n## VIASH START\npar &lt;- list(input = \"\", contrast = c(), design_formula = \"\", output = \"\")\n## VIASH END\n\ncat(\"Reading data\\n\")\nadata &lt;- read_h5ad(par$input)\n\ncat(\"Parse formula\\n\")\nformula &lt;- as.formula(par$design_formula)\n\ncat(\"Create DESeq dataset\\n\")\n# transform counts matrix\ncount_data &lt;- t(as.matrix(adata$X))\nstorage.mode(count_data) &lt;- \"integer\"\n\n# create dataset\ndds &lt;- DESeq2::DESeqDataSetFromMatrix(\n  countData = count_data,\n  colData = adata$obs,\n  design = formula\n)\n\ncat(\"Run DESeq2\\n\")\ndds &lt;- DESeq2::DESeq(dds)\n\nres &lt;- DESeq2::results(dds, contrast = par$contrast) |&gt;\n  as.data.frame()\n\ncat(\"Write to disk\\n\")\ncontrast_names &lt;- gsub(\" \", \"_\", par$contrast)\ncontrast_names &lt;- gsub(\"[^[:alnum:]]\", \"_\", contrast_names)\ncontrast_names &lt;- gsub(\"__\", \"_\", contrast_names)\ncontrast_names &lt;- tolower(contrast_names)\n\nvarm_name &lt;- paste0(\"de_\", paste(contrast_names, collapse = \"_\"))\nadata$varm[[varm_name]] &lt;- res\n\n# Save adata\nzzz &lt;- adata$write_h5ad(par$output, compression = \"gzip\")\n\n\nlibrary(anndata)\n\ncat(\"Create input data\\n\")\nX &lt;- matrix(runif(100, 10, 100), nrow = 10, ncol = 10)\nfor (i in 1:10) {\n  X[1:5, i] &lt;- X[1:5, i] + i * 10\n}\nadata_in &lt;- AnnData(\n  X = X,\n  obs = data.frame(\n    row.names = paste0(\"cell\", 1:10),\n    sm_name = rep(c(\"Belinostat\", \"Dimethyl Sulfoxide\"), each = 5),\n    plate_name = rep(c(\"plate1\", \"plate2\"), times = 5)\n  ),\n  var = data.frame(\n    row.names = paste0(\"gene\", 1:10)\n  )\n)\n\ncat(\"Write input data to file\\n\")\ninput_path &lt;- \"input.h5ad\"\noutput_path &lt;- \"output.h5ad\"\nzzz &lt;- adata_in$write_h5ad(input_path)\n\ncat(\"Run component\\n\")\nzzz &lt;- processx::run(\n  command = meta$executable,\n  args = c(\n    \"--input\", input_path,\n    \"--contrast\", \"sm_name;Dimethyl Sulfoxide;Belinostat\",\n    \"--design_formula\", \"~ sm_name + plate_name\",\n    \"--output\", output_path\n  ),\n  error_on_status = TRUE,\n  echo = TRUE\n)\n\ncat(\"Read output data\\n\")\nadata_out &lt;- read_h5ad(output_path)\n\ncat(\"Preview output data\\n\")\nprint(adata_out)\n\ncat(\"Check DE results:\\n\")\nde_out &lt;- adata_out$varm$de_sm_name_dimethyl_sulfoxide_belinostat\nif (is.null(de_out)) {\n  stop(\"No DE results found\")\n}\n\nprint(de_out)\n\nexpected_colnames &lt;- c(\"baseMean\", \"log2FoldChange\", \"lfcSE\", \"stat\", \"pvalue\", \"padj\")\nif (!all(colnames(de_out) == expected_colnames)) {\n  stop(paste0(\n    \"Column names do not match.\\n\",\n    \"Expected: \", paste(expected_colnames, collapse = \", \"), \"\\n\",\n    \"Actual: \", paste(colnames(de_out), collapse = \", \")\n  ))\n}\n\ncat(\"Done\\n\")",
    "crumbs": [
      "Workflow frameworks",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Example workflow</span>"
    ]
  },
  {
    "objectID": "book/workflow_frameworks/viash_nextflow.html#nextflow",
    "href": "book/workflow_frameworks/viash_nextflow.html#nextflow",
    "title": "11  Viash + Nextflow: A use-case",
    "section": "11.4 Nextflow",
    "text": "11.4 Nextflow\nPath: src/workflow\n\nconfig.vsh.yamlmain.nf\n\n\nname: workflow\ndescription: |\n  A workflow to compute differential expression between two groups of cells in an AnnData object.\n\nargument_groups:\n  - name: Inputs\n    arguments:\n      - type: file\n        name: --input\n        description: Path to the input h5ad file\n        example: s3://my-bucket/my-data.h5ad\n        required: true\n  - name: Differential expression arguments\n    arguments:\n      - type: string\n        name: --contrast\n        description: |\n          Contrast to compute. Must be of length 3:\n\n          1. The name of the column to contrast on\n          2. The name of the first observation type\n          3. The name of the second observation type\n        example: [\"cell_type\", \"batch\", \"sample\"]\n        multiple: true\n        required: true\n      - type: string\n        name: --design_formula\n        description: Design formula for the differential expression model\n        example: ~ batch + cell_type\n  - name: Outputs\n    arguments:\n      - type: file\n        name: --output\n        description: Path to the output h5ad file\n        example: /path/to/output.h5ad\n        required: true\n        direction: output\n\nresources:\n  - type: nextflow_script\n    path: main.nf\n    entrypoint: wf\n\ndependencies:\n  - name: subset_obs\n  - name: subset_var\n  - name: compute_pseudobulk\n  - name: differential_expression\n\nrunners:\n  - type: nextflow\n\n\nworkflow wf {\n  take:\n  ch_in\n\n  main:\n  ch_out = ch_in\n\n    | subset_obs.run(\n      key: \"subset_sm_name\",\n      fromState: [\"input\": \"input\"],\n      args: [\n        \"obs_column\": \"sm_name\",\n        \"obs_values\": [\"Belinostat\", \"Dimethyl Sulfoxide\"]\n      ],\n      toState: [\"data\": \"output\"]\n    )\n\n    | subset_obs.run(\n      key: \"subset_cell_type\",\n      fromState: [\"input\": \"data\"],\n      args: [\n        \"obs_column\": \"cell_type\",\n        \"obs_values\": [\"T cells\"]\n      ],\n      toState: [\"data\": \"output\"]\n    )\n\n    | subset_var.run(\n      fromState: [\"input\": \"data\"],\n      args: [\n        \"var_column\": \"highly_variable\",\n      ],\n      toState: [\"data\": \"output\"]\n    )\n\n    | compute_pseudobulk.run(\n      fromState: [\"input\": \"data\"],\n      args: [\n        \"obs_column_index\": \"plate_well_celltype_reannotated\",\n        \"obs_column_values\": [\"sm_name\", \"cell_type\", \"plate_name\", \"well\"],\n      ],\n      toState: [\"data\": \"output\"]\n    )\n\n    | differential_expression.run(\n      fromState: [\n        \"input\": \"data\",\n        \"contrast\": \"contrast\",\n        \"design_formula\": \"design_formula\"\n      ],\n      toState: [\"data\": \"output\"]\n    )\n\n    | setState([\"output\": \"data\"])\n    \n  emit:\n  ch_out\n}",
    "crumbs": [
      "Workflow frameworks",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Example workflow</span>"
    ]
  },
  {
    "objectID": "book/workflow_frameworks/viash_nextflow.html#running-the-workflow",
    "href": "book/workflow_frameworks/viash_nextflow.html#running-the-workflow",
    "title": "11  Viash + Nextflow: A use-case",
    "section": "11.5 Running the workflow",
    "text": "11.5 Running the workflow\nTo run the workflow, you must first build the project:\nviash ns build --parallel --setup cachedbuild\nExporting load_data =executable=&gt; target/executable/load_data\n[notice] Building container 'polygloty_usecase/load_data:0.1.0' with Dockerfile\nExporting load_data =nextflow=&gt; target/nextflow/load_data\nExporting compute_pseudobulk =executable=&gt; target/executable/compute_pseudobulk\n[notice] Building container 'polygloty_usecase/compute_pseudobulk:0.1.0' with Dockerfile\nExporting compute_pseudobulk =nextflow=&gt; target/nextflow/compute_pseudobulk\nExporting subset_obs =executable=&gt; target/executable/subset_obs\n[notice] Building container 'polygloty_usecase/subset_obs:0.1.0' with Dockerfile\nExporting subset_obs =nextflow=&gt; target/nextflow/subset_obs\nExporting subset_var =executable=&gt; target/executable/subset_var\n[notice] Building container 'polygloty_usecase/subset_var:0.1.0' with Dockerfile\nExporting subset_var =nextflow=&gt; target/nextflow/subset_var\nExporting differential_expression =executable=&gt; target/executable/differential_expression\n[notice] Building container 'polygloty_usecase/differential_expression:0.1.0' with Dockerfile\nExporting differential_expression =nextflow=&gt; target/nextflow/differential_expression\nExporting workflow =nextflow=&gt; target/nextflow/workflow\nAll 11 configs built successfully\nThen, you can run the workflow:\nnextflow run \\\n  target/nextflow/workflow/main.nf \\\n  -with-docker \\\n  --id dataset \\\n  --input s3://openproblems-bio/public/neurips-2023-competition/sc_counts_reannotated_with_counts.h5ad \\\n  --contrast 'sm_name;Belinostat;Dimethyl Sulfoxide' \\\n  --design_formula '~ sm_name + plate_name' \\\n  --publish_dir output\nN E X T F L O W  ~  version 23.10.0\nLaunching `target/nextflow/workflow/main.nf` [condescending_engelbart] DSL2 - revision: f54b192abd\nexecutor &gt;  local (6)\n[e2/da368b] process &gt; workflow:wf:subset_sm_name:processWf:subset_sm_name_process (dataset)                   [100%] 1 of 1 ✔\n[d5/fea947] process &gt; workflow:wf:subset_cell_type:processWf:subset_cell_type_process (dataset)               [100%] 1 of 1 ✔\n[23/a2b0a7] process &gt; workflow:wf:subset_var:processWf:subset_var_process (dataset)                           [100%] 1 of 1 ✔\n[55/a59f07] process &gt; workflow:wf:compute_pseudobulk:processWf:compute_pseudobulk_process (dataset)           [100%] 1 of 1 ✔\n[10/5c0650] process &gt; workflow:wf:differential_expression:processWf:differential_expression_process (dataset) [100%] 1 of 1 ✔\n[91/5f7431] process &gt; workflow:publishStatesSimpleWf:publishStatesProc (dataset)                              [100%] 1 of 1 ✔\nCompleted at: 07-Sep-2024 21:33:16\nDuration    : 8m 59s\nCPU hours   : (a few seconds)\nSucceeded   : 6\nThe workflow will process the dataset, subset the data, compute pseudobulk samples, and perform differential expression analysis. The results will be saved in the output directory.",
    "crumbs": [
      "Workflow frameworks",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Example workflow</span>"
    ]
  },
  {
    "objectID": "book/workflow_frameworks/viash_nextflow.html#workflow-output",
    "href": "book/workflow_frameworks/viash_nextflow.html#workflow-output",
    "title": "11  Viash + Nextflow: A use-case",
    "section": "11.6 Workflow Output",
    "text": "11.6 Workflow Output\ntree output\noutput/\n├── dataset.workflow.output.h5ad\n└── dataset.workflow.state.yaml\n\n1 directory, 2 files\nThe resulting pseudobulk samples and differential expression analysis results are stored in the dataset.workflow.output.h5ad file.\nimport anndata as ad\n\nadata = ad.read(\"output/dataset.workflow.output.h5ad\")\n\nadata\nAnnData object with n_obs × n_vars = 96 × 2000\n    obs: 'sm_name', 'cell_type', 'plate_name', 'well'\n    var: 'highly_variable', 'means', 'dispersions', 'dispersions_norm'\n    varm: 'de_sm_name_belinostat_dimethyl_sulfoxide'\nadata.varm[\"de_sm_name_belinostat_dimethyl_sulfoxide\"]\n          baseMean  log2FoldChange     lfcSE      stat        pvalue          padj\nA2M      12.320857       -0.696714  0.113490 -6.138993  8.304620e-10  2.699001e-09\nA2M-AS1  31.563323       -0.652155  0.074559 -8.746780  2.195265e-18  1.065788e-17\nA2MP1     1.712214       -0.844991  0.262029 -3.224806  1.260582e-03  2.551988e-03\nAARD      0.125909        0.086614  1.234496  0.070161  9.440653e-01           NaN\nABCA1     2.003082       -0.154770  0.240886 -0.642504  5.205457e-01  6.193785e-01\n...            ...             ...       ...       ...           ...           ...\nZNF860    0.024027        0.214981  2.915260  0.073743  9.412147e-01           NaN\nZNF876P   0.568178       -0.703739  0.422580 -1.665340  9.584497e-02  1.486826e-01\nZNF891   28.045500        0.515844  0.066416  7.766889  8.043700e-15  3.338454e-14\nZNF92    46.860620        0.076892  0.048781  1.576261  1.149657e-01  1.743887e-01\nZNF99     0.000000             NaN       NaN       NaN           NaN           NaN\n\n[2000 rows x 6 columns]",
    "crumbs": [
      "Workflow frameworks",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Example workflow</span>"
    ]
  },
  {
    "objectID": "book/workflow_frameworks/best_practices.html",
    "href": "book/workflow_frameworks/best_practices.html",
    "title": "12  Best Practices",
    "section": "",
    "text": "To ensure that your workflow is production-ready, consider the following best practices:\n\nVersion control: Use a version control system like Git to track changes to your workflow code and configuration files. This allows you to collaborate with others, revert to previous versions, and maintain a history of your work.\nAutomated testing: Implement automated tests to validate the correctness of your workflow components and ensure that changes do not introduce regressions. This includes unit tests, integration tests, and end-to-end tests.\nContinuous integration: Set up a continuous integration (CI) pipeline to automatically build, test, and deploy your workflow whenever changes are made to the codebase. This helps catch errors early and ensures that your workflow remains functional.\nDocumentation: Document your workflow code, configuration, and usage to make it easier for others to understand and use your workflow. Include information on how to run the workflow, what inputs are required, and what outputs are produced.\nContainerization: Use containerization technologies like Docker to package your workflow and its dependencies into a self-contained unit. This ensures that your workflow runs consistently across different environments and platforms.\nResource management: Optimize the use of computational resources by parallelizing tasks, optimizing memory usage, and monitoring resource consumption. This helps improve the performance and scalability of your workflow.\nError handling: Implement robust error handling mechanisms to gracefully handle failures and exceptions during workflow execution. This includes logging errors, retrying failed tasks, and notifying users of issues.\nData provenance: Capture metadata about the inputs, outputs, and parameters of your workflow to enable reproducibility and traceability. This includes recording information about the data sources, processing steps, and results produced by the workflow.\nVersioned releases: Create versioned releases of your workflow and accompanying container images to ensure that users can reproduce the exact results of a specific analysis. This involves tagging releases, documenting changes, and archiving previous versions.",
    "crumbs": [
      "Workflow frameworks",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Best practices</span>"
    ]
  },
  {
    "objectID": "book/book_slides.html",
    "href": "book/book_slides.html",
    "title": "Slides",
    "section": "",
    "text": "Here are the slides used during the workshop:\n    View slides in full screen\n       \n      \n    \n  \n  Download PDF File\n   \n    Unable to display PDF file. Download instead.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "book/references.html",
    "href": "book/references.html",
    "title": "References",
    "section": "",
    "text": "Heumos, Lukas, Anna C. Schaar, Christopher Lance, Anastasia\nLitinetskaya, Felix Drost, Luke Zappia, Malte D. Lücken, et al. 2023.\n“Best Practices for Single-Cell Analysis Across\nModalities.” Nature Reviews Genetics 24 (8): 550–72. https://doi.org/10.1038/s41576-023-00586-w.\n\n\n“OP3 H5AD on S3.” 2024. https://openproblems-bio.s3.amazonaws.com/public/neurips-2023-competition/sc_counts_reannotated_with_counts.h5ad.\n\n\n“Open Problems Kaggle Competition - Single Cell\nPerturbations.” 2023. https://www.kaggle.com/competitions/open-problems-single-cell-perturbations/overview.\n\n\n“OpenProblems Perturbation Prediction Benchmark.” 2024. https://openproblems.bio/results/perturbation_prediction/.\n\n\n“SRA SRP527159.” 2024. https://trace.ncbi.nlm.nih.gov/Traces/?view=study&acc=SRP527159.\n\n\nWratten, Laura, Andreas Wilm, and Jonathan Göke. 2021.\n“Reproducible, Scalable, and Shareable Analysis Pipelines with\nBioinformatics Workflow Managers.” Nature Methods 18\n(10): 1161–68. https://doi.org/10.1038/s41592-021-01254-9.\n\n\nZappia, Luke, and Fabian J. Theis. 2021. “Over 1000 Tools Reveal\nTrends in the Single-Cell RNA-Seq Analysis Landscape.” Genome\nBiology 22 (1). https://doi.org/10.1186/s13059-021-02519-4.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "book/in_memory/index.html",
    "href": "book/in_memory/index.html",
    "title": "In-memory interoperability",
    "section": "",
    "text": "One aproach to interoperability is to work on in-memory representations of one object, and convert these in memory between different programming languages. This does not require you to write out your datasets and read them in in the different programming enivronment, but it does require you to set up an environment in both languages, which can be cumbersome.\nOne language will act as the main language, and you will intereact with the other language using an FFI (foreign function interface).\n\n\n\n\n\n\nFigure 1: A schematic overview\n\n\n\nWhen evaluating R code within a Python program, we will make use of Rpy2 to accomplish this. When evaluating Python code within an R program, we will make use of Reticulate.",
    "crumbs": [
      "In-memory interoperability"
    ]
  },
  {
    "objectID": "book/disk_based/index.html",
    "href": "book/disk_based/index.html",
    "title": "Disk-based interoperability",
    "section": "",
    "text": "Disk-based interoperability is a strategy for achieving interoperability between tools written in different programming languages by storing intermediate results in standardized, language-agnostic file formats. This approach allows for sequential execution of scripts written in different languages, enabling researchers to leverage the best tools for each analysis step.\nThe upside of this approach is that it is relatively simple as the scripts are mostly unchanged, only prepended with a reading operation and appended with writing operation. Each script can be written and tested independently in their suitable respective frameworks. This modular polyglotism of disk-based interoperability is one of the key strengths of workflow languages like Nextflow and Snakemake.\nThe downside is that it can lead to increased storage requirements and I/O overhead which grows with the amount of scripts. As disk serialization and deserialization can be much slower than memory operations, this SerDe problem can become a problem for very large datasets. Debugging is only possible for individual scripts and the workflow is not as interactive and explorative as the in-memory approach.",
    "crumbs": [
      "Disk-based interoperability"
    ]
  },
  {
    "objectID": "book/workflow_frameworks/index.html",
    "href": "book/workflow_frameworks/index.html",
    "title": "Production-ready Workflows",
    "section": "",
    "text": "Productionization of Single-Cell Analysis Workflows\nProductionization is the process of transforming research-oriented analysis pipelines into robust, scalable, and maintainable workflows that can be reliably executed in a production environment (Figure 1). This transition is essential for ensuring the reproducibility of results, facilitating collaboration among researchers, and enabling the efficient processing of large and complex single-cell datasets.",
    "crumbs": [
      "Workflow frameworks"
    ]
  },
  {
    "objectID": "book/workflow_frameworks/index.html#productionization-of-single-cell-analysis-workflows",
    "href": "book/workflow_frameworks/index.html#productionization-of-single-cell-analysis-workflows",
    "title": "Production-ready Workflows",
    "section": "",
    "text": "Figure 1: An example of the productionization process for single-cell analysis workflows. A) The research environment is characterized by scattered data, manual steps, and ad-hoc analysis pipelines. B) The production environment is streamlined, automated, and standardized, with reproducibility engines in place.",
    "crumbs": [
      "Workflow frameworks"
    ]
  },
  {
    "objectID": "book/workflow_frameworks/index.html#but-how-to-ensure-that-your-workflow-is-production-ready",
    "href": "book/workflow_frameworks/index.html#but-how-to-ensure-that-your-workflow-is-production-ready",
    "title": "Production-ready Workflows",
    "section": "But how to ensure that your workflow is production-ready?",
    "text": "But how to ensure that your workflow is production-ready?\nIn this chapter, we will explore:\n\nKey qualities of workflows built to stand the test of time\nWhich technologies and workflow frameworks contribute to these qualities\nBest practices to keep in mind during development",
    "crumbs": [
      "Workflow frameworks"
    ]
  }
]