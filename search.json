[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Polyglot programming for single-cell analysis",
    "section": "",
    "text": "Preface\nThis book is a collection of notebooks and explanations for the workshop on Polyglot programming for single-cell analysis given at the scverse Conference 2024. For more information, please visit the workshop page.\nIn order to use the best performing methods for each step of the single-cell analysis process, bioinformaticians need to use multiple ecosystems and programming languages. This is unfortunately not that straightforward. This workshop gives an overview of the different levels of interoperability, and how it is possible to integrate them in a single workflow.\nTo get started, read the Introduction chapter.\nTo learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "book/intro.html",
    "href": "book/intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Any bioinformatician that has analysed a single-cell dataset knows that using methods developed for different ecosystems or programming languages is necessary but painful. Any package developer has asked themselves the question on how to best provide access to their tool or method.\nWe will give an overview of the interoperability tools you can use when analysing a single-cell dataset: do you want to convert your data to a different data format, or is just calling one R function in your Jupyter notebook sufficient? Do you want fine-grained control over each step in the analysis pipeline or do you run a series of scripts that you really should convert to a workflow system?\nWe will give information on different options for package developers to provide better interoperability. Should you reimplement your package in a new language? How do you ensure that the results are the same?\nIn order to follow this workshop, we expect the participants to have some Python or R programming knowledge.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "notebooks/usecase.html",
    "href": "notebooks/usecase.html",
    "title": "2  Use-case",
    "section": "",
    "text": "2.1 1. Retrieving the data\nThe dataset has since been uploaded to SRA (“SRA SRP527159” 2024), will be uploaded to GEO, and is currently available from S3 (“OP3 H5AD on S3” 2024).\nIf you haven’t already, you can download the dataset from S3 using the following command:\nif [[ ! -f usecase_data/sc_counts_reannotated_with_counts.h5ad ]]; then\n  aws s3 cp \\\n    --no-sign-request \\\n    s3://openproblems-bio/public/neurips-2023-competition/sc_counts_reannotated_with_counts.h5ad \\\n    usecase_data/sc_counts_reannotated_with_counts.h5ad\nfi",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Use-case</span>"
    ]
  },
  {
    "objectID": "notebooks/usecase.html#loading-the-data",
    "href": "notebooks/usecase.html#loading-the-data",
    "title": "2  Use-case",
    "section": "2.2 2. Loading the data",
    "text": "2.2 2. Loading the data\nThe dataset is stored in an AnnData object, which can be loaded in Python as follows:\n\nimport anndata as ad\n\nadata = ad.read_h5ad(\"usecase_data/sc_counts_reannotated_with_counts.h5ad\")\n\nadata\n\nAnnData object with n_obs × n_vars = 298087 × 21265\n    obs: 'dose_uM', 'timepoint_hr', 'well', 'row', 'col', 'plate_name', 'cell_id', 'cell_type', 'split', 'donor_id', 'sm_name', 'control', 'SMILES', 'sm_lincs_id', 'library_id', 'leiden_res1', 'group', 'cell_type_orig', 'plate_well_celltype_reannotated', 'cell_count_by_well_celltype', 'cell_count_by_plate_well'\n    var: 'highly_variable', 'means', 'dispersions', 'dispersions_norm'\n    uns: 'cell_type_colors', 'celltypist_celltype_colors', 'donor_id_colors', 'hvg', 'leiden_res1_colors', 'log1p', 'neighbors', 'over_clustering', 'rank_genes_groups'\n    obsm: 'HTO_clr', 'X_pca', 'X_umap', 'protein_counts'\n    obsp: 'connectivities', 'distances'\n\n\nThe same code can be run in R using the anndata package (not run):\nlibrary(anndata)\n\nadata &lt;- read_h5ad(\"usecase_data/sc_counts_reannotated_with_counts.h5ad\")\n\nadata",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Use-case</span>"
    ]
  },
  {
    "objectID": "notebooks/usecase.html#subset-data",
    "href": "notebooks/usecase.html#subset-data",
    "title": "2  Use-case",
    "section": "2.3 3. Subset data",
    "text": "2.3 3. Subset data\nSince the dataset is large, we will subset the data to a single small molecule, control, and cell type.\n\nsm_name = \"Belinostat\"\ncontrol_name = \"Dimethyl Sulfoxide\"\ncell_type = \"T cells\"\n\nadata = adata[\n  adata.obs[\"sm_name\"].isin([sm_name, control_name]) &\n  adata.obs[\"cell_type\"].isin([cell_type]),\n].copy()\n\nWe will also subset the genes to the top 2000 most variable genes.\n\nadata = adata[:, adata.var[\"highly_variable\"]].copy()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Use-case</span>"
    ]
  },
  {
    "objectID": "notebooks/usecase.html#compute-pseudobulk",
    "href": "notebooks/usecase.html#compute-pseudobulk",
    "title": "2  Use-case",
    "section": "2.4 4. Compute pseudobulk",
    "text": "2.4 4. Compute pseudobulk\n\nimport pandas as pd\n\nCombine data in a single data frame and compute pseudobulk\n\ncombined = pd.DataFrame(\n  adata.X.toarray(),\n  index=adata.obs[\"plate_well_celltype_reannotated\"],\n)\ncombined.columns = adata.var_names\npb_X = combined.groupby(level=0).sum()\n\n&lt;string&gt;:1: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\nConstruct obs for pseudobulk. Use ‘plate_well_celltype_reannotated’ as index and make sure to retain the columns ‘sm_name’, ‘cell_type’, and ‘plate_name’:\n\npb_obs = adata.obs[[\"sm_name\", \"cell_type\", \"plate_name\", \"well\"]].copy()\npb_obs.index = adata.obs[\"plate_well_celltype_reannotated\"]\npb_obs = pb_obs.drop_duplicates()\n\nCreate AnnData object:\n\npb_adata = ad.AnnData(\n  X=pb_X.loc[pb_obs.index].values,\n  obs=pb_obs,\n  var=adata.var,\n)\n\nStore to disk:\n\npb_adata.write_h5ad(\"usecase_data/pseudobulk.h5ad\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Use-case</span>"
    ]
  },
  {
    "objectID": "notebooks/usecase.html#compute-de",
    "href": "notebooks/usecase.html#compute-de",
    "title": "2  Use-case",
    "section": "2.5 5. Compute DE",
    "text": "2.5 5. Compute DE\n\nlibrary(anndata)\nlibrary(dplyr, warn.conflicts = FALSE)\n\npb_adata &lt;- read_h5ad(\"usecase_data/pseudobulk.h5ad\")\n\nSelect small molecule and control:\n\nsm_name &lt;- \"Belinostat\"\ncontrol_name &lt;- \"Dimethyl Sulfoxide\"\n\nCreate DESeq dataset:\n\n# transform counts matrix\ncount_data &lt;- t(pb_adata$X)\nstorage.mode(count_data) &lt;- \"integer\"\n\n# create dataset\ndds &lt;- DESeq2::DESeqDataSetFromMatrix(\n  countData = count_data,\n  colData = pb_adata$obs,\n  design = ~ sm_name + plate_name,\n)\n\nWarning: replacing previous import 'S4Arrays::makeNindexFromArrayViewport' by\n'DelayedArray::makeNindexFromArrayViewport' when loading 'SummarizedExperiment'\n\n\n  Note: levels of factors in the design contain characters other than\n  letters, numbers, '_' and '.'. It is recommended (but not required) to use\n  only letters, numbers, and delimiters '_' or '.', as these are safe characters\n  for column names in R. [This is a message, not a warning or an error]\n\n\nRun DESeq2:\n\ndds &lt;- DESeq2::DESeq(dds)\n\nestimating size factors\n\n\n  Note: levels of factors in the design contain characters other than\n  letters, numbers, '_' and '.'. It is recommended (but not required) to use\n  only letters, numbers, and delimiters '_' or '.', as these are safe characters\n  for column names in R. [This is a message, not a warning or an error]\n\n\nestimating dispersions\n\n\ngene-wise dispersion estimates\n\n\nmean-dispersion relationship\n\n\n  Note: levels of factors in the design contain characters other than\n  letters, numbers, '_' and '.'. It is recommended (but not required) to use\n  only letters, numbers, and delimiters '_' or '.', as these are safe characters\n  for column names in R. [This is a message, not a warning or an error]\n\n\nfinal dispersion estimates\n\n\nfitting model and testing\n\n\n  Note: levels of factors in the design contain characters other than\n  letters, numbers, '_' and '.'. It is recommended (but not required) to use\n  only letters, numbers, and delimiters '_' or '.', as these are safe characters\n  for column names in R. [This is a message, not a warning or an error]\n\n\nGet results:\n\nres &lt;- DESeq2::results(dds, contrast=c(\"sm_name\", sm_name, control_name)) |&gt;\n  as.data.frame()\n\nPreview results:\n\nres |&gt;\n  arrange(padj) |&gt;\n  head(10)\n\n          baseMean log2FoldChange      lfcSE      stat        pvalue\nBEX5      59.24944       2.187350 0.05660399  38.64304  0.000000e+00\nHIST1H1D 301.38741       1.356543 0.03092962  43.85901  0.000000e+00\nSTMN1    234.72112       2.224633 0.04104002  54.20642  0.000000e+00\nPCSK1N    64.91604       1.899149 0.05480612  34.65214 4.147855e-263\nGZMM     141.39238      -1.309959 0.03806665 -34.41224 1.654371e-259\nMARCKSL1  95.82726       1.423057 0.04311798  33.00380 7.163953e-239\nH1FX     376.28247       1.054890 0.03221858  32.74168 3.988563e-235\nHIST1H1B  30.81805       4.317984 0.14074738  30.67896 1.086254e-206\nFXYD7     61.11526       2.331406 0.07725771  30.17700 4.746707e-200\nING2      79.68893       1.218777 0.04336609  28.10437 8.663682e-174\n                  padj\nBEX5      0.000000e+00\nHIST1H1D  0.000000e+00\nSTMN1     0.000000e+00\nPCSK1N   1.631144e-260\nGZMM     5.204651e-257\nMARCKSL1 1.878150e-236\nH1FX     8.962871e-233\nHIST1H1B 2.135848e-204\nFXYD7    8.296189e-198\nING2     1.362797e-171\n\n\nWrite to disk:\n\nwrite.csv(res, \"usecase_data/de_contrasts.csv\")\n\n\n\n\n\n“OP3 H5AD on S3.” 2024. https://openproblems-bio.s3.amazonaws.com/public/neurips-2023-competition/sc_counts_reannotated_with_counts.h5ad.\n\n\n“Open Problems Kaggle Competition - Single Cell Perturbations.” 2023. https://www.kaggle.com/competitions/open-problems-single-cell-perturbations/overview.\n\n\n“OpenProblems Perturbation Prediction Benchmark.” 2024. https://openproblems.bio/results/perturbation_prediction/.\n\n\n“SRA SRP527159.” 2024. https://trace.ncbi.nlm.nih.gov/Traces/?view=study&acc=SRP527159.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Use-case</span>"
    ]
  },
  {
    "objectID": "notebooks/file_formats.html",
    "href": "notebooks/file_formats.html",
    "title": "3  File formats",
    "section": "",
    "text": "4 File formats\nData format based interoperability",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>File formats</span>"
    ]
  },
  {
    "objectID": "notebooks/file_formats.html#setup",
    "href": "notebooks/file_formats.html#setup",
    "title": "3  File formats",
    "section": "4.1 Setup",
    "text": "4.1 Setup\n\nimport anndata\nimport numpy\nimport scanpy\n\n\nanndata.__version__\n\n'0.10.9'",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>File formats</span>"
    ]
  },
  {
    "objectID": "notebooks/in_memory.html",
    "href": "notebooks/in_memory.html",
    "title": "4  In memory interoperability (from Python)",
    "section": "",
    "text": "In this notebook, we will showcase how to call R code from Python. We will make use of rpy2 and anndata2ri.\nMake sure you have downloaded the data.\nRead in the anndata object\n\nimport anndata as ad\n\nadata_path = \"usecase_data/sc_counts_reannotated_with_counts.h5ad\"\nadata = ad.read_h5ad(adata_path)\n\nWe can use rpy2 to run R code within a Python process. If you wish to convert numpy matrices, you need to use the right convertor.\nThis is an example of how you import rpy2, and convert a matrix for use in R functions.\n\ncounts = adata.X # matrices are columnn major in R, and row-major in Python\ncounts = counts[:100, :1000] # subset for speed of example\ncounts_dense = counts.todense() # sparse matrices are not supported in rpy2\n\n\nimport rpy2\nimport rpy2.robjects as robjects\n\nfrom rpy2.robjects import numpy2ri\nfrom rpy2.robjects import default_converter\n\nnp_cv_rules = default_converter + numpy2ri.converter\n\nwith np_cv_rules.context() as cv:\n    robjects.globalenv[\"counts_matrix\"] = counts_dense\n\n    dim = robjects.r[\"dim\"]\n    print(dim(robjects.globalenv[\"counts_matrix\"]))\n\n[ 100 1000]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>In memory interoperability (from Python)</span>"
    ]
  },
  {
    "objectID": "notebooks/workflows/index.html",
    "href": "notebooks/workflows/index.html",
    "title": "5  Workflows",
    "section": "",
    "text": "5.1 Qualities of a Production-Ready Workflow\nBuilding production-ready workflows for single-cell analysis involves integrating a variety of tools, technologies, and best practices. Here are some key qualities of a production-ready workflow:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Workflows</span>"
    ]
  },
  {
    "objectID": "notebooks/workflows/index.html#qualities-of-a-production-ready-workflow",
    "href": "notebooks/workflows/index.html#qualities-of-a-production-ready-workflow",
    "title": "5  Workflows",
    "section": "",
    "text": "Polyglot: Seamlessly integrate tools and libraries from different programming languages, allowing you to leverage the strengths of each language for specific tasks. This facilitates the use of specialized tools and optimizes the analysis pipeline for performance and efficiency.\nModular: A well-structured workflow should be composed of modular and reusable components, promoting code maintainability and facilitating collaboration. Each module should have a clear purpose and well-defined inputs and outputs, enabling easy integration and replacement of individual steps within the pipeline.\nScalable: Single-cell datasets can be massive, and a production-ready workflow should be able to handle large volumes of data efficiently. This involves utilizing scalable compute environments, optimizing data storage and retrieval, and implementing parallelization strategies to accelerate analysis.\nReproducible: Ensuring reproducibility is crucial for scientific rigor and validation. A production-ready workflow should capture all the necessary information, including code, data, parameters, and software environments, to enable others to replicate the analysis and obtain consistent results.\nPortable: The workflow should be designed to run seamlessly across different computing platforms and environments, promoting accessibility and collaboration. Containerization technologies like Docker can help achieve portability by encapsulating the workflow and its dependencies into a self-contained unit.\nAutomated: Automating routine tasks and data processing steps reduces manual intervention, minimizes errors, and improves efficiency. Workflow management systems can automate the execution of complex pipelines, handling dependencies, parallelization, and error recovery.\nMaintainable: A production-ready workflow should be well-documented, organized, and easy to understand, facilitating updates, modifications, and troubleshooting. Clear documentation of code, data, and parameters ensures that the workflow remains accessible and usable over time.\n\n\n\n\n\n\n\nFigure 5.2: Qualities",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Workflows</span>"
    ]
  },
  {
    "objectID": "notebooks/workflows/index.html#key-components",
    "href": "notebooks/workflows/index.html#key-components",
    "title": "5  Workflows",
    "section": "5.2 Key components",
    "text": "5.2 Key components\n\n\n\nOverview of bioinformatics analysis workflows using an example of transcript expression quantifications (Wratten, Wilm, and Göke 2021).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Workflows</span>"
    ]
  },
  {
    "objectID": "notebooks/workflows/index.html#data-storage",
    "href": "notebooks/workflows/index.html#data-storage",
    "title": "5  Workflows",
    "section": "5.3 Data Storage",
    "text": "5.3 Data Storage",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Workflows</span>"
    ]
  },
  {
    "objectID": "notebooks/workflows/index.html#compute-environments",
    "href": "notebooks/workflows/index.html#compute-environments",
    "title": "5  Workflows",
    "section": "5.4 Compute Environments",
    "text": "5.4 Compute Environments",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Workflows</span>"
    ]
  },
  {
    "objectID": "notebooks/workflows/index.html#containerization",
    "href": "notebooks/workflows/index.html#containerization",
    "title": "5  Workflows",
    "section": "5.5 Containerization",
    "text": "5.5 Containerization",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Workflows</span>"
    ]
  },
  {
    "objectID": "notebooks/workflows/index.html#workflow-management-systems",
    "href": "notebooks/workflows/index.html#workflow-management-systems",
    "title": "5  Workflows",
    "section": "5.6 Workflow Management Systems",
    "text": "5.6 Workflow Management Systems\n\n\n\nTable 5.1: Overview of workflow managers for bioinformatics (Wratten, Wilm, and Göke 2021).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTool\nClass\nEase of use\nExpressiveness\nPortability\nScalability\nLearning resources\nPipeline initiatives\n\n\n\n\nGalaxy\nGraphical\n●●●\n●○○\n●●●\n●●●\n●●●\n●●○\n\n\nKNIME\nGraphical\n●●●\n●○○\n○○○\n●●◐\n●●●\n●●○\n\n\nNextflow\nDSL\n●●○\n●●●\n●●●\n●●●\n●●●\n●●●\n\n\nSnakemake\nDSL\n●●○\n●●●\n●●◐\n●●●\n●●○\n●●●\n\n\nGenPipes\nDSL\n●●○\n●●●\n●●○\n●●○\n●●○\n●●○\n\n\nbPipe\nDSL\n●●○\n●●●\n●●○\n●●◐\n●●○\n●○○\n\n\nPachyderm\nDSL\n●●○\n●●●\n●○○\n●●○\n●●●\n○○○\n\n\nSciPipe\nLibrary\n●●○\n●●●\n○○○\n○○○\n●●○\n○○○\n\n\nLuigi\nLibrary\n●●○\n●●●\n●○○\n●●◐\n●●○\n○○○\n\n\nCromwell + WDL\nExecution + workflow specification\n●○○\n●●○\n●●●\n●●◐\n●●○\n●●○\n\n\ncwltool + CWL\nExecution + workflow specification\n●○○\n●●○\n●●◐\n○○○\n●●●\n●●○\n\n\nToil + CWL/WDL/Python\nExecution + workflow specification\n●○○\n●●●\n●◐○\n●●●\n●●○\n●●○\n\n\n\n\n\n\n\n\n\nTable 5.2: Different approaches to Nextflow programming. Not shown: Portability: ●●●, Scalability: ●●●.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTool(s)\nDialect\nEase of use\nExpressiveness\nModularity\nSeparation of concerns\nLearning resources\nPipeline initiatives\n\n\n\n\nNextflow\nDSL1\n●○○\n●●◐\n●○○\n●○○\n●●●\n●○○\n\n\nNextflow\nDSL2\n●●○\n●●●\n●◐○\n●◐○\n●●○\n●●○\n\n\nNextflow + Groovy\nnf-core\n●●○\n●●●\n●●○\n●◐○\n●○○\n●●●\n\n\nViash + Nextflow\nVDSL3\n●●●\n●●●\n●●●\n●●●\n●○○[^1]\n●●○",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Workflows</span>"
    ]
  },
  {
    "objectID": "notebooks/workflows/index.html#best-practices",
    "href": "notebooks/workflows/index.html#best-practices",
    "title": "5  Workflows",
    "section": "5.7 Best Practices",
    "text": "5.7 Best Practices",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Workflows</span>"
    ]
  },
  {
    "objectID": "notebooks/workflows/index.html#conclusion",
    "href": "notebooks/workflows/index.html#conclusion",
    "title": "5  Workflows",
    "section": "5.8 Conclusion",
    "text": "5.8 Conclusion\n\n\n\n\nHeumos, Lukas, Anna C. Schaar, Christopher Lance, Anastasia Litinetskaya, Felix Drost, Luke Zappia, Malte D. Lücken, et al. 2023. “Best Practices for Single-Cell Analysis Across Modalities.” Nature Reviews Genetics 24 (8): 550–72. https://doi.org/10.1038/s41576-023-00586-w.\n\n\nWratten, Laura, Andreas Wilm, and Jonathan Göke. 2021. “Reproducible, Scalable, and Shareable Analysis Pipelines with Bioinformatics Workflow Managers.” Nature Methods 18 (10): 1161–68. https://doi.org/10.1038/s41592-021-01254-9.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Workflows</span>"
    ]
  },
  {
    "objectID": "book/book_slides.html",
    "href": "book/book_slides.html",
    "title": "Slides",
    "section": "",
    "text": "Here are the slides used during the workshop:\n    View slides in full screen\n       \n      \n    \n  \n  Download PDF File\n   \n    Unable to display PDF file. Download instead.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "book/references.html",
    "href": "book/references.html",
    "title": "References",
    "section": "",
    "text": "Heumos, Lukas, Anna C. Schaar, Christopher Lance, Anastasia\nLitinetskaya, Felix Drost, Luke Zappia, Malte D. Lücken, et al. 2023.\n“Best Practices for Single-Cell Analysis Across\nModalities.” Nature Reviews Genetics 24 (8): 550–72. https://doi.org/10.1038/s41576-023-00586-w.\n\n\n“OP3 H5AD on S3.” 2024. https://openproblems-bio.s3.amazonaws.com/public/neurips-2023-competition/sc_counts_reannotated_with_counts.h5ad.\n\n\n“Open Problems Kaggle Competition - Single Cell\nPerturbations.” 2023. https://www.kaggle.com/competitions/open-problems-single-cell-perturbations/overview.\n\n\n“OpenProblems Perturbation Prediction Benchmark.” 2024. https://openproblems.bio/results/perturbation_prediction/.\n\n\n“SRA SRP527159.” 2024. https://trace.ncbi.nlm.nih.gov/Traces/?view=study&acc=SRP527159.\n\n\nWratten, Laura, Andreas Wilm, and Jonathan Göke. 2021.\n“Reproducible, Scalable, and Shareable Analysis Pipelines with\nBioinformatics Workflow Managers.” Nature Methods 18\n(10): 1161–68. https://doi.org/10.1038/s41592-021-01254-9.",
    "crumbs": [
      "References"
    ]
  }
]