[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Polyglot programming for single-cell analysis",
    "section": "",
    "text": "Preface\nThis book is a collection of notebooks and explanations for the workshop on Polyglot programming for single-cell analysis given at the scverse Conference 2024. For more information, please visit the workshop page.\nIn order to use the best performing methods for each step of the single-cell analysis process, bioinformaticians need to use multiple ecosystems and programming languages. This is unfortunately not that straightforward. This workshop gives an overview of the different levels of interoperability, and how it is possible to integrate them in a single workflow.\nTo get started, read the Introduction chapter.\nTo learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "book/introduction.html",
    "href": "book/introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Code porting\nPorting tools from one language to another can offer complete control and eliminate interoperability concerns. However, one should not underestimate the effort required to reimplement complex algorithms, and the risk of introducing errors.\nFurthermore, work is not done after the initial port – in order for the researcher’s work to be useful to others, the ported code must be maintained and kept up-to-date with the original implementation. For this reason, we don’t consider reimplementation a viable option for most use-cases and will not discuss it further in this book.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "book/introduction.html#in-memory-interoperability",
    "href": "book/introduction.html#in-memory-interoperability",
    "title": "1  Introduction",
    "section": "1.2 In-memory Interoperability",
    "text": "1.2 In-memory Interoperability\nTools like rpy2 and reticulate allow for direct communication between languages within a single analysis session. This approach provides flexibility and avoids intermediate file I/O, but can introduce complexity in managing dependencies and environments.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "book/introduction.html#file-based-interoperability",
    "href": "book/introduction.html#file-based-interoperability",
    "title": "1  Introduction",
    "section": "1.3 File-based Interoperability",
    "text": "1.3 File-based Interoperability\nStoring intermediate results in standardized, language-agnostic file formats (e.g., HDF5, Parquet) allows for sequential execution of scripts written in different languages. This approach is relatively simple but can lead to increased storage requirements and I/O overhead.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "book/introduction.html#workflow-frameworks",
    "href": "book/introduction.html#workflow-frameworks",
    "title": "1  Introduction",
    "section": "1.4 Workflow Frameworks",
    "text": "1.4 Workflow Frameworks\nWorkflow management systems (e.g., Nextflow, Snakemake) provide a structured approach to orchestrate complex, multi-language pipelines, enhancing reproducibility and automation. However, they may require a learning curve and additional configuration.\n\n\n\n\nHeumos, Lukas, Anna C. Schaar, Christopher Lance, Anastasia Litinetskaya, Felix Drost, Luke Zappia, Malte D. Lücken, et al. 2023. “Best Practices for Single-Cell Analysis Across Modalities.” Nature Reviews Genetics 24 (8): 550–72. https://doi.org/10.1038/s41576-023-00586-w.\n\n\nZappia, Luke, and Fabian J. Theis. 2021. “Over 1000 Tools Reveal Trends in the Single-Cell RNA-Seq Analysis Landscape.” Genome Biology 22 (1). https://doi.org/10.1186/s13059-021-02519-4.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "book/usecase/index.html",
    "href": "book/usecase/index.html",
    "title": "2  Use-case",
    "section": "",
    "text": "2.1 1. Retrieving the data\nThe dataset has since been uploaded to SRA (“SRA SRP527159” 2024), will be uploaded to GEO, and is currently available from S3 (“OP3 H5AD on S3” 2024).\nIf you haven’t already, you can download the dataset from S3 using the following command:\nif [[ ! -f data/sc_counts_reannotated_with_counts.h5ad ]]; then\n  aws s3 cp \\\n    --no-sign-request \\\n    s3://openproblems-bio/public/neurips-2023-competition/sc_counts_reannotated_with_counts.h5ad \\\n    data/sc_counts_reannotated_with_counts.h5ad\nfi",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Use-case</span>"
    ]
  },
  {
    "objectID": "book/usecase/index.html#loading-the-data",
    "href": "book/usecase/index.html#loading-the-data",
    "title": "2  Use-case",
    "section": "2.2 2. Loading the data",
    "text": "2.2 2. Loading the data\nThe dataset is stored in an AnnData object, which can be loaded in Python as follows:\n\nimport anndata as ad\n\nadata = ad.read_h5ad(\"data/sc_counts_reannotated_with_counts.h5ad\")\n\nadata\n\nAnnData object with n_obs × n_vars = 298087 × 21265\n    obs: 'dose_uM', 'timepoint_hr', 'well', 'row', 'col', 'plate_name', 'cell_id', 'cell_type', 'split', 'donor_id', 'sm_name', 'control', 'SMILES', 'sm_lincs_id', 'library_id', 'leiden_res1', 'group', 'cell_type_orig', 'plate_well_celltype_reannotated', 'cell_count_by_well_celltype', 'cell_count_by_plate_well'\n    var: 'highly_variable', 'means', 'dispersions', 'dispersions_norm'\n    uns: 'cell_type_colors', 'celltypist_celltype_colors', 'donor_id_colors', 'hvg', 'leiden_res1_colors', 'log1p', 'neighbors', 'over_clustering', 'rank_genes_groups'\n    obsm: 'HTO_clr', 'X_pca', 'X_umap', 'protein_counts'\n    obsp: 'connectivities', 'distances'\n\n\nThe same code can be run in R using the anndata package (not run):\nlibrary(anndata)\n\nadata &lt;- read_h5ad(\"data/sc_counts_reannotated_with_counts.h5ad\")\n\nadata",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Use-case</span>"
    ]
  },
  {
    "objectID": "book/usecase/index.html#subset-data",
    "href": "book/usecase/index.html#subset-data",
    "title": "2  Use-case",
    "section": "2.3 3. Subset data",
    "text": "2.3 3. Subset data\nSince the dataset is large, we will subset the data to a single small molecule, control, and cell type.\n\nsm_name = \"Belinostat\"\ncontrol_name = \"Dimethyl Sulfoxide\"\ncell_type = \"T cells\"\n\nadata = adata[\n  adata.obs[\"sm_name\"].isin([sm_name, control_name]) &\n  adata.obs[\"cell_type\"].isin([cell_type]),\n].copy()\n\nWe will also subset the genes to the top 2000 most variable genes.\n\nadata = adata[:, adata.var[\"highly_variable\"]].copy()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Use-case</span>"
    ]
  },
  {
    "objectID": "book/usecase/index.html#compute-pseudobulk",
    "href": "book/usecase/index.html#compute-pseudobulk",
    "title": "2  Use-case",
    "section": "2.4 4. Compute pseudobulk",
    "text": "2.4 4. Compute pseudobulk\n\nimport pandas as pd\n\nCombine data in a single data frame and compute pseudobulk\n\ncombined = pd.DataFrame(\n  adata.X.toarray(),\n  index=adata.obs[\"plate_well_celltype_reannotated\"],\n)\ncombined.columns = adata.var_names\npb_X = combined.groupby(level=0).sum()\n\n&lt;string&gt;:1: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\nConstruct obs for pseudobulk. Use ‘plate_well_celltype_reannotated’ as index and make sure to retain the columns ‘sm_name’, ‘cell_type’, and ‘plate_name’:\n\npb_obs = adata.obs[[\"sm_name\", \"cell_type\", \"plate_name\", \"well\"]].copy()\npb_obs.index = adata.obs[\"plate_well_celltype_reannotated\"]\npb_obs = pb_obs.drop_duplicates()\n\nCreate AnnData object:\n\npb_adata = ad.AnnData(\n  X=pb_X.loc[pb_obs.index].values,\n  obs=pb_obs,\n  var=adata.var,\n)\n\nStore to disk:\n\npb_adata.write_h5ad(\"data/pseudobulk.h5ad\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Use-case</span>"
    ]
  },
  {
    "objectID": "book/usecase/index.html#compute-de",
    "href": "book/usecase/index.html#compute-de",
    "title": "2  Use-case",
    "section": "2.5 5. Compute DE",
    "text": "2.5 5. Compute DE\n\nlibrary(anndata)\nlibrary(dplyr, warn.conflicts = FALSE)\n\npb_adata &lt;- read_h5ad(\"data/pseudobulk.h5ad\")\n\nSelect small molecule and control:\n\nsm_name &lt;- \"Belinostat\"\ncontrol_name &lt;- \"Dimethyl Sulfoxide\"\n\nCreate DESeq dataset:\n\n# transform counts matrix\ncount_data &lt;- t(pb_adata$X)\nstorage.mode(count_data) &lt;- \"integer\"\n\n# create dataset\ndds &lt;- DESeq2::DESeqDataSetFromMatrix(\n  countData = count_data,\n  colData = pb_adata$obs,\n  design = ~ sm_name + plate_name,\n)\n\nWarning: replacing previous import 'S4Arrays::makeNindexFromArrayViewport' by\n'DelayedArray::makeNindexFromArrayViewport' when loading 'SummarizedExperiment'\n\n\n  Note: levels of factors in the design contain characters other than\n  letters, numbers, '_' and '.'. It is recommended (but not required) to use\n  only letters, numbers, and delimiters '_' or '.', as these are safe characters\n  for column names in R. [This is a message, not a warning or an error]\n\n\nRun DESeq2:\n\ndds &lt;- DESeq2::DESeq(dds)\n\nestimating size factors\n\n\n  Note: levels of factors in the design contain characters other than\n  letters, numbers, '_' and '.'. It is recommended (but not required) to use\n  only letters, numbers, and delimiters '_' or '.', as these are safe characters\n  for column names in R. [This is a message, not a warning or an error]\n\n\nestimating dispersions\n\n\ngene-wise dispersion estimates\n\n\nmean-dispersion relationship\n\n\n  Note: levels of factors in the design contain characters other than\n  letters, numbers, '_' and '.'. It is recommended (but not required) to use\n  only letters, numbers, and delimiters '_' or '.', as these are safe characters\n  for column names in R. [This is a message, not a warning or an error]\n\n\nfinal dispersion estimates\n\n\nfitting model and testing\n\n\n  Note: levels of factors in the design contain characters other than\n  letters, numbers, '_' and '.'. It is recommended (but not required) to use\n  only letters, numbers, and delimiters '_' or '.', as these are safe characters\n  for column names in R. [This is a message, not a warning or an error]\n\n\nGet results:\n\nres &lt;- DESeq2::results(dds, contrast=c(\"sm_name\", sm_name, control_name)) |&gt;\n  as.data.frame()\n\nPreview results:\n\nres |&gt;\n  arrange(padj) |&gt;\n  head(10)\n\n          baseMean log2FoldChange      lfcSE      stat        pvalue\nBEX5      59.24944       2.187350 0.05660399  38.64304  0.000000e+00\nHIST1H1D 301.38741       1.356543 0.03092962  43.85901  0.000000e+00\nSTMN1    234.72112       2.224633 0.04104002  54.20642  0.000000e+00\nPCSK1N    64.91604       1.899149 0.05480612  34.65214 4.147855e-263\nGZMM     141.39238      -1.309959 0.03806665 -34.41224 1.654371e-259\nMARCKSL1  95.82726       1.423057 0.04311798  33.00380 7.163953e-239\nH1FX     376.28247       1.054890 0.03221858  32.74168 3.988563e-235\nHIST1H1B  30.81805       4.317984 0.14074738  30.67896 1.086254e-206\nFXYD7     61.11526       2.331406 0.07725771  30.17700 4.746707e-200\nING2      79.68893       1.218777 0.04336609  28.10437 8.663682e-174\n                  padj\nBEX5      0.000000e+00\nHIST1H1D  0.000000e+00\nSTMN1     0.000000e+00\nPCSK1N   1.631144e-260\nGZMM     5.204651e-257\nMARCKSL1 1.878150e-236\nH1FX     8.962871e-233\nHIST1H1B 2.135848e-204\nFXYD7    8.296189e-198\nING2     1.362797e-171\n\n\nWrite to disk:\n\nwrite.csv(res, \"data/de_contrasts.csv\")\n\n\n\n\n\n“OP3 H5AD on S3.” 2024. https://openproblems-bio.s3.amazonaws.com/public/neurips-2023-competition/sc_counts_reannotated_with_counts.h5ad.\n\n\n“Open Problems Kaggle Competition - Single Cell Perturbations.” 2023. https://www.kaggle.com/competitions/open-problems-single-cell-perturbations/overview.\n\n\n“OpenProblems Perturbation Prediction Benchmark.” 2024. https://openproblems.bio/results/perturbation_prediction/.\n\n\n“SRA SRP527159.” 2024. https://trace.ncbi.nlm.nih.gov/Traces/?view=study&acc=SRP527159.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Use-case</span>"
    ]
  },
  {
    "objectID": "book/in_memory_interoperability.html",
    "href": "book/in_memory_interoperability.html",
    "title": "3  In-memory interoperability",
    "section": "",
    "text": "3.1 Rpy2: basic functionality\nRpy2 is a foreign function interface to R. It can be used in the following way:\nimport rpy2\nimport rpy2.robjects as robjects\n\n/home/runner/work/polygloty/polygloty/renv/python/virtualenvs/renv-python-3.12/lib/python3.12/site-packages/rpy2/rinterface_lib/embedded.py:276: UserWarning: R was initialized outside of rpy2 (R_NilValue != NULL). Trying to use it nevertheless.\n  warnings.warn(msg)\nR was initialized outside of rpy2 (R_NilValue != NULL). Trying to use it nevertheless.\n\nvector = robjects.IntVector([1,2,3])\nrsum = robjects.r['sum']\n\nrsum(vector)\n\n\n        IntVector with 1 elements.\n        \n\n\n\n6\nLuckily, we’re not restricted to just calling R functions and creating R objects. The real power of this in-memory interoperability lies in the conversion of Python objects to R objects to call R functions on, and then to the conversion of the results back to Python objects.\nRpy2 requires specific conversion rules for different Python objects. It is straightforward to create R vectors from corresponding Python lists:\nstr_vector = robjects.StrVector(['abc', 'def', 'ghi'])\nflt_vector = robjects.FloatVector([0.3, 0.8, 0.7])\nint_vector = robjects.IntVector([1, 2, 3])\nmtx = robjects.r.matrix(robjects.IntVector(range(10)), nrow=5)\nHowever, for single cell biology, the objects that are most interesting to convert are (count) matrices, arrays and dataframes. In order to do this, you need to import the corresponding rpy2 modules and specify the conversion context.\nimport numpy as np\n\nfrom rpy2.robjects import numpy2ri\nfrom rpy2.robjects import default_converter\n\nrd_m = np.random.random((10, 7))\n\nwith (default_converter + numpy2ri.converter).context():\n    mtx2 = robjects.r.matrix(rd_m, nrow = 10)\nimport pandas as pd\n\nfrom rpy2.robjects import pandas2ri\n\npd_df = pd.DataFrame({'int_values': [1,2,3],\n                      'str_values': ['abc', 'def', 'ghi']})\n\nwith (default_converter + pandas2ri.converter).context():\n    pd_df_r = robjects.DataFrame(pd_df)\nOne big limitation of rpy2 is the inability to convert sparse matrices: there is no built-in conversion module for scipy. The anndata2ri package provides, apart from functionality to convert SingleCellExperiment objects to an anndata objects, functions to convert sparse matrices.\nTODO: how to subscript sparse matrix? Is it possible?\nimport scipy as sp\n\nfrom anndata2ri import scipy2ri\n\nsparse_matrix = sp.sparse.csc_matrix(rd_m)\n\nwith (default_converter + scipy2ri.converter).context():\n    sp_r = scipy2ri.py2rpy(sparse_matrix)\nWe will showcase how to use anndata2ri to convert an anndata object to a SingleCellExperiment object and vice versa as well:\nimport anndata as ad\nimport scanpy.datasets as scd\n\nimport anndata2ri\n\nadata_paul = scd.paul15()\n\n\n  0%|          | 0.00/9.82M [00:00&lt;?, ?B/s]\n  0%|          | 8.00k/9.82M [00:00&lt;02:22, 72.2kB/s]\n  0%|          | 32.0k/9.82M [00:00&lt;01:07, 153kB/s] \n  1%|          | 96.0k/9.82M [00:00&lt;00:30, 337kB/s]\n  2%|1         | 200k/9.82M [00:00&lt;00:18, 558kB/s] \n  4%|4         | 408k/9.82M [00:00&lt;00:09, 1.01MB/s]\n  8%|8         | 840k/9.82M [00:00&lt;00:04, 1.93MB/s]\n 17%|#6        | 1.65M/9.82M [00:00&lt;00:02, 3.69MB/s]\n 34%|###3      | 3.33M/9.82M [00:00&lt;00:00, 7.23MB/s]\n 60%|######    | 5.90M/9.82M [00:01&lt;00:00, 10.8MB/s]\n 86%|########5 | 8.44M/9.82M [00:01&lt;00:00, 14.5MB/s]\n100%|##########| 9.82M/9.82M [00:01&lt;00:00, 8.29MB/s]\n\n\nwith anndata2ri.converter.context():\n    sce = anndata2ri.py2rpy(adata_paul)\n    ad2 = anndata2ri.rpy2py(sce)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>In-memory interoperability</span>"
    ]
  },
  {
    "objectID": "book/in_memory_interoperability.html#reticulate-basic-functionality",
    "href": "book/in_memory_interoperability.html#reticulate-basic-functionality",
    "title": "3  In-memory interoperability",
    "section": "3.2 Reticulate: basic functionality",
    "text": "3.2 Reticulate: basic functionality",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>In-memory interoperability</span>"
    ]
  },
  {
    "objectID": "book/in_memory_interoperability.html#interactive-sessions",
    "href": "book/in_memory_interoperability.html#interactive-sessions",
    "title": "3  In-memory interoperability",
    "section": "3.3 Interactive sessions",
    "text": "3.3 Interactive sessions\nOne of the most useful ways to take advantage of in-memory interoperability is to use it in interactive sessions, where you’re exploring the data and want to try out some functions non-native to your language of choice.\nJupyter notebooks (and some other notebooks) make this possible from the Python side: using IPython line and cell magic and rpy2, you can easily run an R jupyter cell in your notebooks.\n```python show_magic %load_ext rpy2.ipython # line magic that loads the rpy2 ipython extension. # this extension allows the use of the following cell magic\n%%R -i input -o output # this line allows to specify inputs (which will be converted to R objects) and outputs (which will be converted back to Python objects) # this line is put at the start of a cell # the rest of the cell will be able to be ran as R code\n\nRmarkdown and knitr provide a user friendly way to run code chunks in different languages.\n\n## Usecase\n\n### Usecase: ran in Python\n\nWe will perform the Compute DE step not in R, but in Python\nThe pseudobulked data is read in:\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport anndata as ad\n\npd_adata = ad.read_h5ad(\"usecase/data/pseudobulk.h5ad\")\n:::\nSelect small molecule and control:\n\nsm_name = \"Belinostat\"\ncontrol_name = \"Dimethyl Sulfoxide\"\n\nCreating a DESeq dataset: This requires a bit more effort: we need to import the DESeq2 package, and combine the default, numpy2ri and pandas2ri converter to convert the count matrix and the obs dataframe.\n\nimport numpy as np\n\nimport rpy2\nimport rpy2.robjects as robjects\n\nfrom rpy2.robjects import numpy2ri\nfrom rpy2.robjects import pandas2ri\n\nfrom rpy2.robjects import default_converter\nfrom rpy2.robjects.packages import importr\n\nDESeq2 = importr(\"DESeq2\")\n\nnp_cv_rules = default_converter + numpy2ri.converter + pandas2ri.converter\n\nwith np_cv_rules.context() as cv:\n    counts_dense = np.transpose(pd_adata.X.astype(np.int32))\n\n    robjects.globalenv[\"count_data\"] = counts_dense\n    robjects.globalenv[\"obs_data\"] = pd_adata.obs\n\nWe can also specify R formulas!\n\nfrom rpy2.robjects import Formula\n\ndesign_formula = Formula('~ sm_name + plate_name')\n\ndds = DESeq2.DESeqDataSetFromMatrix(countData = robjects.globalenv[\"count_data\"],\n        colData = robjects.globalenv[\"obs_data\"],\n        design = design_formula)\n\nRun DESeq2:\n\ndds = DESeq2.DESeq(dds)\n\nGet results:\n\ncontrastv = robjects.StrVector([\"sm_name\", sm_name, control_name])\nres = DESeq2.results(dds, contrast=contrastv)\n\nbase = importr('base')\nres = base.as_data_frame(res)\n\nPreview results:\n\ndplyr = importr('dplyr')\nutils = importr('utils')\n\nres = utils.head(dplyr.arrange(res, 'padj'), 10)\n\nWrite to disk: this again requires the pandas2ri converter to convert the results to a pandas dataframe.\n\nwith (robjects.default_converter + pandas2ri.converter).context():\n    res_pd = robjects.conversion.get_conversion().rpy2py(res)\n\n    res_pd.to_csv(\"usecase/data/de_contrasts.csv\")\n\n\n3.3.1 Usecase: ran in R",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>In-memory interoperability</span>"
    ]
  },
  {
    "objectID": "book/on_disk_interoperability.html",
    "href": "book/on_disk_interoperability.html",
    "title": "4  On-disk interoperability",
    "section": "",
    "text": "4.1 Different file formats\nIt’s important to differentiate between language specific file formats and language agnostic file formats. For example, most languages can serialize objects to disk. R has the .RDS file format and Python has the .pickle/.pkl file format, but these are not interoperable. Older versions of the language could also have problems reading in serialized objects created by the latest language version.\nFor a file format to be language agnostic, it should have a mature standard describing how the data is stored on disk. This standard should be implemented in multiple languages, documented and tested for compatibility. Some file formats have a reference implementation in C, which can be used to create bindings for other languages. Most also have a status page that list the implementations and which version or how much of the standard they support.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>On-disk interoperability</span>"
    ]
  },
  {
    "objectID": "book/on_disk_interoperability.html#different-file-formats",
    "href": "book/on_disk_interoperability.html#different-file-formats",
    "title": "4  On-disk interoperability",
    "section": "",
    "text": "4.1.1 Dataframes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFile Format\nSupport in Python\nSupport in R\nText-Based\nBinary\nHigh-Performance\nEase-of-Use\nCompression\nSuitable for Single-Cell\nSuitable for Spatial\n\n\n\n\nCSV\nYes\nYes\nYes\nNo\nNo\nHigh\nNo\nLimited\nNo\n\n\nJSON\nYes\nYes\nYes\nNo\nNo\nMedium\nNo\nLimited\nNo\n\n\nParquet\nYes\nYes\nNo\nYes\nYes\nHigh\nYes\nLimited\nNo\n\n\nFeather\nYes\nYes\nNo\nYes\nYes\nHigh\nYes\nYes\nNo\n\n\nHDF5\nYes\nYes\nNo\nYes\nYes\nMedium\nYes\nYes\nLimited\n\n\n\n\n\n4.1.2 n-dimensional arrays\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFile Format\nSupport in Python\nSupport in R\nText-Based\nBinary\nHigh-Performance\nEase-of-Use\nCompression\nSuitable for Single-Cell\nSuitable for Spatial\n\n\n\n\nh5ad\nYes\nLimited\nNo\nYes\nYes\nMedium\nYes\nYes\nYes\n\n\nZarr\nYes\nYes\nNo\nYes\nYes\nMedium\nYes\nYes\nYes\n\n\nNumPy (npy)\nYes\nLimited\nNo\nYes\nYes\nHigh\nNo\nYes\nNo\n\n\nNetCDF\nYes\nYes\nNo\nYes\nYes\nMedium\nYes\nYes\nYes\n\n\nTIFF\nYes\nLimited\nNo\nYes\nMedium\nMedium\nYes\nNo\nYes",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>On-disk interoperability</span>"
    ]
  },
  {
    "objectID": "book/on_disk_interoperability.html#different-on-disk-pipelines",
    "href": "book/on_disk_interoperability.html#different-on-disk-pipelines",
    "title": "4  On-disk interoperability",
    "section": "4.2 Different on-disk pipelines",
    "text": "4.2 Different on-disk pipelines\nYou can use a shell script to run the pipeline in a sequential manner. This requires all the dependencies to be installed in one large environment.\nUsually you start in a notebook with an exploratory analysis, then move to a script for reproducibility and finally to a pipeline for scalability.\nThe scripts in such a script pipeline are a collection of the code snippets from the notebooks and can be written in different languages and executed in sequence.\nAlternatively, there are frameworks that keep the notebooks and create a pipeline with it. The upside is that you can avoid converting the code snippets in the notebooks to scripts. The downside is that you have to use a specific framework and the notebooks can become very large and unwieldy.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>On-disk interoperability</span>"
    ]
  },
  {
    "objectID": "book/on_disk_interoperability.html#notebook-pipelines",
    "href": "book/on_disk_interoperability.html#notebook-pipelines",
    "title": "4  On-disk interoperability",
    "section": "4.3 Notebook pipelines",
    "text": "4.3 Notebook pipelines\nYou can use Quarto to run code snippets in different languages in the same .qmd notebook. Our Use-case chapter is one example of this.\nFor example, [Papermill]https://github.com/nteract/papermill) can execute Jupyter notebooks in sequence and pass variables between them. Ploomber is another example.\n\n4.3.1 Execute notebooks via the CLI\nJupyter via nbconvert.:\njupyter nbconvert --to notebook --execute my_notebook.ipynb --allow-errors --output-dir outputs/\nRMarkdown:\nRscript -e \"rmarkdown::render('my_notebook.Rmd',params=list(args = myarg))\"",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>On-disk interoperability</span>"
    ]
  },
  {
    "objectID": "book/on_disk_interoperability.html#script-pipelines",
    "href": "book/on_disk_interoperability.html#script-pipelines",
    "title": "4  On-disk interoperability",
    "section": "4.4 Script pipelines",
    "text": "4.4 Script pipelines\n\n4.4.1 Calling scripts in the same environment\nTODO: test these snippets\nFrom Bash:\n#!/bin/bash\n\nbash scripts/1_load_data.sh\npython scripts/2_compute_pseudobulk.py\nRscript scripts/3_plot_results.R\nFrom R:\nsystem(\"bash scripts/1_load_data.sh\")\nsystem(\"python scripts/2_compute_pseudobulk.py\")\nsystem(\"Rscript scripts/3_plot_results.R\")\nFrom Python:\nimport subprocess\n\nsubprocess.run(\"bash scripts/1_load_data.sh\", shell=True)\nsubprocess.run(\"python scripts/2_compute_pseudobulk.py\", shell=True)\nsubprocess.run(\"Rscript scripts/3_plot_results.R\", shell=True)\n\n\n4.4.2 Calling scripts in different environments\nSometimes you might want to run scripts in different environments, as they it’s too much hassle to install all dependencies in one environment, you want to reuse existing ones or you want keep them separate and maintainable.\nYou can interleave your Bash script with environment activation functions e.g. conda activate {script_env} commands. This requires a conda .yaml file for each script environment in order to be reproducible. An important consideration is that packages that impact the on-disk data format should be the same version across environments.\nAlternatively, you can use a workflow manager like Nextflow or Snakemake to manage the environments and dependencies for you. A interesting, but very new approach is to use the Pixi package managment tool to manage the environments and tasks for you. The environments can be composed from multiple features containing dependencies, so you can have a scverse environment with only Python, a rverse environment with only R and even an all environment with both by adding the respective features (if such an environment is resolvable at least).\nRun scripts in different environments with pixi:\npixi run -e bash scripts/1_load_data.sh\npixi run -e scverse scripts/2_compute_pseudobulk.py\npixi run -e rverse scripts/3_plot_results.R\nWith the Pixi task runner, you can define these tasks in their respective environments, make them dependant on each other and run them in a single command.\npixi run pipeline\nYou can create a Docker image with all the pixi environments and run the pipeline in one containerized environment. The image is ~5GB and the pipeline can require a lot of working memory ~20GB, so make sure to increase the RAM allocated to Docker in your settings. Note that the usecase_data/ and scripts/ folders are mounted to the Docker container, so you can interactively edit the scripts and access the data.\ndocker pull berombau/polygloty-docker:latest\ndocker run -it -v $(pwd)/usecase_data:/app/usecase_data -v $(pwd)/scripts:/app/scripts berombau/polygloty-docker:latest pixi run pipeline",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>On-disk interoperability</span>"
    ]
  },
  {
    "objectID": "book/workflow_frameworks/index.html",
    "href": "book/workflow_frameworks/index.html",
    "title": "5  Workflows",
    "section": "",
    "text": "5.1 Productionization\nProductionization is the process of transforming research-oriented analysis pipelines into robust, scalable, and maintainable workflows that can be reliably executed in a production environment (Figure 5.1). This transition is essential for ensuring the reproducibility of results, facilitating collaboration among researchers, and enabling the efficient processing of large and complex single-cell datasets.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Workflows</span>"
    ]
  },
  {
    "objectID": "book/workflow_frameworks/index.html#productionization",
    "href": "book/workflow_frameworks/index.html#productionization",
    "title": "5  Workflows",
    "section": "",
    "text": "Figure 5.1: An example of the productionization process for single-cell analysis workflows. A) The research environment is characterized by scattered data, manual steps, and ad-hoc analysis pipelines. B) The production environment is streamlined, automated, and standardized, with reproducibility engines in place.\n\n\n\n\nBut how to ensure that your workflow is production-ready?\nIn this chapter, we will explore:\n\nKey qualities of workflows built to stand the test of time\nWhich technologies and workflow frameworks contribute to these qualities\nBest practices to keep in mind during development",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Workflows</span>"
    ]
  },
  {
    "objectID": "book/workflow_frameworks/index.html#review-of-workflow-frameworks",
    "href": "book/workflow_frameworks/index.html#review-of-workflow-frameworks",
    "title": "5  Workflows",
    "section": "5.2 Review of Workflow Frameworks",
    "text": "5.2 Review of Workflow Frameworks\nA lot of different workflow frameworks exist, and there are a lot of factors to consider when choosing the right one for your project. Wratten, Wilm, and Göke (2021) conducted a review of popular workflow managers for bioinformatics, evaluating them based on several key aspects, including ease of use, expressiveness, portability, scalability, and learning resources (Table 5.1).\n\n\n\nTable 5.1: Overview of workflow managers for bioinformatics (Wratten, Wilm, and Göke 2021).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTool\nClass\nEase of use\nExpressiveness\nPortability\nScalability\nLearning resources\nPipeline initiatives\n\n\n\n\nGalaxy\nGraphical\n●●●\n●○○\n●●●\n●●●\n●●●\n●●○\n\n\nKNIME\nGraphical\n●●●\n●○○\n○○○\n●●◐\n●●●\n●●○\n\n\nNextflow\nDSL\n●●○\n●●●\n●●●\n●●●\n●●●\n●●●\n\n\nSnakemake\nDSL\n●●○\n●●●\n●●◐\n●●●\n●●○\n●●●\n\n\nGenPipes\nDSL\n●●○\n●●●\n●●○\n●●○\n●●○\n●●○\n\n\nbPipe\nDSL\n●●○\n●●●\n●●○\n●●◐\n●●○\n●○○\n\n\nPachyderm\nDSL\n●●○\n●●●\n●○○\n●●○\n●●●\n○○○\n\n\nSciPipe\nLibrary\n●●○\n●●●\n○○○\n○○○\n●●○\n○○○\n\n\nLuigi\nLibrary\n●●○\n●●●\n●○○\n●●◐\n●●○\n○○○\n\n\nCromwell + WDL\nExecution + workflow specification\n●○○\n●●○\n●●●\n●●◐\n●●○\n●●○\n\n\ncwltool + CWL\nExecution + workflow specification\n●○○\n●●○\n●●◐\n○○○\n●●●\n●●○\n\n\nToil + CWL/WDL/Python\nExecution + workflow specification\n●○○\n●●●\n●◐○\n●●●\n●●○\n●●○\n\n\n\n\n\n\nEven more interesting is the accompanying GitHub repository (GoekeLab/bioinformatics-workflows), which contains a Proof of Concept (PoC) RNA-seq workflow implemented in the different workflow frameworks. These implementations were contributed and reviewed by the developers of the respective frameworks themselves!\n\n\n\nWow! ;)\n\n\nLooking at these implementations, at first glance, one would think that the differences between the frameworks are minimal, and that the choice of framework is mostly a matter of personal preference.\n\n5.2.1 Comparing PoC Workflows to Community-Made Modules\nHowever, comparing the POC workflows (left) to community-made modules (right), it becomes clear that creating production-ready components requires a lot more than specifying a command’s input and output files.\n\n5.2.1.1 Nextflow\n\n\nWratten et al. 2021 PoC (Source):\n\nmain.nf\n\n\nprocess FASTQC {\n  publishDir params.outdir\n\n  input:\n    path index\n    path left\n    path right\n  output:\n    path 'qc'\n\n  \"\"\"\n    mkdir qc && fastqc --quiet '${params.left}' '${params.right}' --outdir qc\n  \"\"\"\n}\n\n\n\n\n\nnf-core (Source):\n\nenvironment.ymlmain.nfmeta.yamltests/main.nf.test\n\n\nchannels:\n  - conda-forge\n  - bioconda\ndependencies:\n  - bioconda::fastqc=0.12.1\n\n\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda \"${moduleDir}/environment.yml\"\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.12.1--hdfd78af_0' :\n        'biocontainers/fastqc:0.12.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    // Make list of old name and new name pairs to use for renaming in the bash while loop\n    def old_new_pairs = reads instanceof Path || reads.size() == 1 ? [[ reads, \"${prefix}.${reads.extension}\" ]] : reads.withIndex().collect { entry, index -&gt; [ entry, \"${prefix}_${index + 1}.${entry.extension}\" ] }\n    def rename_to = old_new_pairs*.join(' ').join(' ')\n    def renamed_files = old_new_pairs.collect{ old_name, new_name -&gt; new_name }.join(' ')\n\n    // The total amount of allocated RAM by FastQC is equal to the number of threads defined (--threads) time the amount of RAM defined (--memory)\n    // https://github.com/s-andrews/FastQC/blob/1faeea0412093224d7f6a07f777fad60a5650795/fastqc#L211-L222\n    // Dividing the task.memory by task.cpu allows to stick to requested amount of RAM in the label\n    def memory_in_mb = MemoryUnit.of(\"${task.memory}\").toUnit('MB') / task.cpus\n    // FastQC memory value allowed range (100 - 10000)\n    def fastqc_memory = memory_in_mb &gt; 10000 ? 10000 : (memory_in_mb &lt; 100 ? 100 : memory_in_mb)\n\n    \"\"\"\n    printf \"%s %s\\\\n\" $rename_to | while read old_name new_name; do\n        [ -f \"\\${new_name}\" ] || ln -s \\$old_name \\$new_name\n    done\n\n    fastqc \\\\\n        $args \\\\\n        --threads $task.cpus \\\\\n        --memory $fastqc_memory \\\\\n        $renamed_files\n\n    cat &lt;&lt;-END_VERSIONS &gt; versions.yml\n    \"${task.process}\":\n        fastqc: \\$( fastqc --version | sed '/FastQC v/!d; s/.*v//' )\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.html\n    touch ${prefix}.zip\n\n    cat &lt;&lt;-END_VERSIONS &gt; versions.yml\n    \"${task.process}\":\n        fastqc: \\$( fastqc --version | sed '/FastQC v/!d; s/.*v//' )\n    END_VERSIONS\n    \"\"\"\n}\n\n\nname: fastqc\ndescription: Run FastQC on sequenced reads\nkeywords:\n  - quality control\n  - qc\n  - adapters\n  - fastq\ntools:\n  - fastqc:\n      description: |\n        FastQC gives general quality metrics about your reads.\n        It provides information about the quality score distribution\n        across your reads, the per base sequence content (%A/C/G/T).\n        You get information about adapter contamination and other\n        overrepresented sequences.\n      homepage: https://www.bioinformatics.babraham.ac.uk/projects/fastqc/\n      documentation: https://www.bioinformatics.babraham.ac.uk/projects/fastqc/Help/\n      licence: [\"GPL-2.0-only\"]\ninput:\n  - meta:\n      type: map\n      description: |\n        Groovy Map containing sample information\n        e.g. [ id:'test', single_end:false ]\n  - reads:\n      type: file\n      description: |\n        List of input FastQ files of size 1 and 2 for single-end and paired-end data,\n        respectively.\noutput:\n  - meta:\n      type: map\n      description: |\n        Groovy Map containing sample information\n        e.g. [ id:'test', single_end:false ]\n  - html:\n      type: file\n      description: FastQC report\n      pattern: \"*_{fastqc.html}\"\n  - zip:\n      type: file\n      description: FastQC report archive\n      pattern: \"*_{fastqc.zip}\"\n  - versions:\n      type: file\n      description: File containing software versions\n      pattern: \"versions.yml\"\nauthors:\n  - \"@drpatelh\"\n  - \"@grst\"\n  - \"@ewels\"\n  - \"@FelixKrueger\"\nmaintainers:\n  - \"@drpatelh\"\n  - \"@grst\"\n  - \"@ewels\"\n  - \"@FelixKrueger\"\n\n\nnextflow_process {\n\n    name \"Test Process FASTQC\"\n    script \"../main.nf\"\n    process \"FASTQC\"\n\n    tag \"modules\"\n    tag \"modules_nfcore\"\n    tag \"fastqc\"\n\n    test(\"sarscov2 single-end [fastq]\") {\n\n        when {\n            process {\n                \"\"\"\n                input[0] = Channel.of([\n                    [ id: 'test', single_end:true ],\n                    [ file(params.modules_testdata_base_path + 'genomics/sarscov2/illumina/fastq/test_1.fastq.gz', checkIfExists: true) ]\n                ])\n                \"\"\"\n            }\n        }\n\n        then {\n            assertAll (\n                { assert process.success },\n                // NOTE The report contains the date inside it, which means that the md5sum is stable per day, but not longer than that. So you can't md5sum it.\n                // looks like this: &lt;div id=\"header_filename\"&gt;Mon 2 Oct 2023&lt;br/&gt;test.gz&lt;/div&gt;\n                // https://github.com/nf-core/modules/pull/3903#issuecomment-1743620039\n                { assert process.out.html[0][1] ==~ \".*/test_fastqc.html\" },\n                { assert process.out.zip[0][1] ==~ \".*/test_fastqc.zip\" },\n                { assert path(process.out.html[0][1]).text.contains(\"&lt;tr&gt;&lt;td&gt;File type&lt;/td&gt;&lt;td&gt;Conventional base calls&lt;/td&gt;&lt;/tr&gt;\") },\n                { assert snapshot(process.out.versions).match() }\n            )\n        }\n    }\n\n    /* The rest of the tests are omitted */\n}\n\n\n\n\n\n\n\n5.2.1.2 Snakemake\n\n\nWratten et al. 2021 PoC (Source):\n\nfastqc.smk\n\n\nrule fastqc:\n    input:\n        get_fastqs,\n    output:\n        directory(\"results/fastqc/{sample}\"),\n    log:\n        \"logs/fastqc/{sample}.log\",\n    conda:\n        \"envs/fastqc.yaml\"\n    params:\n        \"--quiet --outdir\",\n    shell:\n        \"mkdir {output}; fastqc {input} {params} {output} 2&gt; {log}\"\n\n\n\n\n\nsnakemake-wrappers (Source):\n\nenvironment.yamlmeta.yamlwrapper.pytest/Snakefile\n\n\nchannels:\n  - conda-forge\n  - bioconda\n  - nodefaults\ndependencies:\n  - fastqc =0.12.1\n  - snakemake-wrapper-utils =0.6.2\n\n\nname: fastqc\ndescription: |\n  Generate fastq qc statistics using fastqc.\nurl: https://github.com/s-andrews/FastQC\nauthors:\n  - Julian de Ruiter\ninput:\n  - fastq file\noutput:\n  - html file containing statistics\n  - zip file containing statistics\n\n\n\"\"\"Snakemake wrapper for fastqc.\"\"\"\n\n__author__ = \"Julian de Ruiter\"\n__copyright__ = \"Copyright 2017, Julian de Ruiter\"\n__email__ = \"julianderuiter@gmail.com\"\n__license__ = \"MIT\"\n\n\nfrom os import path\nimport re\nfrom tempfile import TemporaryDirectory\nfrom snakemake.shell import shell\nfrom snakemake_wrapper_utils.snakemake import get_mem\n\nextra = snakemake.params.get(\"extra\", \"\")\nlog = snakemake.log_fmt_shell(stdout=True, stderr=True)\n# Define memory per thread (https://github.com/s-andrews/FastQC/blob/master/fastqc#L201-L222)\nmem_mb = int(get_mem(snakemake, \"MiB\") / snakemake.threads)\n\n\ndef basename_without_ext(file_path):\n    \"\"\"Returns basename of file path, without the file extension.\"\"\"\n\n    base = path.basename(file_path)\n    # Remove file extension(s) (similar to the internal fastqc approach)\n    base = re.sub(\"\\\\.gz$\", \"\", base)\n    base = re.sub(\"\\\\.bz2$\", \"\", base)\n    base = re.sub(\"\\\\.txt$\", \"\", base)\n    base = re.sub(\"\\\\.fastq$\", \"\", base)\n    base = re.sub(\"\\\\.fq$\", \"\", base)\n    base = re.sub(\"\\\\.sam$\", \"\", base)\n    base = re.sub(\"\\\\.bam$\", \"\", base)\n\n    return base\n\n\n# If you have multiple input files fastqc doesn't know what to do. Taking silently only first gives unapreciated results\n\nif len(snakemake.input) &gt; 1:\n    raise IOError(\"Got multiple input files, I don't know how to process them!\")\n\n# Run fastqc, since there can be race conditions if multiple jobs\n# use the same fastqc dir, we create a temp dir.\nwith TemporaryDirectory() as tempdir:\n    shell(\n        \"fastqc\"\n        \" --threads {snakemake.threads}\"\n        \" --memory {mem_mb}\"\n        \" {extra}\"\n        \" --outdir {tempdir:q}\"\n        \" {snakemake.input[0]:q}\"\n        \" {log}\"\n    )\n\n    # Move outputs into proper position.\n    output_base = basename_without_ext(snakemake.input[0])\n    html_path = path.join(tempdir, output_base + \"_fastqc.html\")\n    zip_path = path.join(tempdir, output_base + \"_fastqc.zip\")\n\n    if snakemake.output.html != html_path:\n        shell(\"mv {html_path:q} {snakemake.output.html:q}\")\n\n    if snakemake.output.zip != zip_path:\n        shell(\"mv {zip_path:q} {snakemake.output.zip:q}\")\n\n\nrule fastqc:\n    input:\n        \"reads/{sample}.fastq\"\n    output:\n        html=\"qc/fastqc/{sample}.html\",\n        zip=\"qc/fastqc/{sample}_fastqc.zip\" # the suffix _fastqc.zip is necessary for multiqc to find the file. If not using multiqc, you are free to choose an arbitrary filename\n    params:\n        extra = \"--quiet\"\n    log:\n        \"logs/fastqc/{sample}.log\"\n    threads: 1\n    resources:\n        mem_mb = 1024\n    wrapper:\n        \"master/bio/fastqc\"\n\n\n\n\n\n\n\n5.2.1.3 Toil + WDL\n\n\nWratten et al. 2021 PoC (Source):\n\nfastqc.wdl\n\n\ntask FastQCone {\n  input {\n     File reads\n  }\n\n  command {\n     zcat \"${reads}\" | fastqc stdin:readsone\n  }\n\n  output {\n     File fastqc_res = \"readsone_fastqc.html\"\n  }\n  \n  runtime {\n     docker: 'pegi3s/fastqc'\n  }\n}\n\n\n\n\n\nBioWDL (Source):\n\nfastqc.wdl (Excerpt)fastqc.wdl (Full)\n\n\nversion 1.0\n\n# ... license ...\n\ntask Fastqc {\n    input {\n        File seqFile\n        String outdirPath\n        Boolean casava = false\n        ## ... other arguments ...\n\n        # Set javaXmx a little high.\n        String javaXmx=\"1750M\"\n        Int threads = 1\n        String memory = \"2GiB\"\n        Int timeMinutes = 1 + ceil(size(seqFile, \"G\")) * 4\n        String dockerImage = \"quay.io/biocontainers/fastqc:0.12.1--hdfd78af_0\"\n\n        Array[File]? noneArray\n        File? noneFile\n    }\n\n    # Chops of the .gz extension if present.\n    String name = basename(sub(seqFile, \"\\.gz$\",\"\"))\n    # This regex chops of the extension just as fastqc does it.\n    String reportDir = outdirPath + \"/\" + sub(name, \"\\.[^\\.]*$\", \"_fastqc\")\n\n    # We reimplement the perl wrapper here. This has the advantage that it\n    # gives us more control over the amount of memory used.\n    command &lt;&lt;&lt;\n        set -e\n        mkdir -p \"~{outdirPath}\"\n        FASTQC_DIR=\"/usr/local/opt/fastqc-0.12.1\"\n        export CLASSPATH=\"$FASTQC_DIR:$FASTQC_DIR/sam-1.103.jar:$FASTQC_DIR/jbzip2-0.9.jar:$FASTQC_DIR/cisd-jhdf5.jar\"\n        java -Djava.awt.headless=true -XX:ParallelGCThreads=1 \\\n        -Xms200M -Xmx~{javaXmx} \\\n        ~{\"-Dfastqc.output_dir=\" + outdirPath} \\\n        ~{true=\"-Dfastqc.casava=true\" false=\"\" casava} \\\n        # ... other arguments ...\n        ~{\"-Dfastqc.kmer_size=\" + kmers} \\\n        ~{\"-Djava.io.tmpdir=\" + dir} \\\n        uk.ac.babraham.FastQC.FastQCApplication \\\n        ~{seqFile}\n    &gt;&gt;&gt;\n\n    output {\n        File htmlReport = reportDir + \".html\"\n        File reportZip = reportDir + \".zip\"\n        File? summary = if extract then reportDir + \"/summary.txt\" else noneFile\n        File? rawReport = if extract then reportDir + \"/fastqc_data.txt\" else noneFile\n        Array[File]? images = if extract then glob(reportDir + \"/Images/*.png\") else noneArray\n    }\n\n    runtime {\n        cpu: threads\n        memory: memory\n        time_minutes: timeMinutes\n        docker: dockerImage\n    }\n\n    parameter_meta {\n        # inputs\n        seqFile: {description: \"A fastq file.\", category: \"required\"}\n        outdirPath: {description: \"The path to write the output to.\", catgory: \"required\"}\n        # ... other arguments ...\n        dockerImage: {description: \"The docker image used for this task. Changing this may result in errors which the developers may choose not to address.\", category: \"advanced\"}\n\n        # outputs\n        htmlReport: {description: \"HTML report file.\"}\n        reportZip: {description: \"Source data file.\"}\n        summary: {description: \"Summary file.\"}\n        rawReport: {description: \"Raw report file.\"}\n        images: {description: \"Images in report file.\"}\n    }\n\n    meta {\n        WDL_AID: {\n            exclude: [\"noneFile\", \"noneArray\"]\n        }\n    }\n}\n\n\nversion 1.0\n\n# Copyright (c) 2017 Leiden University Medical Center\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\ntask Fastqc {\n    input {\n        File seqFile\n        String outdirPath\n        Boolean casava = false\n        Boolean nano = false\n        Boolean noFilter = false\n        Boolean extract = false\n        Boolean nogroup = false\n\n        Int? minLength\n        String? format\n        File? contaminants\n        File? adapters\n        File? limits\n        Int? kmers\n        String? dir\n\n        # Set javaXmx a little high. Equal to fastqc default with 7 threads.\n        # This is because some fastq files need more memory. 2G per core\n        # is a nice cluster default, so we use all the rest of the memory for\n        # fastqc so we should have as little OOM crashes as possible even with\n        # weird edge case fastq's.\n        String javaXmx=\"1750M\"\n        Int threads = 1\n        String memory = \"2GiB\"\n        Int timeMinutes = 1 + ceil(size(seqFile, \"G\")) * 4\n        String dockerImage = \"quay.io/biocontainers/fastqc:0.12.1--hdfd78af_0\"\n\n        Array[File]? noneArray\n        File? noneFile\n    }\n\n    # Chops of the .gz extension if present.\n    # The Basename needs to be taken here. Otherwise paths might differ\n    # between similar jobs.\n    String name = basename(sub(seqFile, \"\\.gz$\",\"\"))\n    # This regex chops of the extension and replaces it with _fastqc for\n    # the reportdir.\n    # Just as fastqc does it.\n    String reportDir = outdirPath + \"/\" + sub(name, \"\\.[^\\.]*$\", \"_fastqc\")\n\n    # We reimplement the perl wrapper here. This has the advantage that it\n    # gives us more control over the amount of memory used.\n    command &lt;&lt;&lt;\n        set -e\n        mkdir -p \"~{outdirPath}\"\n        FASTQC_DIR=\"/usr/local/opt/fastqc-0.12.1\"\n        export CLASSPATH=\"$FASTQC_DIR:$FASTQC_DIR/sam-1.103.jar:$FASTQC_DIR/jbzip2-0.9.jar:$FASTQC_DIR/cisd-jhdf5.jar\"\n        java -Djava.awt.headless=true -XX:ParallelGCThreads=1 \\\n        -Xms200M -Xmx~{javaXmx} \\\n        ~{\"-Dfastqc.output_dir=\" + outdirPath} \\\n        ~{true=\"-Dfastqc.casava=true\" false=\"\" casava} \\\n        ~{true=\"-Dfastqc.nano=true\" false=\"\" nano} \\\n        ~{true=\"-Dfastqc.nofilter=true\" false=\"\" noFilter} \\\n        ~{true=\"-Dfastqc.unzip=true\" false=\"\" extract} \\\n        ~{true=\"-Dfastqc.nogroup=true\" false=\"\" nogroup} \\\n        ~{\"-Dfastqc.min_length=\" + minLength} \\\n        ~{\"-Dfastqc.sequence_format=\" + format} \\\n        ~{\"-Dfastqc.threads=\" + threads} \\\n        ~{\"-Dfastqc.contaminant_file=\" + contaminants} \\\n        ~{\"-Dfastqc.adapter_file=\" + adapters} \\\n        ~{\"-Dfastqc.limits_file=\" + limits} \\\n        ~{\"-Dfastqc.kmer_size=\" + kmers} \\\n        ~{\"-Djava.io.tmpdir=\" + dir} \\\n        uk.ac.babraham.FastQC.FastQCApplication \\\n        ~{seqFile}\n    &gt;&gt;&gt;\n\n    output {\n        File htmlReport = reportDir + \".html\"\n        File reportZip = reportDir + \".zip\"\n        File? summary = if extract then reportDir + \"/summary.txt\" else noneFile\n        File? rawReport = if extract then reportDir + \"/fastqc_data.txt\" else noneFile\n        Array[File]? images = if extract then glob(reportDir + \"/Images/*.png\") else noneArray\n    }\n\n    runtime {\n        cpu: threads\n        memory: memory\n        time_minutes: timeMinutes\n        docker: dockerImage\n    }\n\n    parameter_meta {\n        # inputs\n        seqFile: {description: \"A fastq file.\", category: \"required\"}\n        outdirPath: {description: \"The path to write the output to.\", catgory: \"required\"}\n        casava: {description: \"Equivalent to fastqc's --casava flag.\", category: \"advanced\"}\n        nano: {description: \"Equivalent to fastqc's --nano flag.\", category: \"advanced\"}\n        noFilter: {description: \"Equivalent to fastqc's --nofilter flag.\", category: \"advanced\"}\n        extract: {description: \"Equivalent to fastqc's --extract flag.\", category: \"advanced\"}\n        nogroup: {description: \"Equivalent to fastqc's --nogroup flag.\", category: \"advanced\"}\n        minLength: {description: \"Equivalent to fastqc's --min_length option.\", category: \"advanced\"}\n        format: {description: \"Equivalent to fastqc's --format option.\", category: \"advanced\"}\n        contaminants: {description: \"Equivalent to fastqc's --contaminants option.\", category: \"advanced\"}\n        adapters: {description: \"Equivalent to fastqc's --adapters option.\", category: \"advanced\"}\n        limits: {description: \"Equivalent to fastqc's --limits option.\", category: \"advanced\"}\n        kmers: {description: \"Equivalent to fastqc's --kmers option.\", category: \"advanced\"}\n        dir: {description: \"Equivalent to fastqc's --dir option.\", category: \"advanced\"}\n        javaXmx: {description: \"The maximum memory available to the program. Should be lower than `memory` to accommodate JVM overhead.\", category: \"advanced\"}\n        threads: {description: \"The number of cores to use.\", category: \"advanced\"}\n        memory: {description: \"The amount of memory this job will use.\", category: \"advanced\"}\n        timeMinutes: {description: \"The maximum amount of time the job will run in minutes.\", category: \"advanced\"}\n        dockerImage: {description: \"The docker image used for this task. Changing this may result in errors which the developers may choose not to address.\", category: \"advanced\"}\n\n        # outputs\n        htmlReport: {description: \"HTML report file.\"}\n        reportZip: {description: \"Source data file.\"}\n        summary: {description: \"Summary file.\"}\n        rawReport: {description: \"Raw report file.\"}\n        images: {description: \"Images in report file.\"}\n    }\n\n    meta {\n        WDL_AID: {\n            exclude: [\"noneFile\", \"noneArray\"]\n        }\n    }\n}\n\n\n\n\n\n\n\n\n5.2.2 Limitations of the study\nHowever, the Supplementary Table shows that the comparison in Table 5.1 was rather limited, since the score of each category was only based on a single criterion. Of the following categories, only “Scalability” was determined by more than one criterion:\n\nEase of Use: Graphical interface with execution environment (score of 3), programming interface with in-built execution environment (score of 2), separated development and execution environment (score of 1).\nExpressiveness: Based on an existing programming language (3) or a new language or restricted vocabulary (2), primary interaction with graphical user interface (1).\nPortability: Integration with three or more container and package manager platforms (3), two platforms are supported (2), one platform is supported (1).\nScalability: Considers cloud support, scheduler and orchestration tool integration, and executor support. Please refer to Supplementary Table 1 - Sheet 2 (Scalability).\nLearning resources: Official tutorials, forums and events (3), tutorials and forums (2), tutorials or forums (1).\nPipelines Initiatives: Community and curated (3), community or curated (2), not community or curated (1).\n\nBy comparing the example code of the respective workflow frameworks, it also becomes clear that we need not only look at example code of POC workflows, but actual production-ready workflows and pipelines. Such code often require a lot more functionality, including:\n\nError handling\nLogging\nData provenance\nParameterization\nTesting\nDocumentation\nContainerization\nResource management",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Workflows</span>"
    ]
  },
  {
    "objectID": "book/workflow_frameworks/index.html#qualities-of-a-production-ready-workflow",
    "href": "book/workflow_frameworks/index.html#qualities-of-a-production-ready-workflow",
    "title": "5  Workflows",
    "section": "5.3 Qualities of a Production-Ready Workflow",
    "text": "5.3 Qualities of a Production-Ready Workflow\nBuilding production-ready workflows for single-cell analysis involves integrating a variety of tools, technologies, and best practices. In order to meet the demands of large-scale data processing, reproducibility, and collaboration, a production-ready workflow should exhibit the following essential qualities (Figure 5.2):\n\n\n\n\n\n\nFigure 5.2: Essential qualities of a production-ready workflow.\n\n\n\n\nPolyglot: Seamlessly integrate tools and libraries from different programming languages, allowing you to leverage the strengths of each language for specific tasks. This facilitates the use of specialized tools and optimizes the analysis pipeline for performance and efficiency.\nModular: A well-structured workflow should be composed of modular and reusable components, promoting code maintainability and facilitating collaboration. Each module should have a clear purpose and well-defined inputs and outputs, enabling easy integration and replacement of individual steps within the pipeline.\nScalable: Single-cell datasets can be massive, and a production-ready workflow should be able to handle large volumes of data efficiently. This involves utilizing scalable compute environments, optimizing data storage and retrieval, and implementing parallelization strategies to accelerate analysis.\nReproducible: Ensuring reproducibility is crucial for scientific rigor and validation. A production-ready workflow should capture all the necessary information, including code, data, parameters, and software environments, to enable others to replicate the analysis and obtain consistent results.\nPortable: The workflow should be designed to run seamlessly across different computing platforms and environments, promoting accessibility and collaboration. Containerization technologies like Docker can help achieve portability by encapsulating the workflow and its dependencies into a self-contained unit.\nCommunity: Leveraging community resources, tools, and best practices can accelerate the development of production-ready workflows. This is because developing high-quality components can at times be time-consuming, and sharing resources can help reduce duplication of effort and promote collaboration.\nMaintainable: A production-ready workflow should be well-documented, organized, and easy to understand, facilitating updates, modifications, and troubleshooting. Clear documentation of code, data, and parameters ensures that the workflow remains accessible and usable over time.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Workflows</span>"
    ]
  },
  {
    "objectID": "book/workflow_frameworks/index.html#technologies-for-production-ready-workflows",
    "href": "book/workflow_frameworks/index.html#technologies-for-production-ready-workflows",
    "title": "5  Workflows",
    "section": "5.4 Technologies for Production-Ready Workflows",
    "text": "5.4 Technologies for Production-Ready Workflows\nThe essential qualities of a production-ready workflow are achieved through a combination of enabling technologies (Figure 5.3). These technologies provide the foundation for building scalable, reproducible, and maintainable workflows for single-cell analysis.\n\n\n\n\n\n\nFigure 5.3: The essential qualities of a production-ready workflow are achieved through a combination of enabling technologies.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Workflows</span>"
    ]
  },
  {
    "objectID": "book/workflow_frameworks/index.html#quality-assessment-of-workflow-frameworks",
    "href": "book/workflow_frameworks/index.html#quality-assessment-of-workflow-frameworks",
    "title": "5  Workflows",
    "section": "5.5 Quality Assessment of Workflow Frameworks",
    "text": "5.5 Quality Assessment of Workflow Frameworks\nGiven the abovementioned limitations, we decided to conduct our own quality assessment of workflow frameworks. This assessment is still largely in the works, but we’re happy to share the preliminary results with you.\nThe data is based on a review of the documentation and community resources for each framework. We evaluated the frameworks based on the list of essential qualities mentioned in the previous section (Figure 5.2).\n\n5.5.1 Included frameworks\nThe following workflow frameworks were included in the assessment:\n\nNextflow: A domain-specific language for creating scalable and reproducible data analysis pipelines.    \nSnakemake: A workflow management system that uses Python-based rules to define dependencies and execute tasks.    \nGalaxy: A web-based platform for creating, running, and sharing data analysis workflows without the need for programming expertise.    \nViash + Nextflow: A combination of Viash, a tool for defining bioinformatics workflow components, and Nextflow for scalable and reproducible execution.    \nArgo Workflows: A Kubernetes-native workflow engine for orchestrating containerized tasks and automating complex workflows.  \n\n\n\n5.5.2 Quality Assessment Criteria\nThe quality assessment was based on the following criteria:\n\nCommunityMaintainabilityModularityPolyglotPortabilityReproducibilityScalability\n\n\nDoes a library of components exist?\n\nA library of OSS components is available\nCommunity repository has frequent commits\nCommunity repository has &gt; 10 frequent non-employee commitors\nAre there repositories of OSS workflows available?\n\n\n\nHow easy is it to update, modify, and troubleshoot workflows?\n\nWorkflows are well-structured and easy to understand.\nFramework supports documenting the interfaces of components and workflows\nVersion control and collaboration features facilitate team-based development.\nComponents can be unit tested\nDocumentation specifies how to unit test components\n\n\n\nHow effectively does the framework promote modular design and reusability?\n\nSoftware can be easily encapsulated as a modular component.\nComponents have well-defined inputs and outputs.\nComponents can be shared and reused across different workflows.\nThe framework supports dependency management\nThe framework supports versioning of components.\n\n\n\nIs it easy to switch between different programming / scripting language within one workflow?\n\nCalling a script from another language is possible\nComponents can be written in multiple languages and communicate via a file-based interface\nComponents can be written in multiple languages and communicate via an in-memory data channel\n\n\n\nSupport for various compute platforms\n\nSupport for AWS Batch\nSupport for Azure Batch\nSupport for Google Cloud\nSupport for Kubernetes\nSupport for Local execution\nSupport for PBS/Torque\nSupport for SLURM\nSupport for additional compute platforms\n\nSupport for various containerization technologies\n\nSupport for Apptainer\nSupport for Docker\nSupport for Podman\nSupport for additional containerization technologies\n\nSupport for various storage solutions\n\nAWS S3\nAzure Blob Storage / Files\nGoogle Storage\nHTTPS\nFTP\nSupport for additional storage solutions\n\n\n\nHow effectively does the framework ensure reproducibility of results?\n\nIndividual components can list their software dependencies\nPer-component containerisation is supported\nExtending images with additional dependencies is supported\nData provenance tracking is built-in or can be easily integrated.\nFramework promotes versioned releases of the workflow software and images to ensure reproducibility\n\n\n\nHow well does the framework handle large and complex workflows?\n\nThe framework supports asynchronous and distributed execution.\nResource management and optimization features are available.\nPerformance monitoring and profiling tools are provided.\n\n\n\n\nThese criteria and subsequent scores will be further refined and validated as part of our ongoing research.\n\n\n5.5.3 Quality Scores\nThe aggregated quality scores for each framework are shown below. The scores are based on the evaluation of the essential qualities of a production-ready workflow.\n\n\n\n\n\nQuality scores for different workflow frameworks.\n\n\n\n\nRaw scores and detailed explanations behind the reasoning of the resulting scores can be found in the Workflow Quality Assessment Spreadsheet.\n\n\n5.5.4 Quality assessment contributors\n\nJakub Majerčík\nMichaela Müller\nRobrecht Cannoodt\nToni Verbeiren",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Workflows</span>"
    ]
  },
  {
    "objectID": "book/workflow_frameworks/index.html#conclusion",
    "href": "book/workflow_frameworks/index.html#conclusion",
    "title": "5  Workflows",
    "section": "5.6 Conclusion",
    "text": "5.6 Conclusion\n\n\n\n\nWratten, Laura, Andreas Wilm, and Jonathan Göke. 2021. “Reproducible, Scalable, and Shareable Analysis Pipelines with Bioinformatics Workflow Managers.” Nature Methods 18 (10): 1161–68. https://doi.org/10.1038/s41592-021-01254-9.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Workflows</span>"
    ]
  },
  {
    "objectID": "book/book_slides.html",
    "href": "book/book_slides.html",
    "title": "Slides",
    "section": "",
    "text": "Here are the slides used during the workshop:\n    View slides in full screen\n       \n      \n    \n  \n  Download PDF File\n   \n    Unable to display PDF file. Download instead.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "book/references.html",
    "href": "book/references.html",
    "title": "References",
    "section": "",
    "text": "Heumos, Lukas, Anna C. Schaar, Christopher Lance, Anastasia\nLitinetskaya, Felix Drost, Luke Zappia, Malte D. Lücken, et al. 2023.\n“Best Practices for Single-Cell Analysis Across\nModalities.” Nature Reviews Genetics 24 (8): 550–72. https://doi.org/10.1038/s41576-023-00586-w.\n\n\n“OP3 H5AD on S3.” 2024. https://openproblems-bio.s3.amazonaws.com/public/neurips-2023-competition/sc_counts_reannotated_with_counts.h5ad.\n\n\n“Open Problems Kaggle Competition - Single Cell\nPerturbations.” 2023. https://www.kaggle.com/competitions/open-problems-single-cell-perturbations/overview.\n\n\n“OpenProblems Perturbation Prediction Benchmark.” 2024. https://openproblems.bio/results/perturbation_prediction/.\n\n\n“SRA SRP527159.” 2024. https://trace.ncbi.nlm.nih.gov/Traces/?view=study&acc=SRP527159.\n\n\nWratten, Laura, Andreas Wilm, and Jonathan Göke. 2021.\n“Reproducible, Scalable, and Shareable Analysis Pipelines with\nBioinformatics Workflow Managers.” Nature Methods 18\n(10): 1161–68. https://doi.org/10.1038/s41592-021-01254-9.\n\n\nZappia, Luke, and Fabian J. Theis. 2021. “Over 1000 Tools Reveal\nTrends in the Single-Cell RNA-Seq Analysis Landscape.” Genome\nBiology 22 (1). https://doi.org/10.1186/s13059-021-02519-4.",
    "crumbs": [
      "References"
    ]
  }
]