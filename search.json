[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Polyglot programming for single-cell analysis",
    "section": "",
    "text": "Preface\nThis book is a collection of notebooks and explanations for the workshop on Polyglot programming for single-cell analysis given at the scverse Conference 2024. For more information, please visit the workshop page.\nIn order to use the best performing methods for each step of the single-cell analysis process, bioinformaticians need to use multiple ecosystems and programming languages. This is unfortunately not that straightforward. This workshop gives an overview of the different levels of interoperability, and how it is possible to integrate them in a single workflow.\nTo get started, read the Introduction chapter.\nTo learn more about Quarto books visit https://quarto.org/docs/books.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "book/intro.html",
    "href": "book/intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "Any bioinformatician that has analysed a single-cell dataset knows that using methods developed for different ecosystems or programming languages is necessary but painful. Any package developer has asked themselves the question on how to best provide access to their tool or method.\nWe will give an overview of the interoperability tools you can use when analysing a single-cell dataset: do you want to convert your data to a different data format, or is just calling one R function in your Jupyter notebook sufficient? Do you want fine-grained control over each step in the analysis pipeline or do you run a series of scripts that you really should convert to a workflow system?\nWe will give information on different options for package developers to provide better interoperability. Should you reimplement your package in a new language? How do you ensure that the results are the same?\nIn order to follow this workshop, we expect the participants to have some Python or R programming knowledge.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "notebooks/usecase.html",
    "href": "notebooks/usecase.html",
    "title": "2  Use-case",
    "section": "",
    "text": "2.1 1. Retrieving the data\nThe dataset has since been uploaded to SRA (“SRA SRP527159” 2024), will be uploaded to GEO, and is currently available from S3 (“OP3 H5AD on S3” 2024).\nIf you haven’t already, you can download the dataset from S3 using the following command:\nif [[ ! -f usecase_data/sc_counts_reannotated_with_counts.h5ad ]]; then\n  aws s3 cp \\\n    --no-sign-request \\\n    s3://openproblems-bio/public/neurips-2023-competition/sc_counts_reannotated_with_counts.h5ad \\\n    usecase_data/sc_counts_reannotated_with_counts.h5ad\nfi",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Use-case</span>"
    ]
  },
  {
    "objectID": "notebooks/usecase.html#loading-the-data",
    "href": "notebooks/usecase.html#loading-the-data",
    "title": "2  Use-case",
    "section": "2.2 2. Loading the data",
    "text": "2.2 2. Loading the data\nThe dataset is stored in an AnnData object, which can be loaded in Python as follows:\n\nimport anndata as ad\n\nadata = ad.read_h5ad(\"usecase_data/sc_counts_reannotated_with_counts.h5ad\")\n\nadata\n\nAnnData object with n_obs × n_vars = 298087 × 21265\n    obs: 'dose_uM', 'timepoint_hr', 'well', 'row', 'col', 'plate_name', 'cell_id', 'cell_type', 'split', 'donor_id', 'sm_name', 'control', 'SMILES', 'sm_lincs_id', 'library_id', 'leiden_res1', 'group', 'cell_type_orig', 'plate_well_celltype_reannotated', 'cell_count_by_well_celltype', 'cell_count_by_plate_well'\n    var: 'highly_variable', 'means', 'dispersions', 'dispersions_norm'\n    uns: 'cell_type_colors', 'celltypist_celltype_colors', 'donor_id_colors', 'hvg', 'leiden_res1_colors', 'log1p', 'neighbors', 'over_clustering', 'rank_genes_groups'\n    obsm: 'HTO_clr', 'X_pca', 'X_umap', 'protein_counts'\n    obsp: 'connectivities', 'distances'\n\n\nThe same code can be run in R using the anndata package (not run):\nlibrary(anndata)\n\nadata &lt;- read_h5ad(\"usecase_data/sc_counts_reannotated_with_counts.h5ad\")\n\nadata",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Use-case</span>"
    ]
  },
  {
    "objectID": "notebooks/usecase.html#subset-data",
    "href": "notebooks/usecase.html#subset-data",
    "title": "2  Use-case",
    "section": "2.3 3. Subset data",
    "text": "2.3 3. Subset data\nSince the dataset is large, we will subset the data to a single small molecule, control, and cell type.\n\nsm_name = \"Belinostat\"\ncontrol_name = \"Dimethyl Sulfoxide\"\ncell_type = \"T cells\"\n\nadata = adata[\n  adata.obs[\"sm_name\"].isin([sm_name, control_name]) &\n  adata.obs[\"cell_type\"].isin([cell_type]),\n].copy()\n\nWe will also subset the genes to the top 2000 most variable genes.\n\nadata = adata[:, adata.var[\"highly_variable\"]].copy()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Use-case</span>"
    ]
  },
  {
    "objectID": "notebooks/usecase.html#compute-pseudobulk",
    "href": "notebooks/usecase.html#compute-pseudobulk",
    "title": "2  Use-case",
    "section": "2.4 4. Compute pseudobulk",
    "text": "2.4 4. Compute pseudobulk\n\nimport pandas as pd\n\nCombine data in a single data frame and compute pseudobulk\n\ncombined = pd.DataFrame(\n  adata.X.toarray(),\n  index=adata.obs[\"plate_well_celltype_reannotated\"],\n)\ncombined.columns = adata.var_names\npb_X = combined.groupby(level=0).sum()\n\n&lt;string&gt;:1: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\nConstruct obs for pseudobulk. Use ‘plate_well_celltype_reannotated’ as index and make sure to retain the columns ‘sm_name’, ‘cell_type’, and ‘plate_name’:\n\npb_obs = adata.obs[[\"sm_name\", \"cell_type\", \"plate_name\", \"well\"]].copy()\npb_obs.index = adata.obs[\"plate_well_celltype_reannotated\"]\npb_obs = pb_obs.drop_duplicates()\n\nCreate AnnData object:\n\npb_adata = ad.AnnData(\n  X=pb_X.loc[pb_obs.index].values,\n  obs=pb_obs,\n  var=adata.var,\n)\n\nStore to disk:\n\npb_adata.write_h5ad(\"usecase_data/pseudobulk.h5ad\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Use-case</span>"
    ]
  },
  {
    "objectID": "notebooks/usecase.html#compute-de",
    "href": "notebooks/usecase.html#compute-de",
    "title": "2  Use-case",
    "section": "2.5 5. Compute DE",
    "text": "2.5 5. Compute DE\n\nlibrary(anndata)\nlibrary(dplyr, warn.conflicts = FALSE)\n\npb_adata &lt;- read_h5ad(\"usecase_data/pseudobulk.h5ad\")\n\nSelect small molecule and control:\n\nsm_name &lt;- \"Belinostat\"\ncontrol_name &lt;- \"Dimethyl Sulfoxide\"\n\nCreate DESeq dataset:\n\n# transform counts matrix\ncount_data &lt;- t(pb_adata$X)\nstorage.mode(count_data) &lt;- \"integer\"\n\n# create dataset\ndds &lt;- DESeq2::DESeqDataSetFromMatrix(\n  countData = count_data,\n  colData = pb_adata$obs,\n  design = ~ sm_name + plate_name,\n)\n\nWarning: replacing previous import 'S4Arrays::makeNindexFromArrayViewport' by\n'DelayedArray::makeNindexFromArrayViewport' when loading 'SummarizedExperiment'\n\n\n  Note: levels of factors in the design contain characters other than\n  letters, numbers, '_' and '.'. It is recommended (but not required) to use\n  only letters, numbers, and delimiters '_' or '.', as these are safe characters\n  for column names in R. [This is a message, not a warning or an error]\n\n\nRun DESeq2:\n\ndds &lt;- DESeq2::DESeq(dds)\n\nestimating size factors\n\n\n  Note: levels of factors in the design contain characters other than\n  letters, numbers, '_' and '.'. It is recommended (but not required) to use\n  only letters, numbers, and delimiters '_' or '.', as these are safe characters\n  for column names in R. [This is a message, not a warning or an error]\n\n\nestimating dispersions\n\n\ngene-wise dispersion estimates\n\n\nmean-dispersion relationship\n\n\n  Note: levels of factors in the design contain characters other than\n  letters, numbers, '_' and '.'. It is recommended (but not required) to use\n  only letters, numbers, and delimiters '_' or '.', as these are safe characters\n  for column names in R. [This is a message, not a warning or an error]\n\n\nfinal dispersion estimates\n\n\nfitting model and testing\n\n\n  Note: levels of factors in the design contain characters other than\n  letters, numbers, '_' and '.'. It is recommended (but not required) to use\n  only letters, numbers, and delimiters '_' or '.', as these are safe characters\n  for column names in R. [This is a message, not a warning or an error]\n\n\nGet results:\n\nres &lt;- DESeq2::results(dds, contrast=c(\"sm_name\", sm_name, control_name)) |&gt;\n  as.data.frame()\n\nPreview results:\n\nres |&gt;\n  arrange(padj) |&gt;\n  head(10)\n\n          baseMean log2FoldChange      lfcSE      stat        pvalue\nBEX5      59.24944       2.187350 0.05660399  38.64304  0.000000e+00\nHIST1H1D 301.38741       1.356543 0.03092962  43.85901  0.000000e+00\nSTMN1    234.72112       2.224633 0.04104002  54.20642  0.000000e+00\nPCSK1N    64.91604       1.899149 0.05480612  34.65214 4.147855e-263\nGZMM     141.39238      -1.309959 0.03806665 -34.41224 1.654371e-259\nMARCKSL1  95.82726       1.423057 0.04311798  33.00380 7.163953e-239\nH1FX     376.28247       1.054890 0.03221858  32.74168 3.988563e-235\nHIST1H1B  30.81805       4.317984 0.14074738  30.67896 1.086254e-206\nFXYD7     61.11526       2.331406 0.07725771  30.17700 4.746707e-200\nING2      79.68893       1.218777 0.04336609  28.10437 8.663682e-174\n                  padj\nBEX5      0.000000e+00\nHIST1H1D  0.000000e+00\nSTMN1     0.000000e+00\nPCSK1N   1.631144e-260\nGZMM     5.204651e-257\nMARCKSL1 1.878150e-236\nH1FX     8.962871e-233\nHIST1H1B 2.135848e-204\nFXYD7    8.296189e-198\nING2     1.362797e-171\n\n\nWrite to disk:\n\nwrite.csv(res, \"usecase_data/de_contrasts.csv\")\n\n\n\n\n\n“OP3 H5AD on S3.” 2024. https://openproblems-bio.s3.amazonaws.com/public/neurips-2023-competition/sc_counts_reannotated_with_counts.h5ad.\n\n\n“Open Problems Kaggle Competition - Single Cell Perturbations.” 2023. https://www.kaggle.com/competitions/open-problems-single-cell-perturbations/overview.\n\n\n“OpenProblems Perturbation Prediction Benchmark.” 2024. https://openproblems.bio/results/perturbation_prediction/.\n\n\n“SRA SRP527159.” 2024. https://trace.ncbi.nlm.nih.gov/Traces/?view=study&acc=SRP527159.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Use-case</span>"
    ]
  },
  {
    "objectID": "notebooks/file_formats.html",
    "href": "notebooks/file_formats.html",
    "title": "3  File formats",
    "section": "",
    "text": "4 File formats\nData format based interoperability",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>File formats</span>"
    ]
  },
  {
    "objectID": "notebooks/file_formats.html#setup",
    "href": "notebooks/file_formats.html#setup",
    "title": "3  File formats",
    "section": "4.1 Setup",
    "text": "4.1 Setup\n\nimport anndata\nimport numpy\nimport scanpy\n\n\nanndata.__version__\n\n'0.10.9'",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>File formats</span>"
    ]
  },
  {
    "objectID": "notebooks/in_memory.html",
    "href": "notebooks/in_memory.html",
    "title": "4  In memory interoperability (from Python)",
    "section": "",
    "text": "In this notebook, we will showcase how to call R code from Python. We will make use of rpy2 and anndata2ri.\nMake sure you have downloaded the data.\nRead in the anndata object\n\nimport anndata as ad\n\nadata_path = \"usecase_data/sc_counts_reannotated_with_counts.h5ad\"\nadata = ad.read_h5ad(adata_path)\n\nWe can use rpy2 to run R code within a Python process. If you wish to convert numpy matrices, you need to use the right convertor.\nThis is an example of how you import rpy2, and convert a matrix for use in R functions.\n\ncounts = adata.X # matrices are columnn major in R, and row-major in Python\ncounts = counts[:100, :1000] # subset for speed of example\ncounts_dense = counts.todense() # sparse matrices are not supported in rpy2\n\n\nimport rpy2\nimport rpy2.robjects as robjects\n\n/home/runner/work/polygloty/polygloty/renv/python/virtualenvs/renv-python-3.12/lib/python3.12/site-packages/rpy2/rinterface_lib/embedded.py:276: UserWarning: R was initialized outside of rpy2 (R_NilValue != NULL). Trying to use it nevertheless.\n  warnings.warn(msg)\nR was initialized outside of rpy2 (R_NilValue != NULL). Trying to use it nevertheless.\n\nfrom rpy2.robjects import numpy2ri\nfrom rpy2.robjects import default_converter\n\nnp_cv_rules = default_converter + numpy2ri.converter\n\nwith np_cv_rules.context() as cv:\n    robjects.globalenv[\"counts_matrix\"] = counts_dense\n\n    dim = robjects.r[\"dim\"]\n    print(dim(robjects.globalenv[\"counts_matrix\"]))\n\n[ 100 1000]\n\n\n\n5 Usecase\nWe will perform the Compute DE step not in R, but in Python\n\nimport anndata as ad\n\npd_adata = ad.read_h5ad(\"usecase_data/pseudobulk.h5ad\")\n\nSelect small molecule and control:\n\nsm_name = \"Belinostat\"\ncontrol_name = \"Dimethyl Sulfoxide\"\n\nCreate DESeq dataset:\n\nimport rpy2.rinterface\n\nfrom rpy2.robjects.packages import importr\nbase = importr('base')\nprint(base._libPaths())\n\n\n\nimport rpy2\nimport rpy2.robjects as robjects\n\nfrom rpy2.robjects.packages import importr\n\nutils = importr('utils')\n# utils.install_packages('DESeq2')\n\n## base.source(\"../renv/activate.R\")\n\nDESeq2 = importr(\"DESeq2\")\n\n\nimport numpy as np\n\nimport rpy2\nimport rpy2.robjects as robjects\n\nfrom rpy2.robjects import numpy2ri\nfrom rpy2.robjects import pandas2ri\n\nfrom rpy2.robjects import default_converter\nfrom rpy2.robjects.packages import importr\n\nDESeq2 = importr(\"DESeq2\")\n\nnp_cv_rules = default_converter + numpy2ri.converter + pandas2ri.converter\n\nwith np_cv_rules.context() as cv:\n    counts_dense = np.transpose(pd_adata.X.astype(np.int32))\n\n    robjects.globalenv[\"count_data\"] = counts_dense\n    robjects.globalenv[\"obs_data\"] = pd_adata.obs\n\n\nfrom rpy2.robjects import Formula\n\ndesign_formula = Formula('~ sm_name + plate_name')\n\ndds = DESeq2.DESeqDataSetFromMatrix(countData = robjects.globalenv[\"count_data\"],\n        colData = robjects.globalenv[\"obs_data\"],\n        design = design_formula)\n\nRun DESeq2:\n\ndds = DESeq2.DESeq(dds)\n\nGet results:\n\ncontrastv = robjects.StrVector([\"sm_name\", sm_name, control_name])\nres = DESeq2.results(dds, contrast=contrastv)\n\nbase = importr('base')\nres = base.as_data_frame(res)\n\nPreview results:\n\ndplyr = importr('dplyr')\nutils = importr('utils')\n\nres = utils.head(dplyr.arrange(res, 'padj'), 10)\n\nWrite to disk:\n\nwith (robjects.default_converter + pandas2ri.converter).context():\n    res_pd = robjects.conversion.get_conversion().rpy2py(res)\n\n    res_pd.to_csv(\"usecase_data/de_contrasts.csv\")",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>In memory interoperability (from Python)</span>"
    ]
  },
  {
    "objectID": "notebooks/workflows/index.html",
    "href": "notebooks/workflows/index.html",
    "title": "5  Workflows",
    "section": "",
    "text": "5.1 Productionization\nProductionization is the process of transforming research-oriented analysis pipelines into robust, scalable, and maintainable workflows that can be reliably executed in a production environment (Figure 5.1). This transition is essential for ensuring the reproducibility of results, facilitating collaboration among researchers, and enabling the efficient processing of large and complex single-cell datasets.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Workflows</span>"
    ]
  },
  {
    "objectID": "notebooks/workflows/index.html#productionization",
    "href": "notebooks/workflows/index.html#productionization",
    "title": "5  Workflows",
    "section": "",
    "text": "Figure 5.1: An example of the productionization process for single-cell analysis workflows. A) The research environment is characterized by scattered data, manual steps, and ad-hoc analysis pipelines. B) The production environment is streamlined, automated, and standardized, with reproducibility engines in place.\n\n\n\n\nBut how to ensure that your workflow is production-ready?\nIn this chapter, we will explore:\n\nKey qualities of workflows built to stand the test of time\nWhich technologies and workflow frameworks contribute to these qualities\nBest practices to keep in mind during development",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Workflows</span>"
    ]
  },
  {
    "objectID": "notebooks/workflows/index.html#review-of-workflow-frameworks",
    "href": "notebooks/workflows/index.html#review-of-workflow-frameworks",
    "title": "5  Workflows",
    "section": "5.2 Review of Workflow Frameworks",
    "text": "5.2 Review of Workflow Frameworks\nA lot of different workflow frameworks exist, and there are a lot of factors to consider when choosing the right one for your project. Wratten, Wilm, and Göke (2021) conducted a review of popular workflow managers for bioinformatics, evaluating them based on several key aspects, including ease of use, expressiveness, portability, scalability, and learning resources (Table 5.1).\n\n\n\nTable 5.1: Overview of workflow managers for bioinformatics (Wratten, Wilm, and Göke 2021).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTool\nClass\nEase of use\nExpressiveness\nPortability\nScalability\nLearning resources\nPipeline initiatives\n\n\n\n\nGalaxy\nGraphical\n●●●\n●○○\n●●●\n●●●\n●●●\n●●○\n\n\nKNIME\nGraphical\n●●●\n●○○\n○○○\n●●◐\n●●●\n●●○\n\n\nNextflow\nDSL\n●●○\n●●●\n●●●\n●●●\n●●●\n●●●\n\n\nSnakemake\nDSL\n●●○\n●●●\n●●◐\n●●●\n●●○\n●●●\n\n\nGenPipes\nDSL\n●●○\n●●●\n●●○\n●●○\n●●○\n●●○\n\n\nbPipe\nDSL\n●●○\n●●●\n●●○\n●●◐\n●●○\n●○○\n\n\nPachyderm\nDSL\n●●○\n●●●\n●○○\n●●○\n●●●\n○○○\n\n\nSciPipe\nLibrary\n●●○\n●●●\n○○○\n○○○\n●●○\n○○○\n\n\nLuigi\nLibrary\n●●○\n●●●\n●○○\n●●◐\n●●○\n○○○\n\n\nCromwell + WDL\nExecution + workflow specification\n●○○\n●●○\n●●●\n●●◐\n●●○\n●●○\n\n\ncwltool + CWL\nExecution + workflow specification\n●○○\n●●○\n●●◐\n○○○\n●●●\n●●○\n\n\nToil + CWL/WDL/Python\nExecution + workflow specification\n●○○\n●●●\n●◐○\n●●●\n●●○\n●●○\n\n\n\n\n\n\nEven more interesting is the accompanying GitHub repository (GoekeLab/bioinformatics-workflows), which contains a Proof of Concept (PoC) RNA-seq workflow implemented in the different workflow frameworks. These implementations were contributed and reviewed by the developers of the respective frameworks themselves!\n\n\n\nWow! ;)\n\n\nLooking at these implementations, at first glance, one would think that the differences between the frameworks are minimal, and that the choice of framework is mostly a matter of personal preference.\n\n5.2.1 Comparing PoC Workflows to Community-Made Modules\nHowever, comparing the POC workflows (left) to community-made modules (right), it becomes clear that creating production-ready components requires a lot more than specifying a command’s input and output files.\n\n5.2.1.1 Nextflow\n\n\nWratten et al. 2021 PoC (Source):\n\nmain.nf\n\n\nprocess FASTQC {\n  publishDir params.outdir\n\n  input:\n    path index\n    path left\n    path right\n  output:\n    path 'qc'\n\n  \"\"\"\n    mkdir qc && fastqc --quiet '${params.left}' '${params.right}' --outdir qc\n  \"\"\"\n}\n\n\n\n\n\nnf-core (Source):\n\nenvironment.ymlmain.nfmeta.yamltests/main.nf.test\n\n\nchannels:\n  - conda-forge\n  - bioconda\ndependencies:\n  - bioconda::fastqc=0.12.1\n\n\nprocess FASTQC {\n    tag \"$meta.id\"\n    label 'process_medium'\n\n    conda \"${moduleDir}/environment.yml\"\n    container \"${ workflow.containerEngine == 'singularity' && !task.ext.singularity_pull_docker_container ?\n        'https://depot.galaxyproject.org/singularity/fastqc:0.12.1--hdfd78af_0' :\n        'biocontainers/fastqc:0.12.1--hdfd78af_0' }\"\n\n    input:\n    tuple val(meta), path(reads)\n\n    output:\n    tuple val(meta), path(\"*.html\"), emit: html\n    tuple val(meta), path(\"*.zip\") , emit: zip\n    path  \"versions.yml\"           , emit: versions\n\n    when:\n    task.ext.when == null || task.ext.when\n\n    script:\n    def args = task.ext.args ?: ''\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    // Make list of old name and new name pairs to use for renaming in the bash while loop\n    def old_new_pairs = reads instanceof Path || reads.size() == 1 ? [[ reads, \"${prefix}.${reads.extension}\" ]] : reads.withIndex().collect { entry, index -&gt; [ entry, \"${prefix}_${index + 1}.${entry.extension}\" ] }\n    def rename_to = old_new_pairs*.join(' ').join(' ')\n    def renamed_files = old_new_pairs.collect{ old_name, new_name -&gt; new_name }.join(' ')\n\n    // The total amount of allocated RAM by FastQC is equal to the number of threads defined (--threads) time the amount of RAM defined (--memory)\n    // https://github.com/s-andrews/FastQC/blob/1faeea0412093224d7f6a07f777fad60a5650795/fastqc#L211-L222\n    // Dividing the task.memory by task.cpu allows to stick to requested amount of RAM in the label\n    def memory_in_mb = MemoryUnit.of(\"${task.memory}\").toUnit('MB') / task.cpus\n    // FastQC memory value allowed range (100 - 10000)\n    def fastqc_memory = memory_in_mb &gt; 10000 ? 10000 : (memory_in_mb &lt; 100 ? 100 : memory_in_mb)\n\n    \"\"\"\n    printf \"%s %s\\\\n\" $rename_to | while read old_name new_name; do\n        [ -f \"\\${new_name}\" ] || ln -s \\$old_name \\$new_name\n    done\n\n    fastqc \\\\\n        $args \\\\\n        --threads $task.cpus \\\\\n        --memory $fastqc_memory \\\\\n        $renamed_files\n\n    cat &lt;&lt;-END_VERSIONS &gt; versions.yml\n    \"${task.process}\":\n        fastqc: \\$( fastqc --version | sed '/FastQC v/!d; s/.*v//' )\n    END_VERSIONS\n    \"\"\"\n\n    stub:\n    def prefix = task.ext.prefix ?: \"${meta.id}\"\n    \"\"\"\n    touch ${prefix}.html\n    touch ${prefix}.zip\n\n    cat &lt;&lt;-END_VERSIONS &gt; versions.yml\n    \"${task.process}\":\n        fastqc: \\$( fastqc --version | sed '/FastQC v/!d; s/.*v//' )\n    END_VERSIONS\n    \"\"\"\n}\n\n\nname: fastqc\ndescription: Run FastQC on sequenced reads\nkeywords:\n  - quality control\n  - qc\n  - adapters\n  - fastq\ntools:\n  - fastqc:\n      description: |\n        FastQC gives general quality metrics about your reads.\n        It provides information about the quality score distribution\n        across your reads, the per base sequence content (%A/C/G/T).\n        You get information about adapter contamination and other\n        overrepresented sequences.\n      homepage: https://www.bioinformatics.babraham.ac.uk/projects/fastqc/\n      documentation: https://www.bioinformatics.babraham.ac.uk/projects/fastqc/Help/\n      licence: [\"GPL-2.0-only\"]\ninput:\n  - meta:\n      type: map\n      description: |\n        Groovy Map containing sample information\n        e.g. [ id:'test', single_end:false ]\n  - reads:\n      type: file\n      description: |\n        List of input FastQ files of size 1 and 2 for single-end and paired-end data,\n        respectively.\noutput:\n  - meta:\n      type: map\n      description: |\n        Groovy Map containing sample information\n        e.g. [ id:'test', single_end:false ]\n  - html:\n      type: file\n      description: FastQC report\n      pattern: \"*_{fastqc.html}\"\n  - zip:\n      type: file\n      description: FastQC report archive\n      pattern: \"*_{fastqc.zip}\"\n  - versions:\n      type: file\n      description: File containing software versions\n      pattern: \"versions.yml\"\nauthors:\n  - \"@drpatelh\"\n  - \"@grst\"\n  - \"@ewels\"\n  - \"@FelixKrueger\"\nmaintainers:\n  - \"@drpatelh\"\n  - \"@grst\"\n  - \"@ewels\"\n  - \"@FelixKrueger\"\n\n\nnextflow_process {\n\n    name \"Test Process FASTQC\"\n    script \"../main.nf\"\n    process \"FASTQC\"\n\n    tag \"modules\"\n    tag \"modules_nfcore\"\n    tag \"fastqc\"\n\n    test(\"sarscov2 single-end [fastq]\") {\n\n        when {\n            process {\n                \"\"\"\n                input[0] = Channel.of([\n                    [ id: 'test', single_end:true ],\n                    [ file(params.modules_testdata_base_path + 'genomics/sarscov2/illumina/fastq/test_1.fastq.gz', checkIfExists: true) ]\n                ])\n                \"\"\"\n            }\n        }\n\n        then {\n            assertAll (\n                { assert process.success },\n                // NOTE The report contains the date inside it, which means that the md5sum is stable per day, but not longer than that. So you can't md5sum it.\n                // looks like this: &lt;div id=\"header_filename\"&gt;Mon 2 Oct 2023&lt;br/&gt;test.gz&lt;/div&gt;\n                // https://github.com/nf-core/modules/pull/3903#issuecomment-1743620039\n                { assert process.out.html[0][1] ==~ \".*/test_fastqc.html\" },\n                { assert process.out.zip[0][1] ==~ \".*/test_fastqc.zip\" },\n                { assert path(process.out.html[0][1]).text.contains(\"&lt;tr&gt;&lt;td&gt;File type&lt;/td&gt;&lt;td&gt;Conventional base calls&lt;/td&gt;&lt;/tr&gt;\") },\n                { assert snapshot(process.out.versions).match() }\n            )\n        }\n    }\n\n    /* The rest of the tests are omitted */\n}\n\n\n\n\n\n\n\n5.2.1.2 Snakemake\n\n\nWratten et al. 2021 PoC (Source):\n\nfastqc.smk\n\n\nrule fastqc:\n    input:\n        get_fastqs,\n    output:\n        directory(\"results/fastqc/{sample}\"),\n    log:\n        \"logs/fastqc/{sample}.log\",\n    conda:\n        \"envs/fastqc.yaml\"\n    params:\n        \"--quiet --outdir\",\n    shell:\n        \"mkdir {output}; fastqc {input} {params} {output} 2&gt; {log}\"\n\n\n\n\n\nsnakemake-wrappers (Source):\n\nenvironment.yamlmeta.yamlwrapper.pytest/Snakefile\n\n\nchannels:\n  - conda-forge\n  - bioconda\n  - nodefaults\ndependencies:\n  - fastqc =0.12.1\n  - snakemake-wrapper-utils =0.6.2\n\n\nname: fastqc\ndescription: |\n  Generate fastq qc statistics using fastqc.\nurl: https://github.com/s-andrews/FastQC\nauthors:\n  - Julian de Ruiter\ninput:\n  - fastq file\noutput:\n  - html file containing statistics\n  - zip file containing statistics\n\n\n\"\"\"Snakemake wrapper for fastqc.\"\"\"\n\n__author__ = \"Julian de Ruiter\"\n__copyright__ = \"Copyright 2017, Julian de Ruiter\"\n__email__ = \"julianderuiter@gmail.com\"\n__license__ = \"MIT\"\n\n\nfrom os import path\nimport re\nfrom tempfile import TemporaryDirectory\nfrom snakemake.shell import shell\nfrom snakemake_wrapper_utils.snakemake import get_mem\n\nextra = snakemake.params.get(\"extra\", \"\")\nlog = snakemake.log_fmt_shell(stdout=True, stderr=True)\n# Define memory per thread (https://github.com/s-andrews/FastQC/blob/master/fastqc#L201-L222)\nmem_mb = int(get_mem(snakemake, \"MiB\") / snakemake.threads)\n\n\ndef basename_without_ext(file_path):\n    \"\"\"Returns basename of file path, without the file extension.\"\"\"\n\n    base = path.basename(file_path)\n    # Remove file extension(s) (similar to the internal fastqc approach)\n    base = re.sub(\"\\\\.gz$\", \"\", base)\n    base = re.sub(\"\\\\.bz2$\", \"\", base)\n    base = re.sub(\"\\\\.txt$\", \"\", base)\n    base = re.sub(\"\\\\.fastq$\", \"\", base)\n    base = re.sub(\"\\\\.fq$\", \"\", base)\n    base = re.sub(\"\\\\.sam$\", \"\", base)\n    base = re.sub(\"\\\\.bam$\", \"\", base)\n\n    return base\n\n\n# If you have multiple input files fastqc doesn't know what to do. Taking silently only first gives unapreciated results\n\nif len(snakemake.input) &gt; 1:\n    raise IOError(\"Got multiple input files, I don't know how to process them!\")\n\n# Run fastqc, since there can be race conditions if multiple jobs\n# use the same fastqc dir, we create a temp dir.\nwith TemporaryDirectory() as tempdir:\n    shell(\n        \"fastqc\"\n        \" --threads {snakemake.threads}\"\n        \" --memory {mem_mb}\"\n        \" {extra}\"\n        \" --outdir {tempdir:q}\"\n        \" {snakemake.input[0]:q}\"\n        \" {log}\"\n    )\n\n    # Move outputs into proper position.\n    output_base = basename_without_ext(snakemake.input[0])\n    html_path = path.join(tempdir, output_base + \"_fastqc.html\")\n    zip_path = path.join(tempdir, output_base + \"_fastqc.zip\")\n\n    if snakemake.output.html != html_path:\n        shell(\"mv {html_path:q} {snakemake.output.html:q}\")\n\n    if snakemake.output.zip != zip_path:\n        shell(\"mv {zip_path:q} {snakemake.output.zip:q}\")\n\n\nrule fastqc:\n    input:\n        \"reads/{sample}.fastq\"\n    output:\n        html=\"qc/fastqc/{sample}.html\",\n        zip=\"qc/fastqc/{sample}_fastqc.zip\" # the suffix _fastqc.zip is necessary for multiqc to find the file. If not using multiqc, you are free to choose an arbitrary filename\n    params:\n        extra = \"--quiet\"\n    log:\n        \"logs/fastqc/{sample}.log\"\n    threads: 1\n    resources:\n        mem_mb = 1024\n    wrapper:\n        \"master/bio/fastqc\"\n\n\n\n\n\n\n\n5.2.1.3 Toil + WDL\n\n\nWratten et al. 2021 PoC (Source):\n\nfastqc.wdl\n\n\ntask FastQCone {\n  input {\n     File reads\n  }\n\n  command {\n     zcat \"${reads}\" | fastqc stdin:readsone\n  }\n\n  output {\n     File fastqc_res = \"readsone_fastqc.html\"\n  }\n  \n  runtime {\n     docker: 'pegi3s/fastqc'\n  }\n}\n\n\n\n\n\nBioWDL (Source):\n\nfastqc.wdl (Excerpt)fastqc.wdl (Full)\n\n\nversion 1.0\n\n# ... license ...\n\ntask Fastqc {\n    input {\n        File seqFile\n        String outdirPath\n        Boolean casava = false\n        ## ... other arguments ...\n\n        # Set javaXmx a little high.\n        String javaXmx=\"1750M\"\n        Int threads = 1\n        String memory = \"2GiB\"\n        Int timeMinutes = 1 + ceil(size(seqFile, \"G\")) * 4\n        String dockerImage = \"quay.io/biocontainers/fastqc:0.12.1--hdfd78af_0\"\n\n        Array[File]? noneArray\n        File? noneFile\n    }\n\n    # Chops of the .gz extension if present.\n    String name = basename(sub(seqFile, \"\\.gz$\",\"\"))\n    # This regex chops of the extension just as fastqc does it.\n    String reportDir = outdirPath + \"/\" + sub(name, \"\\.[^\\.]*$\", \"_fastqc\")\n\n    # We reimplement the perl wrapper here. This has the advantage that it\n    # gives us more control over the amount of memory used.\n    command &lt;&lt;&lt;\n        set -e\n        mkdir -p \"~{outdirPath}\"\n        FASTQC_DIR=\"/usr/local/opt/fastqc-0.12.1\"\n        export CLASSPATH=\"$FASTQC_DIR:$FASTQC_DIR/sam-1.103.jar:$FASTQC_DIR/jbzip2-0.9.jar:$FASTQC_DIR/cisd-jhdf5.jar\"\n        java -Djava.awt.headless=true -XX:ParallelGCThreads=1 \\\n        -Xms200M -Xmx~{javaXmx} \\\n        ~{\"-Dfastqc.output_dir=\" + outdirPath} \\\n        ~{true=\"-Dfastqc.casava=true\" false=\"\" casava} \\\n        # ... other arguments ...\n        ~{\"-Dfastqc.kmer_size=\" + kmers} \\\n        ~{\"-Djava.io.tmpdir=\" + dir} \\\n        uk.ac.babraham.FastQC.FastQCApplication \\\n        ~{seqFile}\n    &gt;&gt;&gt;\n\n    output {\n        File htmlReport = reportDir + \".html\"\n        File reportZip = reportDir + \".zip\"\n        File? summary = if extract then reportDir + \"/summary.txt\" else noneFile\n        File? rawReport = if extract then reportDir + \"/fastqc_data.txt\" else noneFile\n        Array[File]? images = if extract then glob(reportDir + \"/Images/*.png\") else noneArray\n    }\n\n    runtime {\n        cpu: threads\n        memory: memory\n        time_minutes: timeMinutes\n        docker: dockerImage\n    }\n\n    parameter_meta {\n        # inputs\n        seqFile: {description: \"A fastq file.\", category: \"required\"}\n        outdirPath: {description: \"The path to write the output to.\", catgory: \"required\"}\n        # ... other arguments ...\n        dockerImage: {description: \"The docker image used for this task. Changing this may result in errors which the developers may choose not to address.\", category: \"advanced\"}\n\n        # outputs\n        htmlReport: {description: \"HTML report file.\"}\n        reportZip: {description: \"Source data file.\"}\n        summary: {description: \"Summary file.\"}\n        rawReport: {description: \"Raw report file.\"}\n        images: {description: \"Images in report file.\"}\n    }\n\n    meta {\n        WDL_AID: {\n            exclude: [\"noneFile\", \"noneArray\"]\n        }\n    }\n}\n\n\nversion 1.0\n\n# Copyright (c) 2017 Leiden University Medical Center\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n# SOFTWARE.\n\ntask Fastqc {\n    input {\n        File seqFile\n        String outdirPath\n        Boolean casava = false\n        Boolean nano = false\n        Boolean noFilter = false\n        Boolean extract = false\n        Boolean nogroup = false\n\n        Int? minLength\n        String? format\n        File? contaminants\n        File? adapters\n        File? limits\n        Int? kmers\n        String? dir\n\n        # Set javaXmx a little high. Equal to fastqc default with 7 threads.\n        # This is because some fastq files need more memory. 2G per core\n        # is a nice cluster default, so we use all the rest of the memory for\n        # fastqc so we should have as little OOM crashes as possible even with\n        # weird edge case fastq's.\n        String javaXmx=\"1750M\"\n        Int threads = 1\n        String memory = \"2GiB\"\n        Int timeMinutes = 1 + ceil(size(seqFile, \"G\")) * 4\n        String dockerImage = \"quay.io/biocontainers/fastqc:0.12.1--hdfd78af_0\"\n\n        Array[File]? noneArray\n        File? noneFile\n    }\n\n    # Chops of the .gz extension if present.\n    # The Basename needs to be taken here. Otherwise paths might differ\n    # between similar jobs.\n    String name = basename(sub(seqFile, \"\\.gz$\",\"\"))\n    # This regex chops of the extension and replaces it with _fastqc for\n    # the reportdir.\n    # Just as fastqc does it.\n    String reportDir = outdirPath + \"/\" + sub(name, \"\\.[^\\.]*$\", \"_fastqc\")\n\n    # We reimplement the perl wrapper here. This has the advantage that it\n    # gives us more control over the amount of memory used.\n    command &lt;&lt;&lt;\n        set -e\n        mkdir -p \"~{outdirPath}\"\n        FASTQC_DIR=\"/usr/local/opt/fastqc-0.12.1\"\n        export CLASSPATH=\"$FASTQC_DIR:$FASTQC_DIR/sam-1.103.jar:$FASTQC_DIR/jbzip2-0.9.jar:$FASTQC_DIR/cisd-jhdf5.jar\"\n        java -Djava.awt.headless=true -XX:ParallelGCThreads=1 \\\n        -Xms200M -Xmx~{javaXmx} \\\n        ~{\"-Dfastqc.output_dir=\" + outdirPath} \\\n        ~{true=\"-Dfastqc.casava=true\" false=\"\" casava} \\\n        ~{true=\"-Dfastqc.nano=true\" false=\"\" nano} \\\n        ~{true=\"-Dfastqc.nofilter=true\" false=\"\" noFilter} \\\n        ~{true=\"-Dfastqc.unzip=true\" false=\"\" extract} \\\n        ~{true=\"-Dfastqc.nogroup=true\" false=\"\" nogroup} \\\n        ~{\"-Dfastqc.min_length=\" + minLength} \\\n        ~{\"-Dfastqc.sequence_format=\" + format} \\\n        ~{\"-Dfastqc.threads=\" + threads} \\\n        ~{\"-Dfastqc.contaminant_file=\" + contaminants} \\\n        ~{\"-Dfastqc.adapter_file=\" + adapters} \\\n        ~{\"-Dfastqc.limits_file=\" + limits} \\\n        ~{\"-Dfastqc.kmer_size=\" + kmers} \\\n        ~{\"-Djava.io.tmpdir=\" + dir} \\\n        uk.ac.babraham.FastQC.FastQCApplication \\\n        ~{seqFile}\n    &gt;&gt;&gt;\n\n    output {\n        File htmlReport = reportDir + \".html\"\n        File reportZip = reportDir + \".zip\"\n        File? summary = if extract then reportDir + \"/summary.txt\" else noneFile\n        File? rawReport = if extract then reportDir + \"/fastqc_data.txt\" else noneFile\n        Array[File]? images = if extract then glob(reportDir + \"/Images/*.png\") else noneArray\n    }\n\n    runtime {\n        cpu: threads\n        memory: memory\n        time_minutes: timeMinutes\n        docker: dockerImage\n    }\n\n    parameter_meta {\n        # inputs\n        seqFile: {description: \"A fastq file.\", category: \"required\"}\n        outdirPath: {description: \"The path to write the output to.\", catgory: \"required\"}\n        casava: {description: \"Equivalent to fastqc's --casava flag.\", category: \"advanced\"}\n        nano: {description: \"Equivalent to fastqc's --nano flag.\", category: \"advanced\"}\n        noFilter: {description: \"Equivalent to fastqc's --nofilter flag.\", category: \"advanced\"}\n        extract: {description: \"Equivalent to fastqc's --extract flag.\", category: \"advanced\"}\n        nogroup: {description: \"Equivalent to fastqc's --nogroup flag.\", category: \"advanced\"}\n        minLength: {description: \"Equivalent to fastqc's --min_length option.\", category: \"advanced\"}\n        format: {description: \"Equivalent to fastqc's --format option.\", category: \"advanced\"}\n        contaminants: {description: \"Equivalent to fastqc's --contaminants option.\", category: \"advanced\"}\n        adapters: {description: \"Equivalent to fastqc's --adapters option.\", category: \"advanced\"}\n        limits: {description: \"Equivalent to fastqc's --limits option.\", category: \"advanced\"}\n        kmers: {description: \"Equivalent to fastqc's --kmers option.\", category: \"advanced\"}\n        dir: {description: \"Equivalent to fastqc's --dir option.\", category: \"advanced\"}\n        javaXmx: {description: \"The maximum memory available to the program. Should be lower than `memory` to accommodate JVM overhead.\", category: \"advanced\"}\n        threads: {description: \"The number of cores to use.\", category: \"advanced\"}\n        memory: {description: \"The amount of memory this job will use.\", category: \"advanced\"}\n        timeMinutes: {description: \"The maximum amount of time the job will run in minutes.\", category: \"advanced\"}\n        dockerImage: {description: \"The docker image used for this task. Changing this may result in errors which the developers may choose not to address.\", category: \"advanced\"}\n\n        # outputs\n        htmlReport: {description: \"HTML report file.\"}\n        reportZip: {description: \"Source data file.\"}\n        summary: {description: \"Summary file.\"}\n        rawReport: {description: \"Raw report file.\"}\n        images: {description: \"Images in report file.\"}\n    }\n\n    meta {\n        WDL_AID: {\n            exclude: [\"noneFile\", \"noneArray\"]\n        }\n    }\n}\n\n\n\n\n\n\n\n\n5.2.2 Limitations of the study\nHowever, the Supplementary Table shows that the comparison in Table 5.1 was rather limited, since the score of each category was only based on a single criterion. Of the following categories, only “Scalability” was determined by more than one criterion:\n\nEase of Use: Graphical interface with execution environment (score of 3), programming interface with in-built execution environment (score of 2), separated development and execution environment (score of 1).\nExpressiveness: Based on an existing programming language (3) or a new language or restricted vocabulary (2), primary interaction with graphical user interface (1).\nPortability: Integration with three or more container and package manager platforms (3), two platforms are supported (2), one platform is supported (1).\nScalability: Considers cloud support, scheduler and orchestration tool integration, and executor support. Please refer to Supplementary Table 1 - Sheet 2 (Scalability).\nLearning resources: Official tutorials, forums and events (3), tutorials and forums (2), tutorials or forums (1).\nPipelines Initiatives: Community and curated (3), community or curated (2), not community or curated (1).\n\nBy comparing the example code of the respective workflow frameworks, it also becomes clear that we need not only look at example code of POC workflows, but actual production-ready workflows and pipelines. Such code often require a lot more functionality, including:\n\nError handling\nLogging\nData provenance\nParameterization\nTesting\nDocumentation\nContainerization\nResource management",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Workflows</span>"
    ]
  },
  {
    "objectID": "notebooks/workflows/index.html#qualities-of-a-production-ready-workflow",
    "href": "notebooks/workflows/index.html#qualities-of-a-production-ready-workflow",
    "title": "5  Workflows",
    "section": "5.3 Qualities of a Production-Ready Workflow",
    "text": "5.3 Qualities of a Production-Ready Workflow\nBuilding production-ready workflows for single-cell analysis involves integrating a variety of tools, technologies, and best practices. In order to meet the demands of large-scale data processing, reproducibility, and collaboration, a production-ready workflow should exhibit the following essential qualities (Figure 5.2):\n\n\n\n\n\n\nFigure 5.2: Essential qualities of a production-ready workflow.\n\n\n\n\nPolyglot: Seamlessly integrate tools and libraries from different programming languages, allowing you to leverage the strengths of each language for specific tasks. This facilitates the use of specialized tools and optimizes the analysis pipeline for performance and efficiency.\nModular: A well-structured workflow should be composed of modular and reusable components, promoting code maintainability and facilitating collaboration. Each module should have a clear purpose and well-defined inputs and outputs, enabling easy integration and replacement of individual steps within the pipeline.\nScalable: Single-cell datasets can be massive, and a production-ready workflow should be able to handle large volumes of data efficiently. This involves utilizing scalable compute environments, optimizing data storage and retrieval, and implementing parallelization strategies to accelerate analysis.\nReproducible: Ensuring reproducibility is crucial for scientific rigor and validation. A production-ready workflow should capture all the necessary information, including code, data, parameters, and software environments, to enable others to replicate the analysis and obtain consistent results.\nPortable: The workflow should be designed to run seamlessly across different computing platforms and environments, promoting accessibility and collaboration. Containerization technologies like Docker can help achieve portability by encapsulating the workflow and its dependencies into a self-contained unit.\nCommunity: Leveraging community resources, tools, and best practices can accelerate the development of production-ready workflows. This is because developing high-quality components can at times be time-consuming, and sharing resources can help reduce duplication of effort and promote collaboration.\nMaintainable: A production-ready workflow should be well-documented, organized, and easy to understand, facilitating updates, modifications, and troubleshooting. Clear documentation of code, data, and parameters ensures that the workflow remains accessible and usable over time.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Workflows</span>"
    ]
  },
  {
    "objectID": "notebooks/workflows/index.html#technologies-for-production-ready-workflows",
    "href": "notebooks/workflows/index.html#technologies-for-production-ready-workflows",
    "title": "5  Workflows",
    "section": "5.4 Technologies for Production-Ready Workflows",
    "text": "5.4 Technologies for Production-Ready Workflows\nThe essential qualities of a production-ready workflow are achieved through a combination of enabling technologies (Figure 5.3). These technologies provide the foundation for building scalable, reproducible, and maintainable workflows for single-cell analysis.\n\n\n\n\n\n\nFigure 5.3: The essential qualities of a production-ready workflow are achieved through a combination of enabling technologies.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Workflows</span>"
    ]
  },
  {
    "objectID": "notebooks/workflows/index.html#quality-assessment-of-workflow-frameworks",
    "href": "notebooks/workflows/index.html#quality-assessment-of-workflow-frameworks",
    "title": "5  Workflows",
    "section": "5.5 Quality Assessment of Workflow Frameworks",
    "text": "5.5 Quality Assessment of Workflow Frameworks\nGiven the abovementioned limitations, we decided to conduct our own quality assessment of workflow frameworks. This assessment is still largely in the works, but we’re happy to share the preliminary results with you.\nThe data is based on a review of the documentation and community resources for each framework. We evaluated the frameworks based on the list of essential qualities mentioned in the previous section (Figure 5.2).\n\n5.5.1 Included frameworks\nThe following workflow frameworks were included in the assessment:\n\nNextflow: A domain-specific language for creating scalable and reproducible data analysis pipelines.    \nSnakemake: A workflow management system that uses Python-based rules to define dependencies and execute tasks.    \nGalaxy: A web-based platform for creating, running, and sharing data analysis workflows without the need for programming expertise.    \nViash + Nextflow: A combination of Viash, a tool for defining bioinformatics workflow components, and Nextflow for scalable and reproducible execution.    \nArgo Workflows: A Kubernetes-native workflow engine for orchestrating containerized tasks and automating complex workflows.  \n\n\n\n5.5.2 Quality Assessment Criteria\nThe quality assessment was based on the following criteria:\n\nCommunityMaintainabilityModularityPolyglotPortabilityReproducibilityScalability\n\n\nDoes a library of components exist?:\n\nA library of OSS components is available\nCommunity repository has frequent commits\nCommunity repository has &gt; 10 frequent non-employee commitors\nAre there repositories of OSS workflows available?\n\n\n\nHow easy is it to update, modify, and troubleshoot workflows?:\n\nWorkflows are well-structured and easy to understand.\nFramework supports documenting the interfaces of components and workflows\nVersion control and collaboration features facilitate team-based development.\nComponents can be unit tested\nDocumentation specifies how to unit test components\n\n\n\nHow effectively does the framework promote modular design and reusability?:\n\nSoftware can be easily encapsulated as a modular component.\nComponents have well-defined inputs and outputs.\nComponents can be shared and reused across different workflows.\nThe framework supports dependency management\nThe framework supports versioning of components.\n\n\n\nIs it easy to switch between different programming / scripting language within one workflow?:\n\nCalling a script from another language is possible\nComponents can be written in multiple languages and communicate via a file-based interface\nComponents can be written in multiple languages and communicate via an in-memory data channel\n\n\n\nSupport for various compute platforms:\n\nSupport for AWS Batch\nSupport for Azure Batch\nSupport for Google Cloud\nSupport for Kubernetes\nSupport for Local execution\nSupport for PBS/Torque\nSupport for SLURM\nSupport for additional compute platforms\n\nSupport for various containerization technologies:\n\nSupport for Apptainer\nSupport for Docker\nSupport for Podman\nSupport for additional containerization technologies\n\nSupport for various storage solutions:\n\nAWS S3\nAzure Blob Storage / Files\nGoogle Storage\nHTTPS\nFTP\nSupport for additional storage solutions\n\n\n\nHow effectively does the framework ensure reproducibility of results?:\n\nIndividual components can list their software dependencies\nPer-component containerisation is supported\nExtending images with additional dependencies is supported\nData provenance tracking is built-in or can be easily integrated.\nFramework promotes versioned releases of the workflow software and images to ensure reproducibility\n\n\n\nHow well does the framework handle large and complex workflows?:\n\nThe framework supports asynchronous and distributed execution.\nResource management and optimization features are available.\nPerformance monitoring and profiling tools are provided.\n\n\n\n\nThese criteria and subsequent scores will be further refined and validated as part of our ongoing research.\n\n\n5.5.3 Quality Scores\nThe aggregated quality scores for each framework are shown below. The scores are based on the evaluation of the essential qualities of a production-ready workflow.\n\n\n\n\n\nQuality scores for different workflow frameworks.\n\n\n\n\nRaw scores and detailed explanations behind the reasoning of the resulting scores can be found in the Workflow Quality Assessment Spreadsheet.\n\n\n5.5.4 Quality assessment contributors\n\nJakub Majerčík\nMichaela Müller\nRobrecht Cannoodt\nToni Verbeiren",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Workflows</span>"
    ]
  },
  {
    "objectID": "notebooks/workflows/index.html#conclusion",
    "href": "notebooks/workflows/index.html#conclusion",
    "title": "5  Workflows",
    "section": "5.6 Conclusion",
    "text": "5.6 Conclusion\n\n\n\n\nHeumos, Lukas, Anna C. Schaar, Christopher Lance, Anastasia Litinetskaya, Felix Drost, Luke Zappia, Malte D. Lücken, et al. 2023. “Best Practices for Single-Cell Analysis Across Modalities.” Nature Reviews Genetics 24 (8): 550–72. https://doi.org/10.1038/s41576-023-00586-w.\n\n\nWratten, Laura, Andreas Wilm, and Jonathan Göke. 2021. “Reproducible, Scalable, and Shareable Analysis Pipelines with Bioinformatics Workflow Managers.” Nature Methods 18 (10): 1161–68. https://doi.org/10.1038/s41592-021-01254-9.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Workflows</span>"
    ]
  },
  {
    "objectID": "book/book_slides.html",
    "href": "book/book_slides.html",
    "title": "Slides",
    "section": "",
    "text": "Here are the slides used during the workshop:\n    View slides in full screen\n       \n      \n    \n  \n  Download PDF File\n   \n    Unable to display PDF file. Download instead.",
    "crumbs": [
      "Slides"
    ]
  },
  {
    "objectID": "book/references.html",
    "href": "book/references.html",
    "title": "References",
    "section": "",
    "text": "Heumos, Lukas, Anna C. Schaar, Christopher Lance, Anastasia\nLitinetskaya, Felix Drost, Luke Zappia, Malte D. Lücken, et al. 2023.\n“Best Practices for Single-Cell Analysis Across\nModalities.” Nature Reviews Genetics 24 (8): 550–72. https://doi.org/10.1038/s41576-023-00586-w.\n\n\n“OP3 H5AD on S3.” 2024. https://openproblems-bio.s3.amazonaws.com/public/neurips-2023-competition/sc_counts_reannotated_with_counts.h5ad.\n\n\n“Open Problems Kaggle Competition - Single Cell\nPerturbations.” 2023. https://www.kaggle.com/competitions/open-problems-single-cell-perturbations/overview.\n\n\n“OpenProblems Perturbation Prediction Benchmark.” 2024. https://openproblems.bio/results/perturbation_prediction/.\n\n\n“SRA SRP527159.” 2024. https://trace.ncbi.nlm.nih.gov/Traces/?view=study&acc=SRP527159.\n\n\nWratten, Laura, Andreas Wilm, and Jonathan Göke. 2021.\n“Reproducible, Scalable, and Shareable Analysis Pipelines with\nBioinformatics Workflow Managers.” Nature Methods 18\n(10): 1161–68. https://doi.org/10.1038/s41592-021-01254-9.",
    "crumbs": [
      "References"
    ]
  }
]